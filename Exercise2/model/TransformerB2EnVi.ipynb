{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbcquoc/transformer/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVOKeezWPsSs",
        "outputId": "24fe59fa-1dd1-4afd-bb3b-c2015314b9a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "! pip -q install torchtext==0.6.0\n",
        "! pip -q install pyvi\n",
        "! pip -q install sentencepiece\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8gvN64qvNQIS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X9da_ZuSNQIW"
      },
      "outputs": [],
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "# Embedder(100, 512)(torch.LongTensor([1,2,3,4])).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rP64KizDNQIa"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Bảng pe mình vẽ ở trên\n",
        "        for pos in range(max_seq_length):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x*math.sqrt(self.d_model)\n",
        "        seq_length = x.size(1)\n",
        "\n",
        "        pe = Variable(self.pe[:, :seq_length], requires_grad=False)\n",
        "\n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        # cộng embedding vector với pe\n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# PositionalEncoder(512)(torch.rand(5, 30, 512)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2nJMcGuUNQId"
      },
      "outputs": [],
      "source": [
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    q: batch_size x head x seq_length x d_model\n",
        "    k: batch_size x head x seq_length x d_model\n",
        "    v: batch_size x head x seq_length x d_model\n",
        "    mask: batch_size x 1 x 1 x seq_length\n",
        "    output: batch_size x head x seq_length x d_model\n",
        "    \"\"\"\n",
        "\n",
        "    # attention score được tính bằng cách nhân q với k\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "    # xong rồi thì chuẩn hóa bằng softmax\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output, scores\n",
        "\n",
        "# attention(torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ANQ4C3EENQIh"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model//heads\n",
        "        self.h = heads\n",
        "        self.attn = None\n",
        "\n",
        "        # tạo ra 3 ma trận trọng số là q_linear, k_linear, v_linear như hình trên\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \"\"\"\n",
        "        q: batch_size x seq_length x d_model\n",
        "        k: batch_size x seq_length x d_model\n",
        "        v: batch_size x seq_length x d_model\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        bs = q.size(0)\n",
        "        # nhân ma trận trọng số q_linear, k_linear, v_linear với dữ liệu đầu vào q, k, v\n",
        "        # ở bước encode các bạn lưu ý rằng q, k, v chỉ là một (xem hình trên)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # tính attention score\n",
        "        scores, self.attn = attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
        "\n",
        "        output = self.out(concat)\n",
        "        return output\n",
        "\n",
        "# MultiHeadAttention(8, 512)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 30, 512)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n6-_9Hq-NQIk"
      },
      "outputs": [],
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = d_model\n",
        "\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H1ndbdMXNQIn"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Trong kiến trúc của chúng ta có tầng linear\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-Wwo91xDNQIq"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        x: batch_size x seq_length x d_model\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        x2 = self.norm_1(x)\n",
        "        # tính attention value, các bạn để ý q, k, v là giống nhau\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# EncoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32 , 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6mDt2NPeNQIu"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        x: batch_size x seq_length x d_model\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        \"\"\"\n",
        "        # Các bạn xem hình trên, kiến trúc mình vẽ với code ở chỗ này tương đương nhau.\n",
        "        x2 = self.norm_1(x)\n",
        "        # multihead attention thứ nhất, chú ý các từ ở target\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        # masked mulithead attention thứ 2. k, v là giá trị output của mô hình encoder\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# DecoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZcU8nyvzNQIx"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Một encoder có nhiều encoder layer nhé !!!\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Encoder(232, 512,6,8,0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5lBRYMg_NQI0"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"Một decoder có nhiều decoder layer nhé !!!\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        trg: batch_size x seq_length\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Decoder(232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DpxSCRILNQI3"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\" Cuối cùng ghép chúng lại với nhau để được mô hình transformer hoàn chỉnh\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        trg: batch_size x seq_length\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x vocab_size\n",
        "        \"\"\"\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "\n",
        "# Transformer(232, 232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.LongTensor(32, 30).random_(0, 10),torch.rand(32, 1, 30),torch.rand(32, 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M5tvzW9jNQI6"
      },
      "outputs": [],
      "source": [
        "from torchtext import data\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "\n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkBjLH96NQI8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def nopeak_mask(size, device):\n",
        "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
        "     mô hình không nhìn thấy được các từ ở tương lai\n",
        "    \"\"\"\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    np_mask = np_mask.to(device)\n",
        "\n",
        "    return np_mask\n",
        "\n",
        "def create_masks(src, trg, src_pad, trg_pad, device):\n",
        "    \"\"\" Tạo mask cho encoder,\n",
        "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào\n",
        "    \"\"\"\n",
        "    src_mask = (src != src_pad).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size, device)\n",
        "        if trg.is_cuda:\n",
        "            np_mask.cuda()\n",
        "        trg_mask = trg_mask & np_mask\n",
        "\n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9YoUVx4xjEb7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "\n",
        "def get_synonym(word, SRC):\n",
        "    syns = wordnet.synsets(word)\n",
        "    for s in syns:\n",
        "        for l in s.lemmas():\n",
        "            if SRC.vocab.stoi[l.name()] != 0:\n",
        "                return SRC.vocab.stoi[l.name()]\n",
        "\n",
        "    return 0\n",
        "\n",
        "def multiple_replace(dict, text):\n",
        "  # Create a regular expression  from the dictionary keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # For each match, look-up corresponding value in dictionary\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1IJpUEIMgMbw"
      },
      "outputs": [],
      "source": [
        "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
        "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
        "    \"\"\"\n",
        "    init_tok = TRG.vocab.stoi['<sos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "\n",
        "    # tính sẵn output của encoder\n",
        "    e_output = model.encoder(src, src_mask)\n",
        "\n",
        "    outputs = torch.LongTensor([[init_tok]])\n",
        "\n",
        "    outputs = outputs.to(device)\n",
        "\n",
        "    trg_mask = nopeak_mask(1, device)\n",
        "    # dự đoán kí tự đầu tiên\n",
        "    out = model.out(model.decoder(outputs,\n",
        "    e_output, src_mask, trg_mask))\n",
        "    out = F.softmax(out, dim=-1)\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
        "\n",
        "    outputs = torch.zeros(k, max_len).long()\n",
        "    outputs = outputs.to(device)\n",
        "    outputs[:, 0] = init_tok\n",
        "    outputs[:, 1] = ix[0]\n",
        "\n",
        "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
        "\n",
        "    e_outputs = e_outputs.to(device)\n",
        "    e_outputs[:, :] = e_output[0]\n",
        "\n",
        "    return outputs, e_outputs, log_scores\n",
        "\n",
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "\n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "\n",
        "    return outputs, log_scores\n",
        "\n",
        "def beam_search(src, model, SRC, TRG, device, k, max_len):\n",
        "\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
        "    eos_tok = TRG.vocab.stoi['<eos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "    ind = None\n",
        "    for i in range(2, max_len):\n",
        "\n",
        "        trg_mask = nopeak_mask(i, device)\n",
        "\n",
        "        out = model.out(model.decoder(outputs[:,:i],\n",
        "        e_outputs, src_mask, trg_mask))\n",
        "\n",
        "        out = F.softmax(out, dim=-1)\n",
        "\n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
        "\n",
        "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
        "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
        "        for vec in ones:\n",
        "            i = vec[0]\n",
        "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
        "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
        "\n",
        "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
        "\n",
        "        if num_finished_sentences == k:\n",
        "            alpha = 0.7\n",
        "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
        "            _, ind = torch.max(log_scores * div, 1)\n",
        "            ind = ind.data[0]\n",
        "            break\n",
        "\n",
        "    if ind is None:\n",
        "\n",
        "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
        "\n",
        "    else:\n",
        "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "s-AFuSOIhi7X"
      },
      "outputs": [],
      "source": [
        "# def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n",
        "#     \"\"\"Dịch một câu sử dụng beamsearch\n",
        "#     \"\"\"\n",
        "#     model.eval()\n",
        "#     indexed = []\n",
        "#     sentence = SRC.preprocess(sentence)\n",
        "\n",
        "#     for tok in sentence:\n",
        "#         if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n",
        "#             indexed.append(SRC.vocab.stoi[tok])\n",
        "#         else:\n",
        "#             indexed.append(get_synonym(tok, SRC))\n",
        "\n",
        "#     sentence = Variable(torch.LongTensor([indexed]))\n",
        "\n",
        "#     sentence = sentence.to(device)\n",
        "\n",
        "#     sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n",
        "\n",
        "#     return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uee4YaQNQI_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_uVO0yr_NQJC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torchtext import data\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def clean_text(sentence: str) -> str:\n",
        "    sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "    sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "    sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "    sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "    sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "    sentence = sentence.lower()\n",
        "    return sentence.strip()\n",
        "\n",
        "def read_data(src_file, trg_file):\n",
        "    src_data = open(src_file, encoding=\"utf-8\").read().strip().split('\\n')\n",
        "    trg_data = open(trg_file, encoding=\"utf-8\").read().strip().split('\\n')\n",
        "    return src_data, trg_data\n",
        "\n",
        "\n",
        "def train_spm_bpe_from_lines(lines, model_prefix, vocab_size=16000,\n",
        "                            character_coverage=1.0, user_defined_symbols=None):\n",
        "    tmp = f\"{model_prefix}_corpus.txt\"\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        for s in lines:\n",
        "            f.write(clean_text(s) + \"\\n\")   # ✅ train on cleaned text\n",
        "\n",
        "    uds = user_defined_symbols or []\n",
        "    uds_arg = \",\".join(uds) if uds else \"\"\n",
        "\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=tmp,\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size,\n",
        "        model_type=\"bpe\",\n",
        "        character_coverage=character_coverage,\n",
        "        unk_id=0,\n",
        "        pad_id=-1,\n",
        "        bos_id=-1,\n",
        "        eos_id=-1,\n",
        "        user_defined_symbols=uds_arg if uds_arg else None\n",
        "    )\n",
        "    os.remove(tmp)\n",
        "\n",
        "class tokenize(object):\n",
        "    def __init__(self, model_file):\n",
        "        self.sp = spm.SentencePieceProcessor(model_file=model_file)\n",
        "\n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = clean_text(sentence)\n",
        "        return self.sp.encode(sentence, out_type=str)\n",
        "\n",
        "    def detokenize(self, pieces):\n",
        "        return self.sp.decode(pieces)\n",
        "\n",
        "def create_fields(src_lang, trg_lang,\n",
        "                  src_model_file=\"spm_src_bpe.model\",\n",
        "                  trg_model_file=\"spm_trg_bpe.model\"):\n",
        "    print(\"loading sentencepiece BPE tokenizers (with cleaning)...\")\n",
        "\n",
        "    t_src = tokenize(src_model_file)\n",
        "    t_trg = tokenize(trg_model_file)\n",
        "\n",
        "    SRC = data.Field(\n",
        "        lower=False,\n",
        "        tokenize=t_src.tokenizer,\n",
        "        pad_token=\"<pad>\",\n",
        "        unk_token=\"<unk>\"\n",
        "    )\n",
        "\n",
        "    TRG = data.Field(\n",
        "        lower=False,\n",
        "        tokenize=t_trg.tokenizer,\n",
        "        init_token=\"<sos>\",\n",
        "        eos_token=\"<eos>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        unk_token=\"<unk>\"\n",
        "    )\n",
        "\n",
        "    return SRC, TRG, t_src, t_trg\n",
        "\n",
        "def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n",
        "    print(\"creating dataset and iterator... \")\n",
        "\n",
        "    raw_data = {'src': [line for line in src_data], 'trg': [line for line in trg_data]}\n",
        "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
        "\n",
        "    # ✅ lọc đúng theo số token sau preprocess (clean + sentencepiece)\n",
        "    mask = (\n",
        "        df['src'].apply(lambda s: len(SRC.preprocess(s)) < max_strlen) &\n",
        "        df['trg'].apply(lambda s: len(TRG.preprocess(s)) < max_strlen)\n",
        "    )\n",
        "    df = df.loc[mask]\n",
        "\n",
        "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
        "\n",
        "    data_fields = [('src', SRC), ('trg', TRG)]\n",
        "    ds = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
        "\n",
        "    it = MyIterator(\n",
        "        ds, batch_size=batchsize, device=device,\n",
        "        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "        batch_size_fn=batch_size_fn, train=istrain, shuffle=True\n",
        "    )\n",
        "\n",
        "    os.remove('translate_transformer_temp.csv')\n",
        "\n",
        "    if istrain:\n",
        "        SRC.build_vocab(ds)\n",
        "        TRG.build_vocab(ds)\n",
        "\n",
        "    return it\n",
        "\n",
        "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len, sp_trg):\n",
        "    model.eval()\n",
        "\n",
        "    src_tokens = SRC.preprocess(sentence)  # clean + sp encode\n",
        "    unk = SRC.vocab.stoi.get(\"<unk>\", 0)\n",
        "    src_ids = [SRC.vocab.stoi.get(tok, unk) for tok in src_tokens]\n",
        "    src_tensor = torch.LongTensor([src_ids]).to(device)\n",
        "\n",
        "    pred_pieces_str = beam_search(src_tensor, model, SRC, TRG, device, k, max_len)\n",
        "    pieces = pred_pieces_str.split()\n",
        "    pred_text = sp_trg.detokenize(pieces)\n",
        "    return pred_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "l4POJRxdNQJF"
      },
      "outputs": [],
      "source": [
        "def step(model, optimizer,batch, criterion):\n",
        "    \"\"\"\n",
        "    Một lần cập nhật mô hình\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    src = batch.src.transpose(0,1).cuda()\n",
        "    trg = batch.trg.transpose(0,1).cuda()\n",
        "    trg_input = trg[:, :-1]\n",
        "    src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "    preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "    ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "    loss.backward()\n",
        "    optimizer.step_and_update_lr()\n",
        "\n",
        "    loss = loss.item()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "c5sPA-k_NQJI"
      },
      "outputs": [],
      "source": [
        "def validiate(model, valid_iter, criterion):\n",
        "    \"\"\" Tính loss trên tập validation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = []\n",
        "        for batch in valid_iter:\n",
        "            src = batch.src.transpose(0,1).cuda()\n",
        "            trg = batch.trg.transpose(0,1).cuda()\n",
        "            trg_input = trg[:, :-1]\n",
        "            src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "            ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "\n",
        "            loss = loss.item()\n",
        "\n",
        "            total_loss.append(loss)\n",
        "\n",
        "    avg_loss = np.mean(total_loss)\n",
        "\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OW8pRq91rwJR"
      },
      "outputs": [],
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.init_lr = init_lr\n",
        "        self.d_model = d_model\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_steps = 0\n",
        "\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients with the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        d_model = self.d_model\n",
        "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
        "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
        "\n",
        "    def state_dict(self):\n",
        "        optimizer_state_dict = {\n",
        "            'init_lr':self.init_lr,\n",
        "            'd_model':self.d_model,\n",
        "            'n_warmup_steps':self.n_warmup_steps,\n",
        "            'n_steps':self.n_steps,\n",
        "            '_optimizer':self._optimizer.state_dict(),\n",
        "        }\n",
        "\n",
        "        return optimizer_state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.init_lr = state_dict['init_lr']\n",
        "        self.d_model = state_dict['d_model']\n",
        "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
        "        self.n_steps = state_dict['n_steps']\n",
        "\n",
        "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LHGeSHThtlj-"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            # true_dist = pred.data.clone()\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "            true_dist[:, self.padding_idx] = 0\n",
        "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "            if mask.dim() > 0:\n",
        "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Sg257Gk_Kzzw"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def bleu(valid_src_data, valid_trg_data, model, SRC, TRG, device, k, max_strlen, sp_trg):\n",
        "    SPECIALS = {\"<pad>\", \"<sos>\", \"<eos>\"}\n",
        "\n",
        "    pred_sents = []\n",
        "    for sentence in valid_src_data:\n",
        "        pred_text = translate_sentence(sentence, model, SRC, TRG, device, k, max_strlen, sp_trg)\n",
        "        pred_tok = [t for t in TRG.preprocess(pred_text) if t not in SPECIALS]\n",
        "        pred_sents.append(pred_tok)\n",
        "\n",
        "    trg_sents = []\n",
        "    for sent in valid_trg_data:\n",
        "        ref_tok = [t for t in TRG.preprocess(sent) if t not in SPECIALS]\n",
        "        trg_sents.append([ref_tok])\n",
        "\n",
        "    return bleu_score(pred_sents, trg_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Nhgu-SPTNQJL"
      },
      "outputs": [],
      "source": [
        "opt = {\n",
        "    'train_src_data':'./data/train.en.txt',\n",
        "    'train_trg_data':'./data/train.vi.txt',\n",
        "    'valid_src_data':'./data/public_test.en.txt',\n",
        "    'valid_trg_data':'./data/public_test.vi.txt',\n",
        "    'src_lang': 'spm_src_bpe.model',\n",
        "    'trg_lang': 'spm_trg_bpe.model',\n",
        "    'max_strlen':256,\n",
        "    'batchsize':1500,\n",
        "    'device':'cuda',\n",
        "    'd_model': 512,\n",
        "    'n_layers': 6,\n",
        "    'heads': 8,\n",
        "    'dropout': 0.1,\n",
        "    'lr':0.0001,\n",
        "    'epochs':20,\n",
        "    'printevery': 200,\n",
        "    'k':5,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HOay5MrNQJO",
        "outputId": "6fd8ff36-ff81-43bb-eb10-adec0912dddd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'gdown' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "os.makedirs('./data/', exist_ok=True)\n",
        "! gdown --id 1Fuo_ALIFKlUvOPbK5rUA5OfAS2wKn_95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIWJTjdLNQJR",
        "outputId": "c614ef7d-3073-49b1-c925-1ca2c894942b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "! unzip -o en_vi.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBotIB8pNQJU",
        "outputId": "039ac075-b78f-421d-adfa-3154acd87ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done: spm_src_bpe.model spm_trg_bpe.model\n",
            "loading sentencepiece BPE tokenizers (with cleaning)...\n",
            "creating dataset and iterator... \n",
            "creating dataset and iterator... \n"
          ]
        }
      ],
      "source": [
        "train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n",
        "valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n",
        "\n",
        "SPECIALS = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "\n",
        "if not os.path.exists(\"spm_src_bpe.model\"):\n",
        "    train_spm_bpe_from_lines(train_src_data, \"spm_src_bpe\",\n",
        "                             vocab_size=16000, character_coverage=1.0,\n",
        "                             user_defined_symbols=SPECIALS)\n",
        "\n",
        "if not os.path.exists(\"spm_trg_bpe.model\"):\n",
        "    train_spm_bpe_from_lines(train_trg_data, \"spm_trg_bpe\",\n",
        "                             vocab_size=16000, character_coverage=1.0,\n",
        "                             user_defined_symbols=SPECIALS)\n",
        "\n",
        "print(\"Done:\", \"spm_src_bpe.model\", \"spm_trg_bpe.model\")\n",
        "\n",
        "SRC, TRG, sp_src, sp_trg = create_fields(opt['src_lang'], opt['trg_lang'])\n",
        "\n",
        "train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'],\n",
        "                            opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n",
        "valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'],\n",
        "                            opt['batchsize'], opt['device'], SRC, TRG, istrain=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Gnw9xrJeNQJX"
      },
      "outputs": [],
      "source": [
        "src_pad = SRC.vocab.stoi['<pad>']\n",
        "trg_pad = TRG.vocab.stoi['<pad>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5RccNL8VNQJd"
      },
      "outputs": [],
      "source": [
        "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt['d_model'], opt['n_layers'], opt['heads'], opt['dropout'])\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(opt['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "12debLGiNQJg"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = ScheduledOptim(\n",
        "        torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "        0.2, opt['d_model'], 4000)\n",
        "\n",
        "criterion = LabelSmoothingLoss(len(TRG.vocab), padding_idx=trg_pad, smoothing=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, glob, re\n",
        "import torch\n",
        "\n",
        "def save_checkpoint(path, epoch, model, optimizer, best_metric, history, opt, extra=None):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer._optimizer.state_dict() if hasattr(optimizer, \"_optimizer\") else optimizer.state_dict(),\n",
        "        \"best_metric\": best_metric,\n",
        "        \"history\": history,\n",
        "        \"opt\": opt,\n",
        "        \"extra\": extra or {}\n",
        "    }\n",
        "    # ScheduledOptim có n_steps riêng -> lưu để resume LR schedule đúng\n",
        "    if hasattr(optimizer, \"n_steps\"):\n",
        "        ckpt[\"sched_n_steps\"] = optimizer.n_steps\n",
        "\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "def load_checkpoint(path, model, optimizer=None, map_location=\"cpu\", strict=True):\n",
        "    ckpt = torch.load(path, map_location=map_location)\n",
        "    model.load_state_dict(ckpt[\"model_state\"], strict=strict)\n",
        "\n",
        "    if optimizer is not None:\n",
        "        # restore optimizer state\n",
        "        if hasattr(optimizer, \"_optimizer\"):\n",
        "            optimizer._optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "        else:\n",
        "            optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\n",
        "        # restore ScheduledOptim step count\n",
        "        if hasattr(optimizer, \"n_steps\") and \"sched_n_steps\" in ckpt:\n",
        "            optimizer.n_steps = ckpt[\"sched_n_steps\"]\n",
        "\n",
        "    return ckpt\n",
        "\n",
        "def find_latest_checkpoint(ckpt_dir, prefix=\"last_epoch_\", ext=\".pt\"):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        return None\n",
        "    files = glob.glob(os.path.join(ckpt_dir, f\"{prefix}*{ext}\"))\n",
        "    if not files:\n",
        "        return None\n",
        "    # sort by epoch number in filename\n",
        "    def get_epoch(f):\n",
        "        m = re.search(rf\"{re.escape(prefix)}(\\d+){re.escape(ext)}$\", os.path.basename(f))\n",
        "        return int(m.group(1)) if m else -1\n",
        "    files = sorted(files, key=get_epoch)\n",
        "    return files[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "CKPT_DIR = \"./checkpoints\"\n",
        "LAST_CKPT = os.path.join(CKPT_DIR, \"last.pt\")\n",
        "BEST_CKPT = os.path.join(CKPT_DIR, \"best.pt\")\n",
        "\n",
        "# chọn tiêu chí lưu best:\n",
        "# - \"bleu\": maximize\n",
        "# - \"loss\": minimize\n",
        "BEST_BY = \"bleu\"   # hoặc \"loss\"\n",
        "\n",
        "resume_path = None  # ví dụ: \"./checkpoints/last.pt\" hoặc None\n",
        "# nếu muốn auto-resume từ checkpoint mới nhất:\n",
        "# resume_path = find_latest_checkpoint(CKPT_DIR, prefix=\"last_epoch_\", ext=\".pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "JeZqfQPANQJl",
        "outputId": "467a5beb-217b-47b3-8afc-4f0dc28426a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 000 | iter 00199 | train loss 2.5819 | dt 0.10s\n",
            "epoch 000 | iter 00399 | train loss 2.5573 | dt 0.10s\n",
            "epoch 000 | iter 00599 | train loss 2.5640 | dt 0.11s\n",
            "epoch 000 | iter 00799 | train loss 2.5716 | dt 0.10s\n",
            "epoch 000 | iter 00999 | train loss 2.6071 | dt 0.10s\n",
            "epoch 000 | iter 01199 | train loss 2.6021 | dt 0.10s\n",
            "epoch 000 | iter 01399 | train loss 2.5783 | dt 0.10s\n",
            "epoch 000 | iter 01599 | train loss 2.5619 | dt 0.11s\n",
            "epoch 000 | iter 01799 | train loss 2.6055 | dt 0.10s\n",
            "epoch 000 | iter 01999 | train loss 2.5965 | dt 0.10s\n",
            "epoch 000 | iter 02199 | train loss 2.5952 | dt 0.11s\n",
            "epoch 000 | iter 02399 | train loss 2.5581 | dt 0.11s\n",
            "epoch 000 | iter 02599 | train loss 2.6095 | dt 0.10s\n",
            "epoch 000 | iter 02799 | train loss 2.5644 | dt 0.11s\n",
            "epoch 000 | iter 02999 | train loss 2.5973 | dt 0.10s\n",
            "epoch 000 | iter 03199 | train loss 2.5730 | dt 0.10s\n",
            "epoch 000 | iter 03399 | train loss 2.5874 | dt 0.10s\n",
            "epoch 000 | iter 03599 | train loss 2.6066 | dt 0.09s\n",
            "epoch 000 | iter 03799 | train loss 2.5815 | dt 0.10s\n",
            "epoch 000 | iter 03999 | train loss 2.6090 | dt 0.11s\n",
            "epoch 000 | iter 04199 | train loss 2.5569 | dt 0.10s\n",
            "epoch 000 | iter 04399 | train loss 2.5761 | dt 0.11s\n",
            "epoch 000 | iter 04599 | train loss 2.5882 | dt 0.10s\n",
            "epoch 000 | iter 04799 | train loss 2.5799 | dt 0.11s\n",
            "epoch 000 | iter 04999 | train loss 2.5927 | dt 0.10s\n",
            "epoch 000 | iter 05199 | train loss 2.5721 | dt 0.11s\n",
            "epoch 000 | iter 05399 | train loss 2.5886 | dt 0.10s\n",
            "epoch 000 | iter 05599 | train loss 2.5556 | dt 0.11s\n",
            "epoch 000 | iter 05799 | train loss 2.5878 | dt 0.11s\n",
            "epoch 000 | iter 05999 | train loss 2.6140 | dt 0.10s\n",
            "epoch 000 | iter 06199 | train loss 2.5771 | dt 0.10s\n",
            "epoch 000 | iter 06399 | train loss 2.5756 | dt 0.11s\n",
            "epoch 000 | iter 06599 | train loss 2.6047 | dt 0.10s\n",
            "epoch 000 | iter 06799 | train loss 2.5709 | dt 0.10s\n",
            "epoch 000 | iter 06999 | train loss 2.5699 | dt 0.11s\n",
            "epoch 000 | iter 07199 | train loss 2.5835 | dt 0.10s\n",
            "epoch 000 | iter 07399 | train loss 2.5936 | dt 0.09s\n",
            "epoch 000 | iter 07599 | train loss 2.5758 | dt 0.10s\n",
            "epoch 000 | iter 07799 | train loss 2.6134 | dt 0.11s\n",
            "epoch 000 | iter 07999 | train loss 2.5301 | dt 0.10s\n",
            "epoch 000 | iter 08199 | train loss 2.5831 | dt 0.11s\n",
            "epoch 000 | iter 08399 | train loss 2.6022 | dt 0.10s\n",
            "epoch 000 | iter 08599 | train loss 2.5475 | dt 0.10s\n",
            "epoch 000 | iter 08799 | train loss 2.5653 | dt 0.10s\n",
            "epoch 000 | iter 08999 | train loss 2.5885 | dt 0.10s\n",
            "epoch 000 | iter 09199 | train loss 2.5578 | dt 0.11s\n",
            "epoch 000 | iter 09399 | train loss 2.5704 | dt 0.10s\n",
            "epoch 000 | iter 09599 | train loss 2.5556 | dt 0.11s\n",
            "epoch 000 | iter 09799 | train loss 2.5960 | dt 0.11s\n",
            "epoch 000 | iter 09999 | train loss 2.5644 | dt 0.10s\n",
            "epoch 000 | iter 10199 | train loss 2.5740 | dt 0.11s\n",
            "epoch 000 | iter 10399 | train loss 2.5635 | dt 0.11s\n",
            "epoch 000 | iter 10599 | train loss 2.5858 | dt 0.11s\n",
            "epoch 000 | iter 10799 | train loss 2.6014 | dt 0.10s\n",
            "epoch 000 | iter 10999 | train loss 2.5467 | dt 0.10s\n",
            "epoch 000 | iter 11199 | train loss 2.4451 | dt 0.09s\n",
            "epoch 000 | iter 11399 | train loss 2.4838 | dt 0.10s\n",
            "epoch 000 | iter 11599 | train loss 2.5225 | dt 0.11s\n",
            "epoch 000 | iter 11799 | train loss 2.4878 | dt 0.10s\n",
            "epoch 000 | iter 11999 | train loss 2.5131 | dt 0.10s\n",
            "epoch 000 | iter 12199 | train loss 2.4713 | dt 0.10s\n",
            "[EPOCH 000] valid loss=1.9653 | valid ppl=7.14 | valid BLEU=0.4356 | epoch time=2258.9s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4356\n",
            "epoch 001 | iter 00199 | train loss 2.5325 | dt 0.10s\n",
            "epoch 001 | iter 00399 | train loss 2.5230 | dt 0.11s\n",
            "epoch 001 | iter 00599 | train loss 2.5603 | dt 0.11s\n",
            "epoch 001 | iter 00799 | train loss 2.5560 | dt 0.11s\n",
            "epoch 001 | iter 00999 | train loss 2.5211 | dt 0.11s\n",
            "epoch 001 | iter 01199 | train loss 2.5362 | dt 0.10s\n",
            "epoch 001 | iter 01399 | train loss 2.5440 | dt 0.10s\n",
            "epoch 001 | iter 01599 | train loss 2.5551 | dt 0.11s\n",
            "epoch 001 | iter 01799 | train loss 2.5501 | dt 0.10s\n",
            "epoch 001 | iter 01999 | train loss 2.5230 | dt 0.10s\n",
            "epoch 001 | iter 02199 | train loss 2.5197 | dt 0.09s\n",
            "epoch 001 | iter 02399 | train loss 2.5229 | dt 0.11s\n",
            "epoch 001 | iter 02599 | train loss 2.5243 | dt 0.10s\n",
            "epoch 001 | iter 02799 | train loss 2.4965 | dt 0.11s\n",
            "epoch 001 | iter 02999 | train loss 2.5162 | dt 0.11s\n",
            "epoch 001 | iter 03199 | train loss 2.5579 | dt 0.10s\n",
            "epoch 001 | iter 03399 | train loss 2.5637 | dt 0.10s\n",
            "epoch 001 | iter 03599 | train loss 2.5235 | dt 0.10s\n",
            "epoch 001 | iter 03799 | train loss 2.5192 | dt 0.10s\n",
            "epoch 001 | iter 03999 | train loss 2.5756 | dt 0.11s\n",
            "epoch 001 | iter 04199 | train loss 2.5173 | dt 0.10s\n",
            "epoch 001 | iter 04399 | train loss 2.5071 | dt 0.11s\n",
            "epoch 001 | iter 04599 | train loss 2.5487 | dt 0.11s\n",
            "epoch 001 | iter 04799 | train loss 2.5410 | dt 0.11s\n",
            "epoch 001 | iter 04999 | train loss 2.5825 | dt 0.10s\n",
            "epoch 001 | iter 05199 | train loss 2.5547 | dt 0.09s\n",
            "epoch 001 | iter 05399 | train loss 2.5241 | dt 0.10s\n",
            "epoch 001 | iter 05599 | train loss 2.5295 | dt 0.10s\n",
            "epoch 001 | iter 05799 | train loss 2.5408 | dt 0.10s\n",
            "epoch 001 | iter 05999 | train loss 2.5195 | dt 0.11s\n",
            "epoch 001 | iter 06199 | train loss 2.4853 | dt 0.10s\n",
            "epoch 001 | iter 06399 | train loss 2.5435 | dt 0.11s\n",
            "epoch 001 | iter 06599 | train loss 2.5333 | dt 0.10s\n",
            "epoch 001 | iter 06799 | train loss 2.5322 | dt 0.09s\n",
            "epoch 001 | iter 06999 | train loss 2.5082 | dt 0.10s\n",
            "epoch 001 | iter 07199 | train loss 2.5449 | dt 0.11s\n",
            "epoch 001 | iter 07399 | train loss 2.5422 | dt 0.10s\n",
            "epoch 001 | iter 07599 | train loss 2.5295 | dt 0.10s\n",
            "epoch 001 | iter 07799 | train loss 2.5253 | dt 0.11s\n",
            "epoch 001 | iter 07999 | train loss 2.5307 | dt 0.11s\n",
            "epoch 001 | iter 08199 | train loss 2.4845 | dt 0.11s\n",
            "epoch 001 | iter 08399 | train loss 2.5398 | dt 0.10s\n",
            "epoch 001 | iter 08599 | train loss 2.5520 | dt 0.11s\n",
            "epoch 001 | iter 08799 | train loss 2.5045 | dt 0.11s\n",
            "epoch 001 | iter 08999 | train loss 2.5176 | dt 0.10s\n",
            "epoch 001 | iter 09199 | train loss 2.5157 | dt 0.11s\n",
            "epoch 001 | iter 09399 | train loss 2.4959 | dt 0.08s\n",
            "epoch 001 | iter 09599 | train loss 2.5667 | dt 0.11s\n",
            "epoch 001 | iter 09799 | train loss 2.5630 | dt 0.11s\n",
            "epoch 001 | iter 09999 | train loss 2.5643 | dt 0.10s\n",
            "epoch 001 | iter 10199 | train loss 2.5233 | dt 0.10s\n",
            "epoch 001 | iter 10399 | train loss 2.5218 | dt 0.10s\n",
            "epoch 001 | iter 10599 | train loss 2.5729 | dt 0.10s\n",
            "epoch 001 | iter 10799 | train loss 2.5155 | dt 0.11s\n",
            "epoch 001 | iter 10999 | train loss 2.4990 | dt 0.11s\n",
            "epoch 001 | iter 11199 | train loss 2.4338 | dt 0.11s\n",
            "epoch 001 | iter 11399 | train loss 2.4203 | dt 0.10s\n",
            "epoch 001 | iter 11599 | train loss 2.4115 | dt 0.10s\n",
            "epoch 001 | iter 11799 | train loss 2.4555 | dt 0.10s\n",
            "epoch 001 | iter 11999 | train loss 2.4523 | dt 0.10s\n",
            "epoch 001 | iter 12199 | train loss 2.4400 | dt 0.10s\n",
            "[EPOCH 001] valid loss=1.9417 | valid ppl=6.97 | valid BLEU=0.4412 | epoch time=2261.7s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4412\n",
            "epoch 002 | iter 00199 | train loss 2.4784 | dt 0.10s\n",
            "epoch 002 | iter 00399 | train loss 2.5072 | dt 0.11s\n",
            "epoch 002 | iter 00599 | train loss 2.4722 | dt 0.10s\n",
            "epoch 002 | iter 00799 | train loss 2.4736 | dt 0.10s\n",
            "epoch 002 | iter 00999 | train loss 2.4946 | dt 0.10s\n",
            "epoch 002 | iter 01199 | train loss 2.4929 | dt 0.10s\n",
            "epoch 002 | iter 01399 | train loss 2.5003 | dt 0.11s\n",
            "epoch 002 | iter 01599 | train loss 2.5033 | dt 0.11s\n",
            "epoch 002 | iter 01799 | train loss 2.4949 | dt 0.11s\n",
            "epoch 002 | iter 01999 | train loss 2.4711 | dt 0.11s\n",
            "epoch 002 | iter 02199 | train loss 2.5123 | dt 0.10s\n",
            "epoch 002 | iter 02399 | train loss 2.5006 | dt 0.12s\n",
            "epoch 002 | iter 02599 | train loss 2.5272 | dt 0.11s\n",
            "epoch 002 | iter 02799 | train loss 2.5084 | dt 0.10s\n",
            "epoch 002 | iter 02999 | train loss 2.4967 | dt 0.09s\n",
            "epoch 002 | iter 03199 | train loss 2.4898 | dt 0.10s\n",
            "epoch 002 | iter 03399 | train loss 2.5243 | dt 0.10s\n",
            "epoch 002 | iter 03599 | train loss 2.4800 | dt 0.11s\n",
            "epoch 002 | iter 03799 | train loss 2.5188 | dt 0.11s\n",
            "epoch 002 | iter 03999 | train loss 2.4779 | dt 0.11s\n",
            "epoch 002 | iter 04199 | train loss 2.4475 | dt 0.10s\n",
            "epoch 002 | iter 04399 | train loss 2.4859 | dt 0.11s\n",
            "epoch 002 | iter 04599 | train loss 2.5058 | dt 0.10s\n",
            "epoch 002 | iter 04799 | train loss 2.4847 | dt 0.10s\n",
            "epoch 002 | iter 04999 | train loss 2.5021 | dt 0.10s\n",
            "epoch 002 | iter 05199 | train loss 2.4984 | dt 0.10s\n",
            "epoch 002 | iter 05399 | train loss 2.5124 | dt 0.10s\n",
            "epoch 002 | iter 05599 | train loss 2.4822 | dt 0.11s\n",
            "epoch 002 | iter 05799 | train loss 2.4710 | dt 0.10s\n",
            "epoch 002 | iter 05999 | train loss 2.4964 | dt 0.11s\n",
            "epoch 002 | iter 06199 | train loss 2.4662 | dt 0.10s\n",
            "epoch 002 | iter 06399 | train loss 2.4986 | dt 0.11s\n",
            "epoch 002 | iter 06599 | train loss 2.4838 | dt 0.10s\n",
            "epoch 002 | iter 06799 | train loss 2.4922 | dt 0.10s\n",
            "epoch 002 | iter 06999 | train loss 2.4919 | dt 0.10s\n",
            "epoch 002 | iter 07199 | train loss 2.5294 | dt 0.10s\n",
            "epoch 002 | iter 07399 | train loss 2.4590 | dt 0.10s\n",
            "epoch 002 | iter 07599 | train loss 2.4926 | dt 0.10s\n",
            "epoch 002 | iter 07799 | train loss 2.5257 | dt 0.11s\n",
            "epoch 002 | iter 07999 | train loss 2.4669 | dt 0.10s\n",
            "epoch 002 | iter 08199 | train loss 2.4987 | dt 0.11s\n",
            "epoch 002 | iter 08399 | train loss 2.5062 | dt 0.10s\n",
            "epoch 002 | iter 08599 | train loss 2.4727 | dt 0.10s\n",
            "epoch 002 | iter 08799 | train loss 2.5180 | dt 0.11s\n",
            "epoch 002 | iter 08999 | train loss 2.4944 | dt 0.11s\n",
            "epoch 002 | iter 09199 | train loss 2.4987 | dt 0.10s\n",
            "epoch 002 | iter 09399 | train loss 2.4788 | dt 0.10s\n",
            "epoch 002 | iter 09599 | train loss 2.5182 | dt 0.11s\n",
            "epoch 002 | iter 09799 | train loss 2.4834 | dt 0.10s\n",
            "epoch 002 | iter 09999 | train loss 2.4947 | dt 0.11s\n",
            "epoch 002 | iter 10199 | train loss 2.4949 | dt 0.11s\n",
            "epoch 002 | iter 10399 | train loss 2.4804 | dt 0.10s\n",
            "epoch 002 | iter 10599 | train loss 2.4899 | dt 0.11s\n",
            "epoch 002 | iter 10799 | train loss 2.5439 | dt 0.11s\n",
            "epoch 002 | iter 10999 | train loss 2.5017 | dt 0.10s\n",
            "epoch 002 | iter 11199 | train loss 2.3901 | dt 0.10s\n",
            "epoch 002 | iter 11399 | train loss 2.3782 | dt 0.09s\n",
            "epoch 002 | iter 11599 | train loss 2.3761 | dt 0.10s\n",
            "epoch 002 | iter 11799 | train loss 2.4110 | dt 0.10s\n",
            "epoch 002 | iter 11999 | train loss 2.4264 | dt 0.10s\n",
            "epoch 002 | iter 12199 | train loss 2.4339 | dt 0.10s\n",
            "[EPOCH 002] valid loss=1.9185 | valid ppl=6.81 | valid BLEU=0.4446 | epoch time=2256.7s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4446\n",
            "epoch 003 | iter 00199 | train loss 2.4690 | dt 0.10s\n",
            "epoch 003 | iter 00399 | train loss 2.4339 | dt 0.10s\n",
            "epoch 003 | iter 00599 | train loss 2.4482 | dt 0.10s\n",
            "epoch 003 | iter 00799 | train loss 2.4424 | dt 0.11s\n",
            "epoch 003 | iter 00999 | train loss 2.4651 | dt 0.11s\n",
            "epoch 003 | iter 01199 | train loss 2.4521 | dt 0.11s\n",
            "epoch 003 | iter 01399 | train loss 2.4618 | dt 0.10s\n",
            "epoch 003 | iter 01599 | train loss 2.4615 | dt 0.10s\n",
            "epoch 003 | iter 01799 | train loss 2.4560 | dt 0.10s\n",
            "epoch 003 | iter 01999 | train loss 2.4787 | dt 0.11s\n",
            "epoch 003 | iter 02199 | train loss 2.4397 | dt 0.10s\n",
            "epoch 003 | iter 02399 | train loss 2.4531 | dt 0.11s\n",
            "epoch 003 | iter 02599 | train loss 2.4677 | dt 0.11s\n",
            "epoch 003 | iter 02799 | train loss 2.4667 | dt 0.09s\n",
            "epoch 003 | iter 02999 | train loss 2.4583 | dt 0.10s\n",
            "epoch 003 | iter 03199 | train loss 2.4955 | dt 0.10s\n",
            "epoch 003 | iter 03399 | train loss 2.4689 | dt 0.10s\n",
            "epoch 003 | iter 03599 | train loss 2.4521 | dt 0.10s\n",
            "epoch 003 | iter 03799 | train loss 2.4596 | dt 0.10s\n",
            "epoch 003 | iter 03999 | train loss 2.4850 | dt 0.10s\n",
            "epoch 003 | iter 04199 | train loss 2.4281 | dt 0.10s\n",
            "epoch 003 | iter 04399 | train loss 2.4453 | dt 0.10s\n",
            "epoch 003 | iter 04599 | train loss 2.4721 | dt 0.11s\n",
            "epoch 003 | iter 04799 | train loss 2.4759 | dt 0.11s\n",
            "epoch 003 | iter 04999 | train loss 2.4662 | dt 0.11s\n",
            "epoch 003 | iter 05199 | train loss 2.4642 | dt 0.10s\n",
            "epoch 003 | iter 05399 | train loss 2.4632 | dt 0.10s\n",
            "epoch 003 | iter 05599 | train loss 2.4567 | dt 0.11s\n",
            "epoch 003 | iter 05799 | train loss 2.4553 | dt 0.11s\n",
            "epoch 003 | iter 05999 | train loss 2.4823 | dt 0.11s\n",
            "epoch 003 | iter 06199 | train loss 2.4951 | dt 0.10s\n",
            "epoch 003 | iter 06399 | train loss 2.4515 | dt 0.10s\n",
            "epoch 003 | iter 06599 | train loss 2.4686 | dt 0.11s\n",
            "epoch 003 | iter 06799 | train loss 2.4871 | dt 0.11s\n",
            "epoch 003 | iter 06999 | train loss 2.4346 | dt 0.11s\n",
            "epoch 003 | iter 07199 | train loss 2.4497 | dt 0.11s\n",
            "epoch 003 | iter 07399 | train loss 2.4518 | dt 0.11s\n",
            "epoch 003 | iter 07599 | train loss 2.4590 | dt 0.10s\n",
            "epoch 003 | iter 07799 | train loss 2.4973 | dt 0.10s\n",
            "epoch 003 | iter 07999 | train loss 2.4577 | dt 0.10s\n",
            "epoch 003 | iter 08199 | train loss 2.4641 | dt 0.10s\n",
            "epoch 003 | iter 08399 | train loss 2.4207 | dt 0.11s\n",
            "epoch 003 | iter 08599 | train loss 2.4324 | dt 0.10s\n",
            "epoch 003 | iter 08799 | train loss 2.4981 | dt 0.11s\n",
            "epoch 003 | iter 08999 | train loss 2.4707 | dt 0.10s\n",
            "epoch 003 | iter 09199 | train loss 2.4269 | dt 0.10s\n",
            "epoch 003 | iter 09399 | train loss 2.4622 | dt 0.11s\n",
            "epoch 003 | iter 09599 | train loss 2.4721 | dt 0.10s\n",
            "epoch 003 | iter 09799 | train loss 2.4800 | dt 0.10s\n",
            "epoch 003 | iter 09999 | train loss 2.4390 | dt 0.10s\n",
            "epoch 003 | iter 10199 | train loss 2.4543 | dt 0.10s\n",
            "epoch 003 | iter 10399 | train loss 2.4618 | dt 0.09s\n",
            "epoch 003 | iter 10599 | train loss 2.4801 | dt 0.10s\n",
            "epoch 003 | iter 10799 | train loss 2.4501 | dt 0.10s\n",
            "epoch 003 | iter 10999 | train loss 2.4472 | dt 0.10s\n",
            "epoch 003 | iter 11199 | train loss 2.3319 | dt 0.09s\n",
            "epoch 003 | iter 11399 | train loss 2.3773 | dt 0.10s\n",
            "epoch 003 | iter 11599 | train loss 2.3408 | dt 0.10s\n",
            "epoch 003 | iter 11799 | train loss 2.3552 | dt 0.11s\n",
            "epoch 003 | iter 11999 | train loss 2.3981 | dt 0.10s\n",
            "epoch 003 | iter 12199 | train loss 2.3947 | dt 0.10s\n",
            "[EPOCH 003] valid loss=1.9031 | valid ppl=6.71 | valid BLEU=0.4496 | epoch time=2257.3s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4496\n",
            "epoch 004 | iter 00199 | train loss 2.4097 | dt 0.11s\n",
            "epoch 004 | iter 00399 | train loss 2.4153 | dt 0.11s\n",
            "epoch 004 | iter 00599 | train loss 2.4199 | dt 0.10s\n",
            "epoch 004 | iter 00799 | train loss 2.4592 | dt 0.10s\n",
            "epoch 004 | iter 00999 | train loss 2.4261 | dt 0.10s\n",
            "epoch 004 | iter 01199 | train loss 2.4091 | dt 0.10s\n",
            "epoch 004 | iter 01399 | train loss 2.4262 | dt 0.10s\n",
            "epoch 004 | iter 01599 | train loss 2.4377 | dt 0.11s\n",
            "epoch 004 | iter 01799 | train loss 2.4257 | dt 0.10s\n",
            "epoch 004 | iter 01999 | train loss 2.4045 | dt 0.11s\n",
            "epoch 004 | iter 02199 | train loss 2.4606 | dt 0.11s\n",
            "epoch 004 | iter 02399 | train loss 2.4366 | dt 0.10s\n",
            "epoch 004 | iter 02599 | train loss 2.4335 | dt 0.11s\n",
            "epoch 004 | iter 02799 | train loss 2.3755 | dt 0.10s\n",
            "epoch 004 | iter 02999 | train loss 2.4151 | dt 0.10s\n",
            "epoch 004 | iter 03199 | train loss 2.4508 | dt 0.11s\n",
            "epoch 004 | iter 03399 | train loss 2.4441 | dt 0.10s\n",
            "epoch 004 | iter 03599 | train loss 2.4468 | dt 0.10s\n",
            "epoch 004 | iter 03799 | train loss 2.4348 | dt 0.11s\n",
            "epoch 004 | iter 03999 | train loss 2.4092 | dt 0.10s\n",
            "epoch 004 | iter 04199 | train loss 2.4441 | dt 0.10s\n",
            "epoch 004 | iter 04399 | train loss 2.4425 | dt 0.11s\n",
            "epoch 004 | iter 04599 | train loss 2.4369 | dt 0.11s\n",
            "epoch 004 | iter 04799 | train loss 2.4123 | dt 0.10s\n",
            "epoch 004 | iter 04999 | train loss 2.4259 | dt 0.11s\n",
            "epoch 004 | iter 05199 | train loss 2.4055 | dt 0.10s\n",
            "epoch 004 | iter 05399 | train loss 2.4339 | dt 0.11s\n",
            "epoch 004 | iter 05599 | train loss 2.4364 | dt 0.11s\n",
            "epoch 004 | iter 05799 | train loss 2.4426 | dt 0.10s\n",
            "epoch 004 | iter 05999 | train loss 2.4533 | dt 0.11s\n",
            "epoch 004 | iter 06199 | train loss 2.4112 | dt 0.10s\n",
            "epoch 004 | iter 06399 | train loss 2.4586 | dt 0.11s\n",
            "epoch 004 | iter 06599 | train loss 2.4550 | dt 0.11s\n",
            "epoch 004 | iter 06799 | train loss 2.4441 | dt 0.10s\n",
            "epoch 004 | iter 06999 | train loss 2.4145 | dt 0.10s\n",
            "epoch 004 | iter 07199 | train loss 2.4472 | dt 0.10s\n",
            "epoch 004 | iter 07399 | train loss 2.3805 | dt 0.10s\n",
            "epoch 004 | iter 07599 | train loss 2.4429 | dt 0.10s\n",
            "epoch 004 | iter 07799 | train loss 2.4532 | dt 0.10s\n",
            "epoch 004 | iter 07999 | train loss 2.4312 | dt 0.10s\n",
            "epoch 004 | iter 08199 | train loss 2.4222 | dt 0.11s\n",
            "epoch 004 | iter 08399 | train loss 2.4367 | dt 0.10s\n",
            "epoch 004 | iter 08599 | train loss 2.4464 | dt 0.10s\n",
            "epoch 004 | iter 08799 | train loss 2.4381 | dt 0.10s\n",
            "epoch 004 | iter 08999 | train loss 2.4294 | dt 0.11s\n",
            "epoch 004 | iter 09199 | train loss 2.4522 | dt 0.10s\n",
            "epoch 004 | iter 09399 | train loss 2.4338 | dt 0.10s\n",
            "epoch 004 | iter 09599 | train loss 2.4338 | dt 0.11s\n",
            "epoch 004 | iter 09799 | train loss 2.3885 | dt 0.11s\n",
            "epoch 004 | iter 09999 | train loss 2.4335 | dt 0.10s\n",
            "epoch 004 | iter 10199 | train loss 2.4244 | dt 0.10s\n",
            "epoch 004 | iter 10399 | train loss 2.4152 | dt 0.10s\n",
            "epoch 004 | iter 10599 | train loss 2.4239 | dt 0.10s\n",
            "epoch 004 | iter 10799 | train loss 2.4458 | dt 0.10s\n",
            "epoch 004 | iter 10999 | train loss 2.4423 | dt 0.10s\n",
            "epoch 004 | iter 11199 | train loss 2.3199 | dt 0.10s\n",
            "epoch 004 | iter 11399 | train loss 2.3673 | dt 0.10s\n",
            "epoch 004 | iter 11599 | train loss 2.3327 | dt 0.11s\n",
            "epoch 004 | iter 11799 | train loss 2.3573 | dt 0.10s\n",
            "epoch 004 | iter 11999 | train loss 2.3720 | dt 0.10s\n",
            "epoch 004 | iter 12199 | train loss 2.3671 | dt 0.10s\n",
            "[EPOCH 004] valid loss=1.8849 | valid ppl=6.59 | valid BLEU=0.4523 | epoch time=2253.4s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4523\n",
            "epoch 005 | iter 00199 | train loss 2.4034 | dt 0.11s\n",
            "epoch 005 | iter 00399 | train loss 2.4142 | dt 0.09s\n",
            "epoch 005 | iter 00599 | train loss 2.4109 | dt 0.10s\n",
            "epoch 005 | iter 00799 | train loss 2.3905 | dt 0.10s\n",
            "epoch 005 | iter 00999 | train loss 2.4185 | dt 0.10s\n",
            "epoch 005 | iter 01199 | train loss 2.4027 | dt 0.10s\n",
            "epoch 005 | iter 01399 | train loss 2.4232 | dt 0.11s\n",
            "epoch 005 | iter 01599 | train loss 2.4213 | dt 0.10s\n",
            "epoch 005 | iter 01799 | train loss 2.4079 | dt 0.11s\n",
            "epoch 005 | iter 01999 | train loss 2.4015 | dt 0.10s\n",
            "epoch 005 | iter 02199 | train loss 2.4310 | dt 0.11s\n",
            "epoch 005 | iter 02399 | train loss 2.4159 | dt 0.11s\n",
            "epoch 005 | iter 02599 | train loss 2.3814 | dt 0.11s\n",
            "epoch 005 | iter 02799 | train loss 2.3733 | dt 0.10s\n",
            "epoch 005 | iter 02999 | train loss 2.3818 | dt 0.10s\n",
            "epoch 005 | iter 03199 | train loss 2.4186 | dt 0.10s\n",
            "epoch 005 | iter 03399 | train loss 2.4146 | dt 0.10s\n",
            "epoch 005 | iter 03599 | train loss 2.3769 | dt 0.10s\n",
            "epoch 005 | iter 03799 | train loss 2.3861 | dt 0.11s\n",
            "epoch 005 | iter 03999 | train loss 2.4006 | dt 0.10s\n",
            "epoch 005 | iter 04199 | train loss 2.3868 | dt 0.10s\n",
            "epoch 005 | iter 04399 | train loss 2.4060 | dt 0.11s\n",
            "epoch 005 | iter 04599 | train loss 2.3992 | dt 0.10s\n",
            "epoch 005 | iter 04799 | train loss 2.4163 | dt 0.10s\n",
            "epoch 005 | iter 04999 | train loss 2.4244 | dt 0.10s\n",
            "epoch 005 | iter 05199 | train loss 2.3946 | dt 0.10s\n",
            "epoch 005 | iter 05399 | train loss 2.4141 | dt 0.10s\n",
            "epoch 005 | iter 05599 | train loss 2.4134 | dt 0.11s\n",
            "epoch 005 | iter 05799 | train loss 2.3954 | dt 0.11s\n",
            "epoch 005 | iter 05999 | train loss 2.3997 | dt 0.10s\n",
            "epoch 005 | iter 06199 | train loss 2.4076 | dt 0.10s\n",
            "epoch 005 | iter 06399 | train loss 2.4316 | dt 0.10s\n",
            "epoch 005 | iter 06599 | train loss 2.3871 | dt 0.10s\n",
            "epoch 005 | iter 06799 | train loss 2.4361 | dt 0.11s\n",
            "epoch 005 | iter 06999 | train loss 2.4267 | dt 0.10s\n",
            "epoch 005 | iter 07199 | train loss 2.3908 | dt 0.10s\n",
            "epoch 005 | iter 07399 | train loss 2.4407 | dt 0.10s\n",
            "epoch 005 | iter 07599 | train loss 2.4016 | dt 0.10s\n",
            "epoch 005 | iter 07799 | train loss 2.4434 | dt 0.10s\n",
            "epoch 005 | iter 07999 | train loss 2.4155 | dt 0.11s\n",
            "epoch 005 | iter 08199 | train loss 2.4027 | dt 0.10s\n",
            "epoch 005 | iter 08399 | train loss 2.4106 | dt 0.10s\n",
            "epoch 005 | iter 08599 | train loss 2.4256 | dt 0.10s\n",
            "epoch 005 | iter 08799 | train loss 2.4088 | dt 0.10s\n",
            "epoch 005 | iter 08999 | train loss 2.4258 | dt 0.10s\n",
            "epoch 005 | iter 09199 | train loss 2.3997 | dt 0.10s\n",
            "epoch 005 | iter 09399 | train loss 2.4274 | dt 0.10s\n",
            "epoch 005 | iter 09599 | train loss 2.4037 | dt 0.11s\n",
            "epoch 005 | iter 09799 | train loss 2.4070 | dt 0.11s\n",
            "epoch 005 | iter 09999 | train loss 2.3865 | dt 0.10s\n",
            "epoch 005 | iter 10199 | train loss 2.3976 | dt 0.11s\n",
            "epoch 005 | iter 10399 | train loss 2.4118 | dt 0.11s\n",
            "epoch 005 | iter 10599 | train loss 2.3874 | dt 0.10s\n",
            "epoch 005 | iter 10799 | train loss 2.4190 | dt 0.10s\n",
            "epoch 005 | iter 10999 | train loss 2.3848 | dt 0.10s\n",
            "epoch 005 | iter 11199 | train loss 2.3506 | dt 0.09s\n",
            "epoch 005 | iter 11399 | train loss 2.2912 | dt 0.10s\n",
            "epoch 005 | iter 11599 | train loss 2.3513 | dt 0.10s\n",
            "epoch 005 | iter 11799 | train loss 2.3186 | dt 0.10s\n",
            "epoch 005 | iter 11999 | train loss 2.3023 | dt 0.10s\n",
            "epoch 005 | iter 12199 | train loss 2.3308 | dt 0.10s\n",
            "[EPOCH 005] valid loss=1.8680 | valid ppl=6.48 | valid BLEU=0.4556 | epoch time=2250.1s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4556\n",
            "epoch 006 | iter 00199 | train loss 2.3575 | dt 0.10s\n",
            "epoch 006 | iter 00399 | train loss 2.3613 | dt 0.11s\n",
            "epoch 006 | iter 00599 | train loss 2.3661 | dt 0.10s\n",
            "epoch 006 | iter 00799 | train loss 2.3974 | dt 0.10s\n",
            "epoch 006 | iter 00999 | train loss 2.3657 | dt 0.10s\n",
            "epoch 006 | iter 01199 | train loss 2.3788 | dt 0.10s\n",
            "epoch 006 | iter 01399 | train loss 2.3513 | dt 0.10s\n",
            "epoch 006 | iter 01599 | train loss 2.3718 | dt 0.09s\n",
            "epoch 006 | iter 01799 | train loss 2.3847 | dt 0.10s\n",
            "epoch 006 | iter 01999 | train loss 2.3445 | dt 0.10s\n",
            "epoch 006 | iter 02199 | train loss 2.3993 | dt 0.09s\n",
            "epoch 006 | iter 02399 | train loss 2.3579 | dt 0.10s\n",
            "epoch 006 | iter 02599 | train loss 2.3902 | dt 0.10s\n",
            "epoch 006 | iter 02799 | train loss 2.4045 | dt 0.10s\n",
            "epoch 006 | iter 02999 | train loss 2.4031 | dt 0.10s\n",
            "epoch 006 | iter 03199 | train loss 2.4150 | dt 0.10s\n",
            "epoch 006 | iter 03399 | train loss 2.3818 | dt 0.10s\n",
            "epoch 006 | iter 03599 | train loss 2.4261 | dt 0.10s\n",
            "epoch 006 | iter 03799 | train loss 2.3909 | dt 0.11s\n",
            "epoch 006 | iter 03999 | train loss 2.4143 | dt 0.11s\n",
            "epoch 006 | iter 04199 | train loss 2.3624 | dt 0.10s\n",
            "epoch 006 | iter 04399 | train loss 2.4015 | dt 0.10s\n",
            "epoch 006 | iter 04599 | train loss 2.3674 | dt 0.11s\n",
            "epoch 006 | iter 04799 | train loss 2.3890 | dt 0.10s\n",
            "epoch 006 | iter 04999 | train loss 2.3916 | dt 0.11s\n",
            "epoch 006 | iter 05199 | train loss 2.3701 | dt 0.11s\n",
            "epoch 006 | iter 05399 | train loss 2.3847 | dt 0.10s\n",
            "epoch 006 | iter 05599 | train loss 2.4116 | dt 0.10s\n",
            "epoch 006 | iter 05799 | train loss 2.3693 | dt 0.11s\n",
            "epoch 006 | iter 05999 | train loss 2.3530 | dt 0.10s\n",
            "epoch 006 | iter 06199 | train loss 2.3740 | dt 0.10s\n",
            "epoch 006 | iter 06399 | train loss 2.3993 | dt 0.10s\n",
            "epoch 006 | iter 06599 | train loss 2.4185 | dt 0.11s\n",
            "epoch 006 | iter 06799 | train loss 2.3707 | dt 0.11s\n",
            "epoch 006 | iter 06999 | train loss 2.3669 | dt 0.10s\n",
            "epoch 006 | iter 07199 | train loss 2.3587 | dt 0.10s\n",
            "epoch 006 | iter 07399 | train loss 2.3897 | dt 0.09s\n",
            "epoch 006 | iter 07599 | train loss 2.3980 | dt 0.10s\n",
            "epoch 006 | iter 07799 | train loss 2.3952 | dt 0.10s\n",
            "epoch 006 | iter 07999 | train loss 2.3970 | dt 0.10s\n",
            "epoch 006 | iter 08199 | train loss 2.3838 | dt 0.10s\n",
            "epoch 006 | iter 08399 | train loss 2.3637 | dt 0.10s\n",
            "epoch 006 | iter 08599 | train loss 2.3643 | dt 0.10s\n",
            "epoch 006 | iter 08799 | train loss 2.3985 | dt 0.11s\n",
            "epoch 006 | iter 08999 | train loss 2.3944 | dt 0.10s\n",
            "epoch 006 | iter 09199 | train loss 2.3975 | dt 0.10s\n",
            "epoch 006 | iter 09399 | train loss 2.3652 | dt 0.11s\n",
            "epoch 006 | iter 09599 | train loss 2.3996 | dt 0.12s\n",
            "epoch 006 | iter 09799 | train loss 2.4075 | dt 0.10s\n",
            "epoch 006 | iter 09999 | train loss 2.4053 | dt 0.11s\n",
            "epoch 006 | iter 10199 | train loss 2.3859 | dt 0.10s\n",
            "epoch 006 | iter 10399 | train loss 2.3792 | dt 0.11s\n",
            "epoch 006 | iter 10599 | train loss 2.3729 | dt 0.11s\n",
            "epoch 006 | iter 10799 | train loss 2.3881 | dt 0.09s\n",
            "epoch 006 | iter 10999 | train loss 2.3791 | dt 0.10s\n",
            "epoch 006 | iter 11199 | train loss 2.2950 | dt 0.10s\n",
            "epoch 006 | iter 11399 | train loss 2.3050 | dt 0.10s\n",
            "epoch 006 | iter 11599 | train loss 2.3325 | dt 0.10s\n",
            "epoch 006 | iter 11799 | train loss 2.3228 | dt 0.10s\n",
            "epoch 006 | iter 11999 | train loss 2.2942 | dt 0.11s\n",
            "epoch 006 | iter 12199 | train loss 2.2493 | dt 0.10s\n",
            "[EPOCH 006] valid loss=1.8584 | valid ppl=6.41 | valid BLEU=0.4613 | epoch time=2253.9s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4613\n",
            "epoch 007 | iter 00199 | train loss 2.3415 | dt 0.10s\n",
            "epoch 007 | iter 00399 | train loss 2.3818 | dt 0.10s\n",
            "epoch 007 | iter 00599 | train loss 2.3460 | dt 0.10s\n",
            "epoch 007 | iter 00799 | train loss 2.3394 | dt 0.10s\n",
            "epoch 007 | iter 00999 | train loss 2.3692 | dt 0.11s\n",
            "epoch 007 | iter 01199 | train loss 2.3562 | dt 0.09s\n",
            "epoch 007 | iter 01399 | train loss 2.3229 | dt 0.10s\n",
            "epoch 007 | iter 01599 | train loss 2.3391 | dt 0.10s\n",
            "epoch 007 | iter 01799 | train loss 2.3690 | dt 0.11s\n",
            "epoch 007 | iter 01999 | train loss 2.3626 | dt 0.11s\n",
            "epoch 007 | iter 02199 | train loss 2.3599 | dt 0.11s\n",
            "epoch 007 | iter 02399 | train loss 2.3477 | dt 0.09s\n",
            "epoch 007 | iter 02599 | train loss 2.3680 | dt 0.10s\n",
            "epoch 007 | iter 02799 | train loss 2.3379 | dt 0.10s\n",
            "epoch 007 | iter 02999 | train loss 2.3830 | dt 0.11s\n",
            "epoch 007 | iter 03199 | train loss 2.3528 | dt 0.10s\n",
            "epoch 007 | iter 03399 | train loss 2.3627 | dt 0.10s\n",
            "epoch 007 | iter 03599 | train loss 2.3896 | dt 0.11s\n",
            "epoch 007 | iter 03799 | train loss 2.3775 | dt 0.10s\n",
            "epoch 007 | iter 03999 | train loss 2.3294 | dt 0.10s\n",
            "epoch 007 | iter 04199 | train loss 2.3407 | dt 0.11s\n",
            "epoch 007 | iter 04399 | train loss 2.3677 | dt 0.10s\n",
            "epoch 007 | iter 04599 | train loss 2.3690 | dt 0.11s\n",
            "epoch 007 | iter 04799 | train loss 2.3395 | dt 0.11s\n",
            "epoch 007 | iter 04999 | train loss 2.3529 | dt 0.10s\n",
            "epoch 007 | iter 05199 | train loss 2.3474 | dt 0.11s\n",
            "epoch 007 | iter 05399 | train loss 2.3796 | dt 0.11s\n",
            "epoch 007 | iter 05599 | train loss 2.3968 | dt 0.11s\n",
            "epoch 007 | iter 05799 | train loss 2.3886 | dt 0.11s\n",
            "epoch 007 | iter 05999 | train loss 2.3770 | dt 0.10s\n",
            "epoch 007 | iter 06199 | train loss 2.3707 | dt 0.09s\n",
            "epoch 007 | iter 06399 | train loss 2.3759 | dt 0.10s\n",
            "epoch 007 | iter 06599 | train loss 2.3730 | dt 0.11s\n",
            "epoch 007 | iter 06799 | train loss 2.3649 | dt 0.10s\n",
            "epoch 007 | iter 06999 | train loss 2.3701 | dt 0.10s\n",
            "epoch 007 | iter 07199 | train loss 2.3531 | dt 0.11s\n",
            "epoch 007 | iter 07399 | train loss 2.3561 | dt 0.10s\n",
            "epoch 007 | iter 07599 | train loss 2.3729 | dt 0.11s\n",
            "epoch 007 | iter 07799 | train loss 2.3679 | dt 0.11s\n",
            "epoch 007 | iter 07999 | train loss 2.3783 | dt 0.10s\n",
            "epoch 007 | iter 08199 | train loss 2.3401 | dt 0.10s\n",
            "epoch 007 | iter 08399 | train loss 2.3661 | dt 0.10s\n",
            "epoch 007 | iter 08599 | train loss 2.3694 | dt 0.10s\n",
            "epoch 007 | iter 08799 | train loss 2.3725 | dt 0.11s\n",
            "epoch 007 | iter 08999 | train loss 2.3786 | dt 0.09s\n",
            "epoch 007 | iter 09199 | train loss 2.3633 | dt 0.10s\n",
            "epoch 007 | iter 09399 | train loss 2.3819 | dt 0.11s\n",
            "epoch 007 | iter 09599 | train loss 2.3911 | dt 0.10s\n",
            "epoch 007 | iter 09799 | train loss 2.3690 | dt 0.10s\n",
            "epoch 007 | iter 09999 | train loss 2.3480 | dt 0.10s\n",
            "epoch 007 | iter 10199 | train loss 2.3519 | dt 0.11s\n",
            "epoch 007 | iter 10399 | train loss 2.3432 | dt 0.11s\n",
            "epoch 007 | iter 10599 | train loss 2.3632 | dt 0.10s\n",
            "epoch 007 | iter 10799 | train loss 2.3808 | dt 0.10s\n",
            "epoch 007 | iter 10999 | train loss 2.3608 | dt 0.10s\n",
            "epoch 007 | iter 11199 | train loss 2.2638 | dt 0.10s\n",
            "epoch 007 | iter 11399 | train loss 2.2659 | dt 0.11s\n",
            "epoch 007 | iter 11599 | train loss 2.3182 | dt 0.11s\n",
            "epoch 007 | iter 11799 | train loss 2.3214 | dt 0.10s\n",
            "epoch 007 | iter 11999 | train loss 2.2786 | dt 0.09s\n",
            "epoch 007 | iter 12199 | train loss 2.2693 | dt 0.10s\n",
            "[EPOCH 007] valid loss=1.8488 | valid ppl=6.35 | valid BLEU=0.4619 | epoch time=2251.2s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4619\n",
            "epoch 008 | iter 00199 | train loss 2.3405 | dt 0.11s\n",
            "epoch 008 | iter 00399 | train loss 2.3402 | dt 0.10s\n",
            "epoch 008 | iter 00599 | train loss 2.3457 | dt 0.11s\n",
            "epoch 008 | iter 00799 | train loss 2.3384 | dt 0.10s\n",
            "epoch 008 | iter 00999 | train loss 2.3456 | dt 0.11s\n",
            "epoch 008 | iter 01199 | train loss 2.3514 | dt 0.10s\n",
            "epoch 008 | iter 01399 | train loss 2.3245 | dt 0.10s\n",
            "epoch 008 | iter 01599 | train loss 2.3724 | dt 0.10s\n",
            "epoch 008 | iter 01799 | train loss 2.3457 | dt 0.11s\n",
            "epoch 008 | iter 01999 | train loss 2.3353 | dt 0.10s\n",
            "epoch 008 | iter 02199 | train loss 2.3479 | dt 0.10s\n",
            "epoch 008 | iter 02399 | train loss 2.3731 | dt 0.10s\n",
            "epoch 008 | iter 02599 | train loss 2.3267 | dt 0.10s\n",
            "epoch 008 | iter 02799 | train loss 2.3472 | dt 0.10s\n",
            "epoch 008 | iter 02999 | train loss 2.3144 | dt 0.10s\n",
            "epoch 008 | iter 03199 | train loss 2.3209 | dt 0.10s\n",
            "epoch 008 | iter 03399 | train loss 2.3326 | dt 0.10s\n",
            "epoch 008 | iter 03599 | train loss 2.3487 | dt 0.10s\n",
            "epoch 008 | iter 03799 | train loss 2.3812 | dt 0.11s\n",
            "epoch 008 | iter 03999 | train loss 2.3446 | dt 0.10s\n",
            "epoch 008 | iter 04199 | train loss 2.3750 | dt 0.10s\n",
            "epoch 008 | iter 04399 | train loss 2.3478 | dt 0.10s\n",
            "epoch 008 | iter 04599 | train loss 2.3414 | dt 0.11s\n",
            "epoch 008 | iter 04799 | train loss 2.3379 | dt 0.10s\n",
            "epoch 008 | iter 04999 | train loss 2.3310 | dt 0.10s\n",
            "epoch 008 | iter 05199 | train loss 2.3461 | dt 0.10s\n",
            "epoch 008 | iter 05399 | train loss 2.3658 | dt 0.11s\n",
            "epoch 008 | iter 05599 | train loss 2.3310 | dt 0.11s\n",
            "epoch 008 | iter 05799 | train loss 2.3687 | dt 0.11s\n",
            "epoch 008 | iter 05999 | train loss 2.3432 | dt 0.10s\n",
            "epoch 008 | iter 06199 | train loss 2.3502 | dt 0.11s\n",
            "epoch 008 | iter 06399 | train loss 2.3585 | dt 0.10s\n",
            "epoch 008 | iter 06599 | train loss 2.3360 | dt 0.10s\n",
            "epoch 008 | iter 06799 | train loss 2.3466 | dt 0.10s\n",
            "epoch 008 | iter 06999 | train loss 2.3509 | dt 0.10s\n",
            "epoch 008 | iter 07199 | train loss 2.3293 | dt 0.11s\n",
            "epoch 008 | iter 07399 | train loss 2.3507 | dt 0.10s\n",
            "epoch 008 | iter 07599 | train loss 2.3342 | dt 0.10s\n",
            "epoch 008 | iter 07799 | train loss 2.3496 | dt 0.10s\n",
            "epoch 008 | iter 07999 | train loss 2.3419 | dt 0.09s\n",
            "epoch 008 | iter 08199 | train loss 2.3711 | dt 0.10s\n",
            "epoch 008 | iter 08399 | train loss 2.3533 | dt 0.10s\n",
            "epoch 008 | iter 08599 | train loss 2.3352 | dt 0.09s\n",
            "epoch 008 | iter 08799 | train loss 2.3448 | dt 0.11s\n",
            "epoch 008 | iter 08999 | train loss 2.3651 | dt 0.10s\n",
            "epoch 008 | iter 09199 | train loss 2.3577 | dt 0.10s\n",
            "epoch 008 | iter 09399 | train loss 2.3205 | dt 0.10s\n",
            "epoch 008 | iter 09599 | train loss 2.3455 | dt 0.10s\n",
            "epoch 008 | iter 09799 | train loss 2.3357 | dt 0.11s\n",
            "epoch 008 | iter 09999 | train loss 2.3552 | dt 0.10s\n",
            "epoch 008 | iter 10199 | train loss 2.3412 | dt 0.10s\n",
            "epoch 008 | iter 10399 | train loss 2.3131 | dt 0.10s\n",
            "epoch 008 | iter 10599 | train loss 2.3497 | dt 0.11s\n",
            "epoch 008 | iter 10799 | train loss 2.3725 | dt 0.11s\n",
            "epoch 008 | iter 10999 | train loss 2.3524 | dt 0.11s\n",
            "epoch 008 | iter 11199 | train loss 2.3169 | dt 0.10s\n",
            "epoch 008 | iter 11399 | train loss 2.2526 | dt 0.11s\n",
            "epoch 008 | iter 11599 | train loss 2.2794 | dt 0.09s\n",
            "epoch 008 | iter 11799 | train loss 2.2975 | dt 0.11s\n",
            "epoch 008 | iter 11999 | train loss 2.2371 | dt 0.11s\n",
            "epoch 008 | iter 12199 | train loss 2.2280 | dt 0.10s\n",
            "[EPOCH 008] valid loss=1.8383 | valid ppl=6.29 | valid BLEU=0.4639 | epoch time=2255.7s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4639\n",
            "epoch 009 | iter 00199 | train loss 2.3093 | dt 0.09s\n",
            "epoch 009 | iter 00399 | train loss 2.3332 | dt 0.10s\n",
            "epoch 009 | iter 00599 | train loss 2.2989 | dt 0.10s\n",
            "epoch 009 | iter 00799 | train loss 2.2973 | dt 0.11s\n",
            "epoch 009 | iter 00999 | train loss 2.3474 | dt 0.11s\n",
            "epoch 009 | iter 01199 | train loss 2.3057 | dt 0.11s\n",
            "epoch 009 | iter 01399 | train loss 2.3459 | dt 0.11s\n",
            "epoch 009 | iter 01599 | train loss 2.3331 | dt 0.10s\n",
            "epoch 009 | iter 01799 | train loss 2.3145 | dt 0.11s\n",
            "epoch 009 | iter 01999 | train loss 2.3323 | dt 0.11s\n",
            "epoch 009 | iter 02199 | train loss 2.3348 | dt 0.11s\n",
            "epoch 009 | iter 02399 | train loss 2.3202 | dt 0.09s\n",
            "epoch 009 | iter 02599 | train loss 2.3329 | dt 0.11s\n",
            "epoch 009 | iter 02799 | train loss 2.3232 | dt 0.11s\n",
            "epoch 009 | iter 02999 | train loss 2.3198 | dt 0.10s\n",
            "epoch 009 | iter 03199 | train loss 2.3206 | dt 0.10s\n",
            "epoch 009 | iter 03399 | train loss 2.3088 | dt 0.10s\n",
            "epoch 009 | iter 03599 | train loss 2.3248 | dt 0.10s\n",
            "epoch 009 | iter 03799 | train loss 2.3347 | dt 0.09s\n",
            "epoch 009 | iter 03999 | train loss 2.2988 | dt 0.10s\n",
            "epoch 009 | iter 04199 | train loss 2.3190 | dt 0.10s\n",
            "epoch 009 | iter 04399 | train loss 2.3437 | dt 0.10s\n",
            "epoch 009 | iter 04599 | train loss 2.3468 | dt 0.11s\n",
            "epoch 009 | iter 04799 | train loss 2.3510 | dt 0.10s\n",
            "epoch 009 | iter 04999 | train loss 2.3170 | dt 0.10s\n",
            "epoch 009 | iter 05199 | train loss 2.3245 | dt 0.10s\n",
            "epoch 009 | iter 05399 | train loss 2.3458 | dt 0.10s\n",
            "epoch 009 | iter 05599 | train loss 2.3243 | dt 0.10s\n",
            "epoch 009 | iter 05799 | train loss 2.3117 | dt 0.11s\n",
            "epoch 009 | iter 05999 | train loss 2.3475 | dt 0.11s\n",
            "epoch 009 | iter 06199 | train loss 2.3015 | dt 0.10s\n",
            "epoch 009 | iter 06399 | train loss 2.3036 | dt 0.11s\n",
            "epoch 009 | iter 06599 | train loss 2.3597 | dt 0.11s\n",
            "epoch 009 | iter 06799 | train loss 2.3502 | dt 0.10s\n",
            "epoch 009 | iter 06999 | train loss 2.3431 | dt 0.11s\n",
            "epoch 009 | iter 07199 | train loss 2.3416 | dt 0.11s\n",
            "epoch 009 | iter 07399 | train loss 2.3344 | dt 0.11s\n",
            "epoch 009 | iter 07599 | train loss 2.3456 | dt 0.10s\n",
            "epoch 009 | iter 07799 | train loss 2.3247 | dt 0.10s\n",
            "epoch 009 | iter 07999 | train loss 2.3165 | dt 0.10s\n",
            "epoch 009 | iter 08199 | train loss 2.3410 | dt 0.11s\n",
            "epoch 009 | iter 08399 | train loss 2.3188 | dt 0.09s\n",
            "epoch 009 | iter 08599 | train loss 2.3053 | dt 0.11s\n",
            "epoch 009 | iter 08799 | train loss 2.3534 | dt 0.10s\n",
            "epoch 009 | iter 08999 | train loss 2.3430 | dt 0.10s\n",
            "epoch 009 | iter 09199 | train loss 2.3321 | dt 0.10s\n",
            "epoch 009 | iter 09399 | train loss 2.3187 | dt 0.11s\n",
            "epoch 009 | iter 09599 | train loss 2.3405 | dt 0.10s\n",
            "epoch 009 | iter 09799 | train loss 2.3184 | dt 0.11s\n",
            "epoch 009 | iter 09999 | train loss 2.3306 | dt 0.11s\n",
            "epoch 009 | iter 10199 | train loss 2.3399 | dt 0.10s\n",
            "epoch 009 | iter 10399 | train loss 2.3525 | dt 0.11s\n",
            "epoch 009 | iter 10599 | train loss 2.3320 | dt 0.10s\n",
            "epoch 009 | iter 10799 | train loss 2.3243 | dt 0.10s\n",
            "epoch 009 | iter 10999 | train loss 2.3443 | dt 0.10s\n",
            "epoch 009 | iter 11199 | train loss 2.2817 | dt 0.10s\n",
            "epoch 009 | iter 11399 | train loss 2.2750 | dt 0.10s\n",
            "epoch 009 | iter 11599 | train loss 2.2429 | dt 0.11s\n",
            "epoch 009 | iter 11799 | train loss 2.2216 | dt 0.10s\n",
            "epoch 009 | iter 11999 | train loss 2.2661 | dt 0.11s\n",
            "epoch 009 | iter 12199 | train loss 2.2567 | dt 0.10s\n",
            "[EPOCH 009] valid loss=1.8279 | valid ppl=6.22 | valid BLEU=0.4640 | epoch time=2267.2s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4640\n",
            "epoch 010 | iter 00199 | train loss 2.2886 | dt 0.11s\n",
            "epoch 010 | iter 00399 | train loss 2.3020 | dt 0.10s\n",
            "epoch 010 | iter 00599 | train loss 2.3259 | dt 0.11s\n",
            "epoch 010 | iter 00799 | train loss 2.3023 | dt 0.10s\n",
            "epoch 010 | iter 00999 | train loss 2.3067 | dt 0.10s\n",
            "epoch 010 | iter 01199 | train loss 2.3171 | dt 0.10s\n",
            "epoch 010 | iter 01399 | train loss 2.3105 | dt 0.10s\n",
            "epoch 010 | iter 01599 | train loss 2.2987 | dt 0.10s\n",
            "epoch 010 | iter 01799 | train loss 2.3100 | dt 0.11s\n",
            "epoch 010 | iter 01999 | train loss 2.2882 | dt 0.10s\n",
            "epoch 010 | iter 02199 | train loss 2.3364 | dt 0.11s\n",
            "epoch 010 | iter 02399 | train loss 2.3253 | dt 0.11s\n",
            "epoch 010 | iter 02599 | train loss 2.3071 | dt 0.10s\n",
            "epoch 010 | iter 02799 | train loss 2.3319 | dt 0.10s\n",
            "epoch 010 | iter 02999 | train loss 2.2987 | dt 0.09s\n",
            "epoch 010 | iter 03199 | train loss 2.3198 | dt 0.11s\n",
            "epoch 010 | iter 03399 | train loss 2.3280 | dt 0.09s\n",
            "epoch 010 | iter 03599 | train loss 2.3090 | dt 0.10s\n",
            "epoch 010 | iter 03799 | train loss 2.3297 | dt 0.10s\n",
            "epoch 010 | iter 03999 | train loss 2.2975 | dt 0.10s\n",
            "epoch 010 | iter 04199 | train loss 2.3177 | dt 0.10s\n",
            "epoch 010 | iter 04399 | train loss 2.2978 | dt 0.10s\n",
            "epoch 010 | iter 04599 | train loss 2.2965 | dt 0.09s\n",
            "epoch 010 | iter 04799 | train loss 2.3131 | dt 0.10s\n",
            "epoch 010 | iter 04999 | train loss 2.3218 | dt 0.10s\n",
            "epoch 010 | iter 05199 | train loss 2.3205 | dt 0.10s\n",
            "epoch 010 | iter 05399 | train loss 2.3017 | dt 0.10s\n",
            "epoch 010 | iter 05599 | train loss 2.3049 | dt 0.10s\n",
            "epoch 010 | iter 05799 | train loss 2.2963 | dt 0.09s\n",
            "epoch 010 | iter 05999 | train loss 2.3402 | dt 0.11s\n",
            "epoch 010 | iter 06199 | train loss 2.3281 | dt 0.09s\n",
            "epoch 010 | iter 06399 | train loss 2.3430 | dt 0.11s\n",
            "epoch 010 | iter 06599 | train loss 2.3099 | dt 0.10s\n",
            "epoch 010 | iter 06799 | train loss 2.2849 | dt 0.10s\n",
            "epoch 010 | iter 06999 | train loss 2.3266 | dt 0.10s\n",
            "epoch 010 | iter 07199 | train loss 2.3254 | dt 0.10s\n",
            "epoch 010 | iter 07399 | train loss 2.3165 | dt 0.10s\n",
            "epoch 010 | iter 07599 | train loss 2.3168 | dt 0.11s\n",
            "epoch 010 | iter 07799 | train loss 2.3150 | dt 0.10s\n",
            "epoch 010 | iter 07999 | train loss 2.3057 | dt 0.10s\n",
            "epoch 010 | iter 08199 | train loss 2.3186 | dt 0.10s\n",
            "epoch 010 | iter 08399 | train loss 2.3161 | dt 0.11s\n",
            "epoch 010 | iter 08599 | train loss 2.3205 | dt 0.11s\n",
            "epoch 010 | iter 08799 | train loss 2.3291 | dt 0.11s\n",
            "epoch 010 | iter 08999 | train loss 2.3282 | dt 0.10s\n",
            "epoch 010 | iter 09199 | train loss 2.3308 | dt 0.11s\n",
            "epoch 010 | iter 09399 | train loss 2.3154 | dt 0.11s\n",
            "epoch 010 | iter 09599 | train loss 2.3200 | dt 0.11s\n",
            "epoch 010 | iter 09799 | train loss 2.2987 | dt 0.10s\n",
            "epoch 010 | iter 09999 | train loss 2.2973 | dt 0.11s\n",
            "epoch 010 | iter 10199 | train loss 2.3291 | dt 0.10s\n",
            "epoch 010 | iter 10399 | train loss 2.3401 | dt 0.10s\n",
            "epoch 010 | iter 10599 | train loss 2.3111 | dt 0.10s\n",
            "epoch 010 | iter 10799 | train loss 2.3409 | dt 0.09s\n",
            "epoch 010 | iter 10999 | train loss 2.3041 | dt 0.11s\n",
            "epoch 010 | iter 11199 | train loss 2.2486 | dt 0.11s\n",
            "epoch 010 | iter 11399 | train loss 2.2145 | dt 0.11s\n",
            "epoch 010 | iter 11599 | train loss 2.2189 | dt 0.09s\n",
            "epoch 010 | iter 11799 | train loss 2.2406 | dt 0.10s\n",
            "epoch 010 | iter 11999 | train loss 2.2453 | dt 0.10s\n",
            "epoch 010 | iter 12199 | train loss 2.2076 | dt 0.11s\n",
            "[EPOCH 010] valid loss=1.8202 | valid ppl=6.17 | valid BLEU=0.4693 | epoch time=2253.6s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4693\n",
            "epoch 011 | iter 00199 | train loss 2.2766 | dt 0.10s\n",
            "epoch 011 | iter 00399 | train loss 2.2976 | dt 0.10s\n",
            "epoch 011 | iter 00599 | train loss 2.3355 | dt 0.10s\n",
            "epoch 011 | iter 00799 | train loss 2.2879 | dt 0.10s\n",
            "epoch 011 | iter 00999 | train loss 2.3043 | dt 0.11s\n",
            "epoch 011 | iter 01199 | train loss 2.2636 | dt 0.10s\n",
            "epoch 011 | iter 01399 | train loss 2.2677 | dt 0.10s\n",
            "epoch 011 | iter 01599 | train loss 2.2912 | dt 0.10s\n",
            "epoch 011 | iter 01799 | train loss 2.3031 | dt 0.10s\n",
            "epoch 011 | iter 01999 | train loss 2.2865 | dt 0.11s\n",
            "epoch 011 | iter 02199 | train loss 2.2921 | dt 0.10s\n",
            "epoch 011 | iter 02399 | train loss 2.2923 | dt 0.10s\n",
            "epoch 011 | iter 02599 | train loss 2.3004 | dt 0.11s\n",
            "epoch 011 | iter 02799 | train loss 2.2999 | dt 0.10s\n",
            "epoch 011 | iter 02999 | train loss 2.2817 | dt 0.10s\n",
            "epoch 011 | iter 03199 | train loss 2.3160 | dt 0.10s\n",
            "epoch 011 | iter 03399 | train loss 2.2970 | dt 0.11s\n",
            "epoch 011 | iter 03599 | train loss 2.3226 | dt 0.11s\n",
            "epoch 011 | iter 03799 | train loss 2.2857 | dt 0.10s\n",
            "epoch 011 | iter 03999 | train loss 2.2962 | dt 0.10s\n",
            "epoch 011 | iter 04199 | train loss 2.3051 | dt 0.10s\n",
            "epoch 011 | iter 04399 | train loss 2.2933 | dt 0.11s\n",
            "epoch 011 | iter 04599 | train loss 2.3124 | dt 0.11s\n",
            "epoch 011 | iter 04799 | train loss 2.3050 | dt 0.11s\n",
            "epoch 011 | iter 04999 | train loss 2.2867 | dt 0.10s\n",
            "epoch 011 | iter 05199 | train loss 2.3065 | dt 0.10s\n",
            "epoch 011 | iter 05399 | train loss 2.2870 | dt 0.10s\n",
            "epoch 011 | iter 05599 | train loss 2.2989 | dt 0.11s\n",
            "epoch 011 | iter 05799 | train loss 2.2939 | dt 0.10s\n",
            "epoch 011 | iter 05999 | train loss 2.2835 | dt 0.10s\n",
            "epoch 011 | iter 06199 | train loss 2.3386 | dt 0.10s\n",
            "epoch 011 | iter 06399 | train loss 2.2911 | dt 0.10s\n",
            "epoch 011 | iter 06599 | train loss 2.3045 | dt 0.10s\n",
            "epoch 011 | iter 06799 | train loss 2.3090 | dt 0.11s\n",
            "epoch 011 | iter 06999 | train loss 2.3016 | dt 0.11s\n",
            "epoch 011 | iter 07199 | train loss 2.3060 | dt 0.10s\n",
            "epoch 011 | iter 07399 | train loss 2.2988 | dt 0.10s\n",
            "epoch 011 | iter 07599 | train loss 2.2904 | dt 0.09s\n",
            "epoch 011 | iter 07799 | train loss 2.3102 | dt 0.11s\n",
            "epoch 011 | iter 07999 | train loss 2.2539 | dt 0.10s\n",
            "epoch 011 | iter 08199 | train loss 2.2897 | dt 0.10s\n",
            "epoch 011 | iter 08399 | train loss 2.3015 | dt 0.10s\n",
            "epoch 011 | iter 08599 | train loss 2.3254 | dt 0.10s\n",
            "epoch 011 | iter 08799 | train loss 2.2981 | dt 0.10s\n",
            "epoch 011 | iter 08999 | train loss 2.3022 | dt 0.10s\n",
            "epoch 011 | iter 09199 | train loss 2.3031 | dt 0.10s\n",
            "epoch 011 | iter 09399 | train loss 2.3043 | dt 0.10s\n",
            "epoch 011 | iter 09599 | train loss 2.3262 | dt 0.11s\n",
            "epoch 011 | iter 09799 | train loss 2.3021 | dt 0.10s\n",
            "epoch 011 | iter 09999 | train loss 2.2679 | dt 0.11s\n",
            "epoch 011 | iter 10199 | train loss 2.3234 | dt 0.10s\n",
            "epoch 011 | iter 10399 | train loss 2.3136 | dt 0.10s\n",
            "epoch 011 | iter 10599 | train loss 2.3090 | dt 0.10s\n",
            "epoch 011 | iter 10799 | train loss 2.3093 | dt 0.10s\n",
            "epoch 011 | iter 10999 | train loss 2.3143 | dt 0.11s\n",
            "epoch 011 | iter 11199 | train loss 2.2054 | dt 0.11s\n",
            "epoch 011 | iter 11399 | train loss 2.2338 | dt 0.10s\n",
            "epoch 011 | iter 11599 | train loss 2.2373 | dt 0.10s\n",
            "epoch 011 | iter 11799 | train loss 2.2121 | dt 0.10s\n",
            "epoch 011 | iter 11999 | train loss 2.2207 | dt 0.10s\n",
            "epoch 011 | iter 12199 | train loss 2.2238 | dt 0.10s\n",
            "[EPOCH 011] valid loss=1.8127 | valid ppl=6.13 | valid BLEU=0.4716 | epoch time=2259.5s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4716\n",
            "epoch 012 | iter 00199 | train loss 2.2652 | dt 0.10s\n",
            "epoch 012 | iter 00399 | train loss 2.2861 | dt 0.10s\n",
            "epoch 012 | iter 00599 | train loss 2.2903 | dt 0.10s\n",
            "epoch 012 | iter 00799 | train loss 2.3244 | dt 0.10s\n",
            "epoch 012 | iter 00999 | train loss 2.2521 | dt 0.10s\n",
            "epoch 012 | iter 01199 | train loss 2.2822 | dt 0.10s\n",
            "epoch 012 | iter 01399 | train loss 2.2795 | dt 0.10s\n",
            "epoch 012 | iter 01599 | train loss 2.2873 | dt 0.10s\n",
            "epoch 012 | iter 01799 | train loss 2.2635 | dt 0.10s\n",
            "epoch 012 | iter 01999 | train loss 2.2755 | dt 0.10s\n",
            "epoch 012 | iter 02199 | train loss 2.2920 | dt 0.11s\n",
            "epoch 012 | iter 02399 | train loss 2.2812 | dt 0.10s\n",
            "epoch 012 | iter 02599 | train loss 2.2612 | dt 0.10s\n",
            "epoch 012 | iter 02799 | train loss 2.2941 | dt 0.10s\n",
            "epoch 012 | iter 02999 | train loss 2.2751 | dt 0.10s\n",
            "epoch 012 | iter 03199 | train loss 2.2861 | dt 0.11s\n",
            "epoch 012 | iter 03399 | train loss 2.2436 | dt 0.10s\n",
            "epoch 012 | iter 03599 | train loss 2.2962 | dt 0.10s\n",
            "epoch 012 | iter 03799 | train loss 2.2678 | dt 0.10s\n",
            "epoch 012 | iter 03999 | train loss 2.2855 | dt 0.10s\n",
            "epoch 012 | iter 04199 | train loss 2.2894 | dt 0.11s\n",
            "epoch 012 | iter 04399 | train loss 2.2683 | dt 0.10s\n",
            "epoch 012 | iter 04599 | train loss 2.2897 | dt 0.11s\n",
            "epoch 012 | iter 04799 | train loss 2.3073 | dt 0.10s\n",
            "epoch 012 | iter 04999 | train loss 2.2874 | dt 0.11s\n",
            "epoch 012 | iter 05199 | train loss 2.2993 | dt 0.09s\n",
            "epoch 012 | iter 05399 | train loss 2.2923 | dt 0.09s\n",
            "epoch 012 | iter 05599 | train loss 2.2875 | dt 0.11s\n",
            "epoch 012 | iter 05799 | train loss 2.2931 | dt 0.09s\n",
            "epoch 012 | iter 05999 | train loss 2.2829 | dt 0.10s\n",
            "epoch 012 | iter 06199 | train loss 2.3015 | dt 0.10s\n",
            "epoch 012 | iter 06399 | train loss 2.2832 | dt 0.10s\n",
            "epoch 012 | iter 06599 | train loss 2.2685 | dt 0.10s\n",
            "epoch 012 | iter 06799 | train loss 2.2613 | dt 0.09s\n",
            "epoch 012 | iter 06999 | train loss 2.2728 | dt 0.11s\n",
            "epoch 012 | iter 07199 | train loss 2.2973 | dt 0.10s\n",
            "epoch 012 | iter 07399 | train loss 2.2824 | dt 0.11s\n",
            "epoch 012 | iter 07599 | train loss 2.3006 | dt 0.10s\n",
            "epoch 012 | iter 07799 | train loss 2.3128 | dt 0.10s\n",
            "epoch 012 | iter 07999 | train loss 2.2890 | dt 0.10s\n",
            "epoch 012 | iter 08199 | train loss 2.2596 | dt 0.10s\n",
            "epoch 012 | iter 08399 | train loss 2.3044 | dt 0.11s\n",
            "epoch 012 | iter 08599 | train loss 2.2691 | dt 0.10s\n",
            "epoch 012 | iter 08799 | train loss 2.2854 | dt 0.10s\n",
            "epoch 012 | iter 08999 | train loss 2.2959 | dt 0.10s\n",
            "epoch 012 | iter 09199 | train loss 2.2776 | dt 0.10s\n",
            "epoch 012 | iter 09399 | train loss 2.2654 | dt 0.11s\n",
            "epoch 012 | iter 09599 | train loss 2.2853 | dt 0.10s\n",
            "epoch 012 | iter 09799 | train loss 2.3023 | dt 0.10s\n",
            "epoch 012 | iter 09999 | train loss 2.2996 | dt 0.10s\n",
            "epoch 012 | iter 10199 | train loss 2.2872 | dt 0.09s\n",
            "epoch 012 | iter 10399 | train loss 2.2892 | dt 0.09s\n",
            "epoch 012 | iter 10599 | train loss 2.2990 | dt 0.10s\n",
            "epoch 012 | iter 10799 | train loss 2.2896 | dt 0.10s\n",
            "epoch 012 | iter 10999 | train loss 2.2640 | dt 0.10s\n",
            "epoch 012 | iter 11199 | train loss 2.1986 | dt 0.10s\n",
            "epoch 012 | iter 11399 | train loss 2.2145 | dt 0.10s\n",
            "epoch 012 | iter 11599 | train loss 2.2269 | dt 0.08s\n",
            "epoch 012 | iter 11799 | train loss 2.1787 | dt 0.11s\n",
            "epoch 012 | iter 11999 | train loss 2.2189 | dt 0.11s\n",
            "epoch 012 | iter 12199 | train loss 2.2603 | dt 0.11s\n",
            "[EPOCH 012] valid loss=1.8105 | valid ppl=6.11 | valid BLEU=0.4716 | epoch time=2246.1s\n",
            "epoch 013 | iter 00199 | train loss 2.2265 | dt 0.10s\n",
            "epoch 013 | iter 00399 | train loss 2.2473 | dt 0.09s\n",
            "epoch 013 | iter 00599 | train loss 2.2682 | dt 0.11s\n",
            "epoch 013 | iter 00799 | train loss 2.2665 | dt 0.10s\n",
            "epoch 013 | iter 00999 | train loss 2.2626 | dt 0.10s\n",
            "epoch 013 | iter 01199 | train loss 2.2708 | dt 0.11s\n",
            "epoch 013 | iter 01399 | train loss 2.2763 | dt 0.10s\n",
            "epoch 013 | iter 01599 | train loss 2.2816 | dt 0.11s\n",
            "epoch 013 | iter 01799 | train loss 2.2718 | dt 0.09s\n",
            "epoch 013 | iter 01999 | train loss 2.2736 | dt 0.10s\n",
            "epoch 013 | iter 02199 | train loss 2.2724 | dt 0.10s\n",
            "epoch 013 | iter 02399 | train loss 2.2816 | dt 0.10s\n",
            "epoch 013 | iter 02599 | train loss 2.2619 | dt 0.10s\n",
            "epoch 013 | iter 02799 | train loss 2.2565 | dt 0.11s\n",
            "epoch 013 | iter 02999 | train loss 2.2980 | dt 0.10s\n",
            "epoch 013 | iter 03199 | train loss 2.2683 | dt 0.11s\n",
            "epoch 013 | iter 03399 | train loss 2.2957 | dt 0.10s\n",
            "epoch 013 | iter 03599 | train loss 2.2964 | dt 0.10s\n",
            "epoch 013 | iter 03799 | train loss 2.2506 | dt 0.11s\n",
            "epoch 013 | iter 03999 | train loss 2.2713 | dt 0.09s\n",
            "epoch 013 | iter 04199 | train loss 2.2924 | dt 0.11s\n",
            "epoch 013 | iter 04399 | train loss 2.2866 | dt 0.09s\n",
            "epoch 013 | iter 04599 | train loss 2.2715 | dt 0.10s\n",
            "epoch 013 | iter 04799 | train loss 2.2675 | dt 0.10s\n",
            "epoch 013 | iter 04999 | train loss 2.2581 | dt 0.12s\n",
            "epoch 013 | iter 05199 | train loss 2.2733 | dt 0.10s\n",
            "epoch 013 | iter 05399 | train loss 2.2672 | dt 0.10s\n",
            "epoch 013 | iter 05599 | train loss 2.2640 | dt 0.10s\n",
            "epoch 013 | iter 05799 | train loss 2.2846 | dt 0.10s\n",
            "epoch 013 | iter 05999 | train loss 2.2519 | dt 0.10s\n",
            "epoch 013 | iter 06199 | train loss 2.2874 | dt 0.10s\n",
            "epoch 013 | iter 06399 | train loss 2.2982 | dt 0.10s\n",
            "epoch 013 | iter 06599 | train loss 2.2507 | dt 0.10s\n",
            "epoch 013 | iter 06799 | train loss 2.2802 | dt 0.10s\n",
            "epoch 013 | iter 06999 | train loss 2.2711 | dt 0.11s\n",
            "epoch 013 | iter 07199 | train loss 2.2855 | dt 0.10s\n",
            "epoch 013 | iter 07399 | train loss 2.2685 | dt 0.11s\n",
            "epoch 013 | iter 07599 | train loss 2.2697 | dt 0.10s\n",
            "epoch 013 | iter 07799 | train loss 2.2964 | dt 0.10s\n",
            "epoch 013 | iter 07999 | train loss 2.2645 | dt 0.10s\n",
            "epoch 013 | iter 08199 | train loss 2.2743 | dt 0.09s\n",
            "epoch 013 | iter 08399 | train loss 2.2570 | dt 0.10s\n",
            "epoch 013 | iter 08599 | train loss 2.2868 | dt 0.11s\n",
            "epoch 013 | iter 08799 | train loss 2.2736 | dt 0.11s\n",
            "epoch 013 | iter 08999 | train loss 2.2584 | dt 0.10s\n",
            "epoch 013 | iter 09199 | train loss 2.3039 | dt 0.11s\n",
            "epoch 013 | iter 09399 | train loss 2.2720 | dt 0.10s\n",
            "epoch 013 | iter 09599 | train loss 2.2872 | dt 0.10s\n",
            "epoch 013 | iter 09799 | train loss 2.2735 | dt 0.11s\n",
            "epoch 013 | iter 09999 | train loss 2.2569 | dt 0.10s\n",
            "epoch 013 | iter 10199 | train loss 2.2949 | dt 0.10s\n",
            "epoch 013 | iter 10399 | train loss 2.2732 | dt 0.10s\n",
            "epoch 013 | iter 10599 | train loss 2.2928 | dt 0.11s\n",
            "epoch 013 | iter 10799 | train loss 2.2963 | dt 0.10s\n",
            "epoch 013 | iter 10999 | train loss 2.2725 | dt 0.10s\n",
            "epoch 013 | iter 11199 | train loss 2.2051 | dt 0.10s\n",
            "epoch 013 | iter 11399 | train loss 2.1824 | dt 0.10s\n",
            "epoch 013 | iter 11599 | train loss 2.1939 | dt 0.10s\n",
            "epoch 013 | iter 11799 | train loss 2.1964 | dt 0.10s\n",
            "epoch 013 | iter 11999 | train loss 2.1867 | dt 0.11s\n",
            "epoch 013 | iter 12199 | train loss 2.1807 | dt 0.11s\n",
            "[EPOCH 013] valid loss=1.8002 | valid ppl=6.05 | valid BLEU=0.4722 | epoch time=2268.3s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4722\n",
            "epoch 014 | iter 00199 | train loss 2.2680 | dt 0.10s\n",
            "epoch 014 | iter 00399 | train loss 2.2451 | dt 0.10s\n",
            "epoch 014 | iter 00599 | train loss 2.2755 | dt 0.10s\n",
            "epoch 014 | iter 00799 | train loss 2.2476 | dt 0.10s\n",
            "epoch 014 | iter 00999 | train loss 2.2757 | dt 0.11s\n",
            "epoch 014 | iter 01199 | train loss 2.2783 | dt 0.10s\n",
            "epoch 014 | iter 01399 | train loss 2.2535 | dt 0.10s\n",
            "epoch 014 | iter 01599 | train loss 2.2465 | dt 0.10s\n",
            "epoch 014 | iter 01799 | train loss 2.2396 | dt 0.10s\n",
            "epoch 014 | iter 01999 | train loss 2.2805 | dt 0.11s\n",
            "epoch 014 | iter 02199 | train loss 2.2887 | dt 0.10s\n",
            "epoch 014 | iter 02399 | train loss 2.2483 | dt 0.10s\n",
            "epoch 014 | iter 02599 | train loss 2.2347 | dt 0.10s\n",
            "epoch 014 | iter 02799 | train loss 2.2596 | dt 0.10s\n",
            "epoch 014 | iter 02999 | train loss 2.2621 | dt 0.10s\n",
            "epoch 014 | iter 03199 | train loss 2.2444 | dt 0.10s\n",
            "epoch 014 | iter 03399 | train loss 2.2556 | dt 0.10s\n",
            "epoch 014 | iter 03599 | train loss 2.2313 | dt 0.10s\n",
            "epoch 014 | iter 03799 | train loss 2.2488 | dt 0.11s\n",
            "epoch 014 | iter 03999 | train loss 2.2427 | dt 0.11s\n",
            "epoch 014 | iter 04199 | train loss 2.2680 | dt 0.10s\n",
            "epoch 014 | iter 04399 | train loss 2.2573 | dt 0.11s\n",
            "epoch 014 | iter 04599 | train loss 2.2792 | dt 0.11s\n",
            "epoch 014 | iter 04799 | train loss 2.2575 | dt 0.11s\n",
            "epoch 014 | iter 04999 | train loss 2.2390 | dt 0.10s\n",
            "epoch 014 | iter 05199 | train loss 2.2602 | dt 0.10s\n",
            "epoch 014 | iter 05399 | train loss 2.2724 | dt 0.10s\n",
            "epoch 014 | iter 05599 | train loss 2.2557 | dt 0.10s\n",
            "epoch 014 | iter 05799 | train loss 2.2785 | dt 0.11s\n",
            "epoch 014 | iter 05999 | train loss 2.2702 | dt 0.10s\n",
            "epoch 014 | iter 06199 | train loss 2.2655 | dt 0.10s\n",
            "epoch 014 | iter 06399 | train loss 2.2353 | dt 0.10s\n",
            "epoch 014 | iter 06599 | train loss 2.2523 | dt 0.10s\n",
            "epoch 014 | iter 06799 | train loss 2.2812 | dt 0.09s\n",
            "epoch 014 | iter 06999 | train loss 2.2780 | dt 0.10s\n",
            "epoch 014 | iter 07199 | train loss 2.2685 | dt 0.11s\n",
            "epoch 014 | iter 07399 | train loss 2.2555 | dt 0.10s\n",
            "epoch 014 | iter 07599 | train loss 2.2760 | dt 0.11s\n",
            "epoch 014 | iter 07799 | train loss 2.2520 | dt 0.10s\n",
            "epoch 014 | iter 07999 | train loss 2.2599 | dt 0.10s\n",
            "epoch 014 | iter 08199 | train loss 2.2526 | dt 0.10s\n",
            "epoch 014 | iter 08399 | train loss 2.2581 | dt 0.10s\n",
            "epoch 014 | iter 08599 | train loss 2.2915 | dt 0.11s\n",
            "epoch 014 | iter 08799 | train loss 2.2782 | dt 0.10s\n",
            "epoch 014 | iter 08999 | train loss 2.2697 | dt 0.10s\n",
            "epoch 014 | iter 09199 | train loss 2.2601 | dt 0.10s\n",
            "epoch 014 | iter 09399 | train loss 2.2516 | dt 0.10s\n",
            "epoch 014 | iter 09599 | train loss 2.2784 | dt 0.11s\n",
            "epoch 014 | iter 09799 | train loss 2.2621 | dt 0.10s\n",
            "epoch 014 | iter 09999 | train loss 2.2584 | dt 0.10s\n",
            "epoch 014 | iter 10199 | train loss 2.2467 | dt 0.10s\n",
            "epoch 014 | iter 10399 | train loss 2.2616 | dt 0.11s\n",
            "epoch 014 | iter 10599 | train loss 2.2841 | dt 0.11s\n",
            "epoch 014 | iter 10799 | train loss 2.2931 | dt 0.11s\n",
            "epoch 014 | iter 10999 | train loss 2.2680 | dt 0.11s\n",
            "epoch 014 | iter 11199 | train loss 2.1766 | dt 0.11s\n",
            "epoch 014 | iter 11399 | train loss 2.1840 | dt 0.10s\n",
            "epoch 014 | iter 11599 | train loss 2.2114 | dt 0.11s\n",
            "epoch 014 | iter 11799 | train loss 2.1849 | dt 0.10s\n",
            "epoch 014 | iter 11999 | train loss 2.1874 | dt 0.10s\n",
            "epoch 014 | iter 12199 | train loss 2.1766 | dt 0.10s\n",
            "[EPOCH 014] valid loss=1.7973 | valid ppl=6.03 | valid BLEU=0.4754 | epoch time=2253.8s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4754\n",
            "epoch 015 | iter 00199 | train loss 2.2151 | dt 0.10s\n",
            "epoch 015 | iter 00399 | train loss 2.2432 | dt 0.11s\n",
            "epoch 015 | iter 00599 | train loss 2.2262 | dt 0.10s\n",
            "epoch 015 | iter 00799 | train loss 2.2478 | dt 0.10s\n",
            "epoch 015 | iter 00999 | train loss 2.2398 | dt 0.10s\n",
            "epoch 015 | iter 01199 | train loss 2.2322 | dt 0.10s\n",
            "epoch 015 | iter 01399 | train loss 2.2414 | dt 0.10s\n",
            "epoch 015 | iter 01599 | train loss 2.2010 | dt 0.10s\n",
            "epoch 015 | iter 01799 | train loss 2.2606 | dt 0.11s\n",
            "epoch 015 | iter 01999 | train loss 2.2484 | dt 0.11s\n",
            "epoch 015 | iter 02199 | train loss 2.2511 | dt 0.10s\n",
            "epoch 015 | iter 02399 | train loss 2.2550 | dt 0.10s\n",
            "epoch 015 | iter 02599 | train loss 2.2506 | dt 0.10s\n",
            "epoch 015 | iter 02799 | train loss 2.2519 | dt 0.10s\n",
            "epoch 015 | iter 02999 | train loss 2.2720 | dt 0.11s\n",
            "epoch 015 | iter 03199 | train loss 2.2338 | dt 0.11s\n",
            "epoch 015 | iter 03399 | train loss 2.2274 | dt 0.10s\n",
            "epoch 015 | iter 03599 | train loss 2.2677 | dt 0.11s\n",
            "epoch 015 | iter 03799 | train loss 2.2581 | dt 0.11s\n",
            "epoch 015 | iter 03999 | train loss 2.2383 | dt 0.10s\n",
            "epoch 015 | iter 04199 | train loss 2.2597 | dt 0.11s\n",
            "epoch 015 | iter 04399 | train loss 2.2565 | dt 0.11s\n",
            "epoch 015 | iter 04599 | train loss 2.2670 | dt 0.10s\n",
            "epoch 015 | iter 04799 | train loss 2.2641 | dt 0.09s\n",
            "epoch 015 | iter 04999 | train loss 2.2294 | dt 0.10s\n",
            "epoch 015 | iter 05199 | train loss 2.2768 | dt 0.10s\n",
            "epoch 015 | iter 05399 | train loss 2.2676 | dt 0.11s\n",
            "epoch 015 | iter 05599 | train loss 2.2473 | dt 0.11s\n",
            "epoch 015 | iter 05799 | train loss 2.2691 | dt 0.10s\n",
            "epoch 015 | iter 05999 | train loss 2.2376 | dt 0.10s\n",
            "epoch 015 | iter 06199 | train loss 2.2689 | dt 0.10s\n",
            "epoch 015 | iter 06399 | train loss 2.2337 | dt 0.10s\n",
            "epoch 015 | iter 06599 | train loss 2.2601 | dt 0.11s\n",
            "epoch 015 | iter 06799 | train loss 2.2307 | dt 0.11s\n",
            "epoch 015 | iter 06999 | train loss 2.2517 | dt 0.10s\n",
            "epoch 015 | iter 07199 | train loss 2.2508 | dt 0.10s\n",
            "epoch 015 | iter 07399 | train loss 2.2518 | dt 0.10s\n",
            "epoch 015 | iter 07599 | train loss 2.2420 | dt 0.10s\n",
            "epoch 015 | iter 07799 | train loss 2.2319 | dt 0.11s\n",
            "epoch 015 | iter 07999 | train loss 2.2624 | dt 0.10s\n",
            "epoch 015 | iter 08199 | train loss 2.2511 | dt 0.10s\n",
            "epoch 015 | iter 08399 | train loss 2.2616 | dt 0.10s\n",
            "epoch 015 | iter 08599 | train loss 2.2582 | dt 0.11s\n",
            "epoch 015 | iter 08799 | train loss 2.2547 | dt 0.10s\n",
            "epoch 015 | iter 08999 | train loss 2.2717 | dt 0.10s\n",
            "epoch 015 | iter 09199 | train loss 2.2698 | dt 0.10s\n",
            "epoch 015 | iter 09399 | train loss 2.2207 | dt 0.10s\n",
            "epoch 015 | iter 09599 | train loss 2.2262 | dt 0.10s\n",
            "epoch 015 | iter 09799 | train loss 2.2794 | dt 0.09s\n",
            "epoch 015 | iter 09999 | train loss 2.2796 | dt 0.11s\n",
            "epoch 015 | iter 10199 | train loss 2.2555 | dt 0.09s\n",
            "epoch 015 | iter 10399 | train loss 2.2527 | dt 0.10s\n",
            "epoch 015 | iter 10599 | train loss 2.2474 | dt 0.11s\n",
            "epoch 015 | iter 10799 | train loss 2.2783 | dt 0.11s\n",
            "epoch 015 | iter 10999 | train loss 2.2482 | dt 0.10s\n",
            "epoch 015 | iter 11199 | train loss 2.2138 | dt 0.10s\n",
            "epoch 015 | iter 11399 | train loss 2.1885 | dt 0.10s\n",
            "epoch 015 | iter 11599 | train loss 2.2103 | dt 0.11s\n",
            "epoch 015 | iter 11799 | train loss 2.1467 | dt 0.11s\n",
            "epoch 015 | iter 11999 | train loss 2.1822 | dt 0.11s\n",
            "epoch 015 | iter 12199 | train loss 2.1562 | dt 0.10s\n",
            "[EPOCH 015] valid loss=1.7894 | valid ppl=5.99 | valid BLEU=0.4776 | epoch time=2278.0s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4776\n",
            "epoch 016 | iter 00199 | train loss 2.2442 | dt 0.11s\n",
            "epoch 016 | iter 00399 | train loss 2.2300 | dt 0.10s\n",
            "epoch 016 | iter 00599 | train loss 2.2325 | dt 0.10s\n",
            "epoch 016 | iter 00799 | train loss 2.2116 | dt 0.10s\n",
            "epoch 016 | iter 00999 | train loss 2.2099 | dt 0.11s\n",
            "epoch 016 | iter 01199 | train loss 2.2222 | dt 0.10s\n",
            "epoch 016 | iter 01399 | train loss 2.2197 | dt 0.10s\n",
            "epoch 016 | iter 01599 | train loss 2.2364 | dt 0.11s\n",
            "epoch 016 | iter 01799 | train loss 2.2280 | dt 0.11s\n",
            "epoch 016 | iter 01999 | train loss 2.2353 | dt 0.10s\n",
            "epoch 016 | iter 02199 | train loss 2.2342 | dt 0.10s\n",
            "epoch 016 | iter 02399 | train loss 2.2301 | dt 0.10s\n",
            "epoch 016 | iter 02599 | train loss 2.2463 | dt 0.10s\n",
            "epoch 016 | iter 02799 | train loss 2.2391 | dt 0.10s\n",
            "epoch 016 | iter 02999 | train loss 2.2669 | dt 0.11s\n",
            "epoch 016 | iter 03199 | train loss 2.2648 | dt 0.10s\n",
            "epoch 016 | iter 03399 | train loss 2.2227 | dt 0.10s\n",
            "epoch 016 | iter 03599 | train loss 2.2405 | dt 0.10s\n",
            "epoch 016 | iter 03799 | train loss 2.2517 | dt 0.10s\n",
            "epoch 016 | iter 03999 | train loss 2.2470 | dt 0.10s\n",
            "epoch 016 | iter 04199 | train loss 2.2318 | dt 0.10s\n",
            "epoch 016 | iter 04399 | train loss 2.2294 | dt 0.11s\n",
            "epoch 016 | iter 04599 | train loss 2.2534 | dt 0.10s\n",
            "epoch 016 | iter 04799 | train loss 2.2640 | dt 0.10s\n",
            "epoch 016 | iter 04999 | train loss 2.2425 | dt 0.10s\n",
            "epoch 016 | iter 05199 | train loss 2.2363 | dt 0.10s\n",
            "epoch 016 | iter 05399 | train loss 2.2533 | dt 0.11s\n",
            "epoch 016 | iter 05599 | train loss 2.2403 | dt 0.10s\n",
            "epoch 016 | iter 05799 | train loss 2.2018 | dt 0.10s\n",
            "epoch 016 | iter 05999 | train loss 2.2385 | dt 0.10s\n",
            "epoch 016 | iter 06199 | train loss 2.2223 | dt 0.10s\n",
            "epoch 016 | iter 06399 | train loss 2.2255 | dt 0.11s\n",
            "epoch 016 | iter 06599 | train loss 2.2497 | dt 0.11s\n",
            "epoch 016 | iter 06799 | train loss 2.2485 | dt 0.10s\n",
            "epoch 016 | iter 06999 | train loss 2.2506 | dt 0.10s\n",
            "epoch 016 | iter 07199 | train loss 2.2457 | dt 0.10s\n",
            "epoch 016 | iter 07399 | train loss 2.2398 | dt 0.10s\n",
            "epoch 016 | iter 07599 | train loss 2.2399 | dt 0.10s\n",
            "epoch 016 | iter 07799 | train loss 2.2586 | dt 0.10s\n",
            "epoch 016 | iter 07999 | train loss 2.2293 | dt 0.10s\n",
            "epoch 016 | iter 08199 | train loss 2.2585 | dt 0.11s\n",
            "epoch 016 | iter 08399 | train loss 2.2432 | dt 0.10s\n",
            "epoch 016 | iter 08599 | train loss 2.2639 | dt 0.11s\n",
            "epoch 016 | iter 08799 | train loss 2.2360 | dt 0.11s\n",
            "epoch 016 | iter 08999 | train loss 2.2509 | dt 0.10s\n",
            "epoch 016 | iter 09199 | train loss 2.2434 | dt 0.10s\n",
            "epoch 016 | iter 09399 | train loss 2.2553 | dt 0.10s\n",
            "epoch 016 | iter 09599 | train loss 2.2663 | dt 0.10s\n",
            "epoch 016 | iter 09799 | train loss 2.2423 | dt 0.10s\n",
            "epoch 016 | iter 09999 | train loss 2.2372 | dt 0.10s\n",
            "epoch 016 | iter 10199 | train loss 2.2270 | dt 0.10s\n",
            "epoch 016 | iter 10399 | train loss 2.2327 | dt 0.11s\n",
            "epoch 016 | iter 10599 | train loss 2.2707 | dt 0.11s\n",
            "epoch 016 | iter 10799 | train loss 2.2528 | dt 0.10s\n",
            "epoch 016 | iter 10999 | train loss 2.2215 | dt 0.10s\n",
            "epoch 016 | iter 11199 | train loss 2.1821 | dt 0.10s\n",
            "epoch 016 | iter 11399 | train loss 2.1483 | dt 0.10s\n",
            "epoch 016 | iter 11599 | train loss 2.1620 | dt 0.11s\n",
            "epoch 016 | iter 11799 | train loss 2.1806 | dt 0.10s\n",
            "epoch 016 | iter 11999 | train loss 2.1600 | dt 0.10s\n",
            "epoch 016 | iter 12199 | train loss 2.1584 | dt 0.10s\n",
            "[EPOCH 016] valid loss=1.7851 | valid ppl=5.96 | valid BLEU=0.4776 | epoch time=2258.6s\n",
            "epoch 017 | iter 00199 | train loss 2.2328 | dt 0.11s\n",
            "epoch 017 | iter 00399 | train loss 2.2235 | dt 0.11s\n",
            "epoch 017 | iter 00599 | train loss 2.2463 | dt 0.10s\n",
            "epoch 017 | iter 00799 | train loss 2.2091 | dt 0.10s\n",
            "epoch 017 | iter 00999 | train loss 2.2256 | dt 0.10s\n",
            "epoch 017 | iter 01199 | train loss 2.2263 | dt 0.10s\n",
            "epoch 017 | iter 01399 | train loss 2.2215 | dt 0.10s\n",
            "epoch 017 | iter 01599 | train loss 2.2264 | dt 0.10s\n",
            "epoch 017 | iter 01799 | train loss 2.2355 | dt 0.10s\n",
            "epoch 017 | iter 01999 | train loss 2.2194 | dt 0.10s\n",
            "epoch 017 | iter 02199 | train loss 2.2465 | dt 0.10s\n",
            "epoch 017 | iter 02399 | train loss 2.2447 | dt 0.11s\n",
            "epoch 017 | iter 02599 | train loss 2.2025 | dt 0.10s\n",
            "epoch 017 | iter 02799 | train loss 2.2409 | dt 0.11s\n",
            "epoch 017 | iter 02999 | train loss 2.2421 | dt 0.11s\n",
            "epoch 017 | iter 03199 | train loss 2.2242 | dt 0.10s\n",
            "epoch 017 | iter 03399 | train loss 2.2236 | dt 0.10s\n",
            "epoch 017 | iter 03599 | train loss 2.2205 | dt 0.10s\n",
            "epoch 017 | iter 03799 | train loss 2.2107 | dt 0.10s\n",
            "epoch 017 | iter 03999 | train loss 2.2437 | dt 0.11s\n",
            "epoch 017 | iter 04199 | train loss 2.2293 | dt 0.10s\n",
            "epoch 017 | iter 04399 | train loss 2.2136 | dt 0.10s\n",
            "epoch 017 | iter 04599 | train loss 2.2247 | dt 0.11s\n",
            "epoch 017 | iter 04799 | train loss 2.2425 | dt 0.10s\n",
            "epoch 017 | iter 04999 | train loss 2.2431 | dt 0.09s\n",
            "epoch 017 | iter 05199 | train loss 2.2309 | dt 0.11s\n",
            "epoch 017 | iter 05399 | train loss 2.2245 | dt 0.11s\n",
            "epoch 017 | iter 05599 | train loss 2.2143 | dt 0.10s\n",
            "epoch 017 | iter 05799 | train loss 2.2216 | dt 0.11s\n",
            "epoch 017 | iter 05999 | train loss 2.2082 | dt 0.10s\n",
            "epoch 017 | iter 06199 | train loss 2.2230 | dt 0.10s\n",
            "epoch 017 | iter 06399 | train loss 2.2232 | dt 0.11s\n",
            "epoch 017 | iter 06599 | train loss 2.2526 | dt 0.10s\n",
            "epoch 017 | iter 06799 | train loss 2.2409 | dt 0.11s\n",
            "epoch 017 | iter 06999 | train loss 2.2549 | dt 0.10s\n",
            "epoch 017 | iter 07199 | train loss 2.2378 | dt 0.10s\n",
            "epoch 017 | iter 07399 | train loss 2.2680 | dt 0.10s\n",
            "epoch 017 | iter 07599 | train loss 2.2383 | dt 0.10s\n",
            "epoch 017 | iter 07799 | train loss 2.2256 | dt 0.10s\n",
            "epoch 017 | iter 07999 | train loss 2.2221 | dt 0.11s\n",
            "epoch 017 | iter 08199 | train loss 2.2491 | dt 0.10s\n",
            "epoch 017 | iter 08399 | train loss 2.2257 | dt 0.10s\n",
            "epoch 017 | iter 08599 | train loss 2.2393 | dt 0.10s\n",
            "epoch 017 | iter 08799 | train loss 2.2490 | dt 0.10s\n",
            "epoch 017 | iter 08999 | train loss 2.2545 | dt 0.10s\n",
            "epoch 017 | iter 09199 | train loss 2.2189 | dt 0.10s\n",
            "epoch 017 | iter 09399 | train loss 2.2522 | dt 0.10s\n",
            "epoch 017 | iter 09599 | train loss 2.2234 | dt 0.10s\n",
            "epoch 017 | iter 09799 | train loss 2.2673 | dt 0.10s\n",
            "epoch 017 | iter 09999 | train loss 2.2289 | dt 0.10s\n",
            "epoch 017 | iter 10199 | train loss 2.2157 | dt 0.11s\n",
            "epoch 017 | iter 10399 | train loss 2.2338 | dt 0.10s\n",
            "epoch 017 | iter 10599 | train loss 2.2303 | dt 0.10s\n",
            "epoch 017 | iter 10799 | train loss 2.2268 | dt 0.10s\n",
            "epoch 017 | iter 10999 | train loss 2.1914 | dt 0.09s\n",
            "epoch 017 | iter 11199 | train loss 2.1483 | dt 0.10s\n",
            "epoch 017 | iter 11399 | train loss 2.1594 | dt 0.10s\n",
            "epoch 017 | iter 11599 | train loss 2.1783 | dt 0.11s\n",
            "epoch 017 | iter 11799 | train loss 2.1830 | dt 0.11s\n",
            "epoch 017 | iter 11999 | train loss 2.1644 | dt 0.09s\n",
            "epoch 017 | iter 12199 | train loss 2.1433 | dt 0.10s\n",
            "[EPOCH 017] valid loss=1.7787 | valid ppl=5.92 | valid BLEU=0.4775 | epoch time=2270.9s\n",
            "epoch 018 | iter 00199 | train loss 2.2236 | dt 0.10s\n",
            "epoch 018 | iter 00399 | train loss 2.2067 | dt 0.11s\n",
            "epoch 018 | iter 00599 | train loss 2.2025 | dt 0.11s\n",
            "epoch 018 | iter 00799 | train loss 2.2009 | dt 0.11s\n",
            "epoch 018 | iter 00999 | train loss 2.1729 | dt 0.10s\n",
            "epoch 018 | iter 01199 | train loss 2.1753 | dt 0.10s\n",
            "epoch 018 | iter 01399 | train loss 2.2340 | dt 0.10s\n",
            "epoch 018 | iter 01599 | train loss 2.2159 | dt 0.11s\n",
            "epoch 018 | iter 01799 | train loss 2.2013 | dt 0.10s\n",
            "epoch 018 | iter 01999 | train loss 2.2278 | dt 0.10s\n",
            "epoch 018 | iter 02199 | train loss 2.2384 | dt 0.10s\n",
            "epoch 018 | iter 02399 | train loss 2.2223 | dt 0.11s\n",
            "epoch 018 | iter 02599 | train loss 2.2097 | dt 0.10s\n",
            "epoch 018 | iter 02799 | train loss 2.2486 | dt 0.10s\n",
            "epoch 018 | iter 02999 | train loss 2.2147 | dt 0.10s\n",
            "epoch 018 | iter 03199 | train loss 2.2226 | dt 0.11s\n",
            "epoch 018 | iter 03399 | train loss 2.2471 | dt 0.10s\n",
            "epoch 018 | iter 03599 | train loss 2.2172 | dt 0.10s\n",
            "epoch 018 | iter 03799 | train loss 2.2140 | dt 0.11s\n",
            "epoch 018 | iter 03999 | train loss 2.2331 | dt 0.11s\n",
            "epoch 018 | iter 04199 | train loss 2.1972 | dt 0.10s\n",
            "epoch 018 | iter 04399 | train loss 2.2073 | dt 0.10s\n",
            "epoch 018 | iter 04599 | train loss 2.2196 | dt 0.10s\n",
            "epoch 018 | iter 04799 | train loss 2.2338 | dt 0.10s\n",
            "epoch 018 | iter 04999 | train loss 2.2004 | dt 0.10s\n",
            "epoch 018 | iter 05199 | train loss 2.2362 | dt 0.10s\n",
            "epoch 018 | iter 05399 | train loss 2.2192 | dt 0.10s\n",
            "epoch 018 | iter 05599 | train loss 2.2082 | dt 0.11s\n",
            "epoch 018 | iter 05799 | train loss 2.2157 | dt 0.11s\n",
            "epoch 018 | iter 05999 | train loss 2.2025 | dt 0.10s\n",
            "epoch 018 | iter 06199 | train loss 2.2365 | dt 0.11s\n",
            "epoch 018 | iter 06399 | train loss 2.2239 | dt 0.10s\n",
            "epoch 018 | iter 06599 | train loss 2.2304 | dt 0.10s\n",
            "epoch 018 | iter 06799 | train loss 2.2539 | dt 0.10s\n",
            "epoch 018 | iter 06999 | train loss 2.2056 | dt 0.10s\n",
            "epoch 018 | iter 07199 | train loss 2.2312 | dt 0.11s\n",
            "epoch 018 | iter 07399 | train loss 2.2485 | dt 0.11s\n",
            "epoch 018 | iter 07599 | train loss 2.2170 | dt 0.11s\n",
            "epoch 018 | iter 07799 | train loss 2.2258 | dt 0.10s\n",
            "epoch 018 | iter 07999 | train loss 2.2373 | dt 0.10s\n",
            "epoch 018 | iter 08199 | train loss 2.2245 | dt 0.10s\n",
            "epoch 018 | iter 08399 | train loss 2.2252 | dt 0.10s\n",
            "epoch 018 | iter 08599 | train loss 2.2298 | dt 0.11s\n",
            "epoch 018 | iter 08799 | train loss 2.2423 | dt 0.10s\n",
            "epoch 018 | iter 08999 | train loss 2.2190 | dt 0.10s\n",
            "epoch 018 | iter 09199 | train loss 2.2469 | dt 0.11s\n",
            "epoch 018 | iter 09399 | train loss 2.2083 | dt 0.10s\n",
            "epoch 018 | iter 09599 | train loss 2.2485 | dt 0.11s\n",
            "epoch 018 | iter 09799 | train loss 2.2387 | dt 0.10s\n",
            "epoch 018 | iter 09999 | train loss 2.2227 | dt 0.10s\n",
            "epoch 018 | iter 10199 | train loss 2.2241 | dt 0.10s\n",
            "epoch 018 | iter 10399 | train loss 2.2165 | dt 0.10s\n",
            "epoch 018 | iter 10599 | train loss 2.2266 | dt 0.11s\n",
            "epoch 018 | iter 10799 | train loss 2.2397 | dt 0.10s\n",
            "epoch 018 | iter 10999 | train loss 2.1980 | dt 0.11s\n",
            "epoch 018 | iter 11199 | train loss 2.1842 | dt 0.10s\n",
            "epoch 018 | iter 11399 | train loss 2.1517 | dt 0.10s\n",
            "epoch 018 | iter 11599 | train loss 2.1423 | dt 0.10s\n",
            "epoch 018 | iter 11799 | train loss 2.1447 | dt 0.10s\n",
            "epoch 018 | iter 11999 | train loss 2.1354 | dt 0.10s\n",
            "epoch 018 | iter 12199 | train loss 2.1429 | dt 0.11s\n",
            "[EPOCH 018] valid loss=1.7731 | valid ppl=5.89 | valid BLEU=0.4809 | epoch time=2270.7s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4809\n",
            "epoch 019 | iter 00199 | train loss 2.2045 | dt 0.10s\n",
            "epoch 019 | iter 00399 | train loss 2.1917 | dt 0.10s\n",
            "epoch 019 | iter 00599 | train loss 2.2004 | dt 0.10s\n",
            "epoch 019 | iter 00799 | train loss 2.2112 | dt 0.11s\n",
            "epoch 019 | iter 00999 | train loss 2.2182 | dt 0.11s\n",
            "epoch 019 | iter 01199 | train loss 2.1909 | dt 0.11s\n",
            "epoch 019 | iter 01399 | train loss 2.1999 | dt 0.10s\n",
            "epoch 019 | iter 01599 | train loss 2.2063 | dt 0.10s\n",
            "epoch 019 | iter 01799 | train loss 2.2315 | dt 0.10s\n",
            "epoch 019 | iter 01999 | train loss 2.2138 | dt 0.10s\n",
            "epoch 019 | iter 02199 | train loss 2.1941 | dt 0.11s\n",
            "epoch 019 | iter 02399 | train loss 2.2059 | dt 0.10s\n",
            "epoch 019 | iter 02599 | train loss 2.1983 | dt 0.10s\n",
            "epoch 019 | iter 02799 | train loss 2.1969 | dt 0.11s\n",
            "epoch 019 | iter 02999 | train loss 2.2287 | dt 0.10s\n",
            "epoch 019 | iter 03199 | train loss 2.2145 | dt 0.12s\n",
            "epoch 019 | iter 03399 | train loss 2.2031 | dt 0.10s\n",
            "epoch 019 | iter 03599 | train loss 2.2022 | dt 0.10s\n",
            "epoch 019 | iter 03799 | train loss 2.2218 | dt 0.10s\n",
            "epoch 019 | iter 03999 | train loss 2.1806 | dt 0.10s\n",
            "epoch 019 | iter 04199 | train loss 2.1986 | dt 0.11s\n",
            "epoch 019 | iter 04399 | train loss 2.2112 | dt 0.11s\n",
            "epoch 019 | iter 04599 | train loss 2.2179 | dt 0.10s\n",
            "epoch 019 | iter 04799 | train loss 2.1884 | dt 0.11s\n",
            "epoch 019 | iter 04999 | train loss 2.2179 | dt 0.10s\n",
            "epoch 019 | iter 05199 | train loss 2.2314 | dt 0.10s\n",
            "epoch 019 | iter 05399 | train loss 2.2055 | dt 0.10s\n",
            "epoch 019 | iter 05599 | train loss 2.2283 | dt 0.10s\n",
            "epoch 019 | iter 05799 | train loss 2.2103 | dt 0.10s\n",
            "epoch 019 | iter 05999 | train loss 2.2061 | dt 0.09s\n",
            "epoch 019 | iter 06199 | train loss 2.2250 | dt 0.10s\n",
            "epoch 019 | iter 06399 | train loss 2.2155 | dt 0.10s\n",
            "epoch 019 | iter 06599 | train loss 2.2142 | dt 0.10s\n",
            "epoch 019 | iter 06799 | train loss 2.2174 | dt 0.10s\n",
            "epoch 019 | iter 06999 | train loss 2.2038 | dt 0.10s\n",
            "epoch 019 | iter 07199 | train loss 2.2147 | dt 0.10s\n",
            "epoch 019 | iter 07399 | train loss 2.2249 | dt 0.10s\n",
            "epoch 019 | iter 07599 | train loss 2.1960 | dt 0.11s\n",
            "epoch 019 | iter 07799 | train loss 2.2236 | dt 0.10s\n",
            "epoch 019 | iter 07999 | train loss 2.2007 | dt 0.08s\n",
            "epoch 019 | iter 08199 | train loss 2.2217 | dt 0.10s\n",
            "epoch 019 | iter 08399 | train loss 2.2402 | dt 0.11s\n",
            "epoch 019 | iter 08599 | train loss 2.2361 | dt 0.10s\n",
            "epoch 019 | iter 08799 | train loss 2.2312 | dt 0.11s\n",
            "epoch 019 | iter 08999 | train loss 2.2127 | dt 0.11s\n",
            "epoch 019 | iter 09199 | train loss 2.2050 | dt 0.10s\n",
            "epoch 019 | iter 09399 | train loss 2.2325 | dt 0.10s\n",
            "epoch 019 | iter 09599 | train loss 2.2038 | dt 0.11s\n",
            "epoch 019 | iter 09799 | train loss 2.2554 | dt 0.10s\n",
            "epoch 019 | iter 09999 | train loss 2.2385 | dt 0.10s\n",
            "epoch 019 | iter 10199 | train loss 2.1962 | dt 0.10s\n",
            "epoch 019 | iter 10399 | train loss 2.2470 | dt 0.09s\n",
            "epoch 019 | iter 10599 | train loss 2.2270 | dt 0.10s\n",
            "epoch 019 | iter 10799 | train loss 2.1962 | dt 0.11s\n",
            "epoch 019 | iter 10999 | train loss 2.1951 | dt 0.10s\n",
            "epoch 019 | iter 11199 | train loss 2.1437 | dt 0.11s\n",
            "epoch 019 | iter 11399 | train loss 2.1611 | dt 0.10s\n",
            "epoch 019 | iter 11599 | train loss 2.1573 | dt 0.10s\n",
            "epoch 019 | iter 11799 | train loss 2.1289 | dt 0.11s\n",
            "epoch 019 | iter 11999 | train loss 2.1103 | dt 0.10s\n",
            "epoch 019 | iter 12199 | train loss 2.1519 | dt 0.11s\n",
            "[EPOCH 019] valid loss=1.7680 | valid ppl=5.86 | valid BLEU=0.4840 | epoch time=2278.0s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.4840\n"
          ]
        }
      ],
      "source": [
        "import time, math\n",
        "import torch\n",
        "\n",
        "train_loss_hist = []\n",
        "valid_loss_hist = []\n",
        "valid_bleu_hist = []\n",
        "valid_ppl_hist  = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# best_metric init\n",
        "if BEST_BY == \"bleu\":\n",
        "    best_metric = -1e9\n",
        "else:\n",
        "    best_metric = 1e9\n",
        "\n",
        "# ---- Resume nếu có ----\n",
        "if resume_path is not None and os.path.exists(resume_path):\n",
        "    ckpt = load_checkpoint(\n",
        "        resume_path,\n",
        "        model=model,\n",
        "        optimizer=optimizer,  # ScheduledOptim wrapper\n",
        "        map_location=opt[\"device\"] if \"cuda\" in str(opt[\"device\"]) else \"cpu\",\n",
        "        strict=True\n",
        "    )\n",
        "    start_epoch = int(ckpt.get(\"epoch\", -1)) + 1\n",
        "    best_metric = ckpt.get(\"best_metric\", best_metric)\n",
        "\n",
        "    hist = ckpt.get(\"history\", None)\n",
        "    if hist:\n",
        "        train_loss_hist = hist.get(\"train_loss_hist\", train_loss_hist)\n",
        "        valid_loss_hist = hist.get(\"valid_loss_hist\", valid_loss_hist)\n",
        "        valid_bleu_hist = hist.get(\"valid_bleu_hist\", valid_bleu_hist)\n",
        "        valid_ppl_hist  = hist.get(\"valid_ppl_hist\",  valid_ppl_hist)\n",
        "\n",
        "    print(f\"[RESUME] from {resume_path} | start_epoch={start_epoch} | best_metric={best_metric}\")\n",
        "\n",
        "for epoch in range(start_epoch, opt['epochs']):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    n_print = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        t1 = time.time()\n",
        "        loss = step(model, optimizer, batch, criterion)\n",
        "        running += float(loss)\n",
        "        n_print += 1\n",
        "\n",
        "        if (i + 1) % opt['printevery'] == 0:\n",
        "            avg_loss = running / n_print\n",
        "            print(f\"epoch {epoch:03d} | iter {i:05d} | train loss {avg_loss:.4f} | dt {time.time()-t1:.2f}s\")\n",
        "            running = 0.0\n",
        "            n_print = 0\n",
        "\n",
        "    # ----- validation -----\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        vloss = validiate(model, valid_iter, criterion)\n",
        "\n",
        "    vppl = math.exp(vloss) if vloss < 20 else float('inf')  # guard overflow\n",
        "    vbleu = bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)\n",
        "\n",
        "    # lưu history\n",
        "    train_loss_hist.append(None)  # nếu bạn muốn epoch-train-loss thì tính riêng\n",
        "    valid_loss_hist.append(float(vloss))\n",
        "    valid_ppl_hist.append(float(vppl))\n",
        "    valid_bleu_hist.append(float(vbleu))\n",
        "\n",
        "    print(f\"[EPOCH {epoch:03d}] valid loss={vloss:.4f} | valid ppl={vppl:.2f} | valid BLEU={vbleu:.4f} | epoch time={time.time()-t0:.1f}s\")\n",
        "\n",
        "    # ----- Save \"last\" checkpoint -----\n",
        "    history = {\n",
        "        \"train_loss_hist\": train_loss_hist,\n",
        "        \"valid_loss_hist\": valid_loss_hist,\n",
        "        \"valid_bleu_hist\": valid_bleu_hist,\n",
        "        \"valid_ppl_hist\":  valid_ppl_hist,\n",
        "    }\n",
        "\n",
        "    # last (overwrite)\n",
        "    save_checkpoint(\n",
        "        LAST_CKPT,\n",
        "        epoch=epoch,\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        best_metric=best_metric,\n",
        "        history=history,\n",
        "        opt=opt\n",
        "    )\n",
        "\n",
        "    # optional: keep per-epoch file\n",
        "    save_checkpoint(\n",
        "        os.path.join(CKPT_DIR, f\"last_epoch_{epoch:03d}.pt\"),\n",
        "        epoch=epoch,\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        best_metric=best_metric,\n",
        "        history=history,\n",
        "        opt=opt\n",
        "    )\n",
        "\n",
        "    # ----- Save \"best\" checkpoint -----\n",
        "    improved = False\n",
        "    if BEST_BY == \"bleu\":\n",
        "        if vbleu > best_metric:\n",
        "            improved = True\n",
        "            best_metric = float(vbleu)\n",
        "    else:  # BEST_BY == \"loss\"\n",
        "        if vloss < best_metric:\n",
        "            improved = True\n",
        "            best_metric = float(vloss)\n",
        "\n",
        "    if improved:\n",
        "        save_checkpoint(\n",
        "            BEST_CKPT,\n",
        "            epoch=epoch,\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            best_metric=best_metric,\n",
        "            history=history,\n",
        "            opt=opt,\n",
        "            extra={\"best_by\": BEST_BY}\n",
        "        )\n",
        "        print(f\"[BEST] saved -> {BEST_CKPT} | best_{BEST_BY}={best_metric:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[EPOCH 000] valid loss=1.9653 | valid BLEU=0.4356\n",
            "[EPOCH 001] valid loss=1.9417 | valid BLEU=0.4412\n",
            "[EPOCH 002] valid loss=1.9185 | valid BLEU=0.4446\n",
            "[EPOCH 003] valid loss=1.9031 | valid BLEU=0.4496\n",
            "[EPOCH 004] valid loss=1.8849 | valid BLEU=0.4523\n",
            "[EPOCH 005] valid loss=1.8680 | valid BLEU=0.4556\n",
            "[EPOCH 006] valid loss=1.8584 | valid BLEU=0.4613\n",
            "[EPOCH 007] valid loss=1.8488 | valid BLEU=0.4619\n",
            "[EPOCH 008] valid loss=1.8383 | valid BLEU=0.4639\n",
            "[EPOCH 009] valid loss=1.8279 | valid BLEU=0.4640\n",
            "[EPOCH 010] valid loss=1.8202 | valid BLEU=0.4693\n",
            "[EPOCH 011] valid loss=1.8127 | valid BLEU=0.4716\n",
            "[EPOCH 012] valid loss=1.8105 | valid BLEU=0.4716\n",
            "[EPOCH 013] valid loss=1.8002 | valid BLEU=0.4722\n",
            "[EPOCH 014] valid loss=1.7973 | valid BLEU=0.4754\n",
            "[EPOCH 015] valid loss=1.7894 | valid BLEU=0.4776\n",
            "[EPOCH 016] valid loss=1.7851 | valid BLEU=0.4776\n",
            "[EPOCH 017] valid loss=1.7787 | valid BLEU=0.4775\n",
            "[EPOCH 018] valid loss=1.7731 | valid BLEU=0.4809\n",
            "[EPOCH 019] valid loss=1.7680 | valid BLEU=0.4840\n",
            "[EPOCH 019] valid loss=1.7680 | valid BLEU=0.4840\n",
            "[EPOCH 019] valid loss=1.7680 | valid BLEU=0.4840\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import torch\n",
        "\n",
        "CKPT_DIR = Path(\"./checkpoints\")          # dùng /\n",
        "ckpt_paths = sorted(CKPT_DIR.glob(\"*.pt\"))\n",
        "\n",
        "# nếu file bạn dạng last_epoch_001.pt\n",
        "_epoch_re = re.compile(r\"last_epoch_(\\d+)\\.pt$\", re.IGNORECASE)\n",
        "\n",
        "def get_epoch_from_name(p: Path):\n",
        "    m = _epoch_re.search(p.name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def extract_loss_bleu_from_ckpt(ckpt):\n",
        "    \"\"\"\n",
        "    Trả về (valid_loss, valid_bleu) nếu đọc được từ ckpt, không thì (None, None)\n",
        "    Hỗ trợ các format save phổ biến:\n",
        "      - ckpt[\"history\"][\"valid_loss_hist\"], ckpt[\"history\"][\"valid_bleu_hist\"] (lấy phần tử cuối)\n",
        "      - ckpt[\"valid_loss\"], ckpt[\"valid_bleu\"]\n",
        "      - ckpt[\"metrics\"] dict\n",
        "    \"\"\"\n",
        "    vloss = vbleu = None\n",
        "\n",
        "    if isinstance(ckpt, dict):\n",
        "        # 1) history\n",
        "        hist = ckpt.get(\"history\", None)\n",
        "        if isinstance(hist, dict):\n",
        "            vloss_hist = hist.get(\"valid_loss_hist\", None)\n",
        "            vbleu_hist = hist.get(\"valid_bleu_hist\", None)\n",
        "            if isinstance(vloss_hist, list) and len(vloss_hist):\n",
        "                vloss = float(vloss_hist[-1])\n",
        "            if isinstance(vbleu_hist, list) and len(vbleu_hist):\n",
        "                vbleu = float(vbleu_hist[-1])\n",
        "\n",
        "        # 2) direct keys\n",
        "        if vloss is None and \"valid_loss\" in ckpt:\n",
        "            try: vloss = float(ckpt[\"valid_loss\"])\n",
        "            except: pass\n",
        "        if vbleu is None and \"valid_bleu\" in ckpt:\n",
        "            try: vbleu = float(ckpt[\"valid_bleu\"])\n",
        "            except: pass\n",
        "\n",
        "        # 3) metrics dict\n",
        "        metrics = ckpt.get(\"metrics\", None)\n",
        "        if isinstance(metrics, dict):\n",
        "            if vloss is None and \"valid_loss\" in metrics:\n",
        "                try: vloss = float(metrics[\"valid_loss\"])\n",
        "                except: pass\n",
        "            if vbleu is None and \"valid_bleu\" in metrics:\n",
        "                try: vbleu = float(metrics[\"valid_bleu\"])\n",
        "                except: pass\n",
        "\n",
        "    return vloss, vbleu\n",
        "\n",
        "\n",
        "# (tuỳ chọn) sort theo epoch trong tên file cho đẹp; nếu không match thì giữ thứ tự glob\n",
        "def sort_key(p: Path):\n",
        "    e = get_epoch_from_name(p)\n",
        "    return (e is None, e if e is not None else p.name)\n",
        "\n",
        "ckpt_paths = sorted(ckpt_paths, key=sort_key)\n",
        "\n",
        "for p in ckpt_paths:\n",
        "    ckpt = torch.load(p.as_posix(), map_location=\"cpu\")\n",
        "\n",
        "    # epoch: ưu tiên epoch trong tên file, fallback sang ckpt[\"epoch\"] nếu có\n",
        "    epoch = get_epoch_from_name(p)\n",
        "    if epoch is None and isinstance(ckpt, dict) and \"epoch\" in ckpt:\n",
        "        try:\n",
        "            epoch = int(ckpt[\"epoch\"])\n",
        "        except:\n",
        "            epoch = None\n",
        "\n",
        "    vloss, vbleu = extract_loss_bleu_from_ckpt(ckpt)\n",
        "\n",
        "    epoch_str = f\"{epoch:03d}\" if isinstance(epoch, int) else \"???\"\n",
        "    loss_str  = f\"{vloss:.4f}\" if isinstance(vloss, (int, float)) else \"NA\"\n",
        "    bleu_str  = f\"{vbleu:.4f}\" if isinstance(vbleu, (int, float)) else \"NA\"\n",
        "\n",
        "    print(f\"[EPOCH {epoch_str}] valid loss={loss_str} | valid BLEU={bleu_str}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: checkpoints/best.pt | epoch: 19 | best_metric: 0.48396389323447486\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "BEST_CKPT = \"checkpoints/best.pt\"\n",
        "DEVICE = opt[\"device\"]\n",
        "\n",
        "def strip_prefix(sd, prefix):\n",
        "    if not prefix:\n",
        "        return sd\n",
        "    out = {}\n",
        "    for k, v in sd.items():\n",
        "        out[k[len(prefix):] if k.startswith(prefix) else k] = v\n",
        "    return out\n",
        "\n",
        "def load_best(model, ckpt_path=BEST_CKPT, device=DEVICE, strict=True):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    sd = ckpt[\"model_state\"]  # ✅ đúng format của bạn\n",
        "\n",
        "    # nếu từng train bằng DataParallel thì có \"module.\"\n",
        "    sd = strip_prefix(sd, \"module.\")\n",
        "    # đôi khi save prefix \"model.\"\n",
        "    sd = strip_prefix(sd, \"model.\")\n",
        "\n",
        "    model.load_state_dict(sd, strict=strict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return ckpt\n",
        "\n",
        "ckpt = load_best(model)\n",
        "print(\"Loaded:\", BEST_CKPT, \"| epoch:\", ckpt.get(\"epoch\"), \"| best_metric:\", ckpt.get(\"best_metric\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating tst: 100%|██████████| 3000/3000 [17:09<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: test.pred.txt | n_lines: 3000\n",
            "HYP ex: kiến thức, thực hành sử dụng dịch vụ y tế công cộng của người tham gia bảo hiểm y tế và một số yếu tố ảnh hưởng tại vi khí hậu, lào\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def detok_spm_text(s: str) -> str:\n",
        "    s = s.replace(\"▁\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)\n",
        "    return s\n",
        "\n",
        "def normalize_pred(pred):\n",
        "    if isinstance(pred, list):\n",
        "        s = \"\".join(pred) if any(\"▁\" in t for t in pred) else \" \".join(pred)\n",
        "    else:\n",
        "        s = str(pred)\n",
        "\n",
        "    for t in [\"<sos>\", \"<eos>\", \"<pad>\"]:\n",
        "        s = s.replace(t, \" \")\n",
        "    s = \" \".join(s.split()).strip()\n",
        "    return detok_spm_text(s)\n",
        "\n",
        "LIMIT_TRANSLATE = None  # 50 để test nhanh, None=full\n",
        "src_used = valid_src_data if LIMIT_TRANSLATE is None else valid_src_data[:LIMIT_TRANSLATE]\n",
        "\n",
        "test_hyp_vi = []\n",
        "for src in tqdm(src_used, desc=\"Translating tst\"):\n",
        "    pred = translate_sentence(src, model, SRC, TRG, opt[\"device\"], opt[\"k\"], opt[\"max_strlen\"], sp_trg)\n",
        "    test_hyp_vi.append(normalize_pred(pred))\n",
        "\n",
        "OUT_TXT = \"test.pred.txt\"\n",
        "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in test_hyp_vi:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(\"Saved:\", OUT_TXT, \"| n_lines:\", len(test_hyp_vi))\n",
        "print(\"HYP ex:\", test_hyp_vi[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SacreBLEU score: 37.80\n",
            "N-gram precisions: [66.54782635575843, 45.54966028346926, 32.60881503366319, 24.164953605378823]\n"
          ]
        }
      ],
      "source": [
        "import sacrebleu\n",
        "\n",
        "# Đọc file reference\n",
        "with open(\"data/public_test.vi.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    refs = [f.read().splitlines()] # Phải để trong list của list\n",
        "\n",
        "# Đọc file prediction\n",
        "with open(\"test.pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    preds = f.read().splitlines()\n",
        "\n",
        "# Tính toán\n",
        "# Chú ý: mặc định sacrebleu dùng tokenizer '13a' (chuẩn quốc tế)\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "\n",
        "print(f\"SacreBLEU score: {bleu.score:.2f}\")\n",
        "print(f\"N-gram precisions: {bleu.precisions}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "v5E8G0-8QFbj"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'BLEUScore' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_src_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mk\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_strlen\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp_trg\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mTypeError\u001b[39m: 'BLEUScore' object is not callable"
          ]
        }
      ],
      "source": [
        "bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0CwtdJeUNQJo",
        "outputId": "190c4a93-436a-4b00-832b-ae8f4c183fe4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cần có các biện pháp can thiệp sớm để phòng ngừa tiến triển và hạn chế các biến chứng của suy tĩnh mạch chi dưới cho sinh viên điều dưỡng có dấu hiệu sớm suy tĩnh mạch chi dưới và đảm bảo sức khoẻ, an toàn và nâng cao chất lượng cuộc sống cho sinh viên điều dưỡng và nhân viên y tế trong tương lai.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = 'It is necessary to have early intervention measures to prevent the progression and limit the complications of varicose veins of the lower extremities for nursing students with early signs of varicose veins of the lower extremities and ensure health and safety and improve the quality of life for nursing students and medical staffs in the future.'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)\n",
        "trans_sent\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
