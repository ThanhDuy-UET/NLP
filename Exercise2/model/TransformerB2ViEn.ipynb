{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbcquoc/transformer/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVOKeezWPsSs",
        "outputId": "24fe59fa-1dd1-4afd-bb3b-c2015314b9a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "! pip -q install torchtext==0.6.0\n",
        "! pip -q install pyvi\n",
        "! pip -q install sentencepiece\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8gvN64qvNQIS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X9da_ZuSNQIW"
      },
      "outputs": [],
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "# Embedder(100, 512)(torch.LongTensor([1,2,3,4])).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rP64KizDNQIa"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "\n",
        "        # Bảng pe mình vẽ ở trên\n",
        "        for pos in range(max_seq_length):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x*math.sqrt(self.d_model)\n",
        "        seq_length = x.size(1)\n",
        "\n",
        "        pe = Variable(self.pe[:, :seq_length], requires_grad=False)\n",
        "\n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        # cộng embedding vector với pe\n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# PositionalEncoder(512)(torch.rand(5, 30, 512)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2nJMcGuUNQId"
      },
      "outputs": [],
      "source": [
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    q: batch_size x head x seq_length x d_model\n",
        "    k: batch_size x head x seq_length x d_model\n",
        "    v: batch_size x head x seq_length x d_model\n",
        "    mask: batch_size x 1 x 1 x seq_length\n",
        "    output: batch_size x head x seq_length x d_model\n",
        "    \"\"\"\n",
        "\n",
        "    # attention score được tính bằng cách nhân q với k\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "    # xong rồi thì chuẩn hóa bằng softmax\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "\n",
        "    output = torch.matmul(scores, v)\n",
        "    return output, scores\n",
        "\n",
        "# attention(torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ANQ4C3EENQIh"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model//heads\n",
        "        self.h = heads\n",
        "        self.attn = None\n",
        "\n",
        "        # tạo ra 3 ma trận trọng số là q_linear, k_linear, v_linear như hình trên\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \"\"\"\n",
        "        q: batch_size x seq_length x d_model\n",
        "        k: batch_size x seq_length x d_model\n",
        "        v: batch_size x seq_length x d_model\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        bs = q.size(0)\n",
        "        # nhân ma trận trọng số q_linear, k_linear, v_linear với dữ liệu đầu vào q, k, v\n",
        "        # ở bước encode các bạn lưu ý rằng q, k, v chỉ là một (xem hình trên)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # tính attention score\n",
        "        scores, self.attn = attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
        "\n",
        "        output = self.out(concat)\n",
        "        return output\n",
        "\n",
        "# MultiHeadAttention(8, 512)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 30, 512)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n6-_9Hq-NQIk"
      },
      "outputs": [],
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "\n",
        "        self.size = d_model\n",
        "\n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H1ndbdMXNQIn"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Trong kiến trúc của chúng ta có tầng linear\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-Wwo91xDNQIq"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        x: batch_size x seq_length x d_model\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        x2 = self.norm_1(x)\n",
        "        # tính attention value, các bạn để ý q, k, v là giống nhau\n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# EncoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32 , 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6mDt2NPeNQIu"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "\n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        x: batch_size x seq_length x d_model\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        \"\"\"\n",
        "        # Các bạn xem hình trên, kiến trúc mình vẽ với code ở chỗ này tương đương nhau.\n",
        "        x2 = self.norm_1(x)\n",
        "        # multihead attention thứ nhất, chú ý các từ ở target\n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        # masked mulithead attention thứ 2. k, v là giá trị output của mô hình encoder\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# DecoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZcU8nyvzNQIx"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Một encoder có nhiều encoder layer nhé !!!\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Encoder(232, 512,6,8,0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5lBRYMg_NQI0"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"Một decoder có nhiều decoder layer nhé !!!\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        trg: batch_size x seq_length\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Decoder(232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DpxSCRILNQI3"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\" Cuối cùng ghép chúng lại với nhau để được mô hình transformer hoàn chỉnh\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        trg: batch_size x seq_length\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x vocab_size\n",
        "        \"\"\"\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "\n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "\n",
        "# Transformer(232, 232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.LongTensor(32, 30).random_(0, 10),torch.rand(32, 1, 30),torch.rand(32, 1, 30)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "M5tvzW9jNQI6"
      },
      "outputs": [],
      "source": [
        "from torchtext import data\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "\n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NkBjLH96NQI8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def nopeak_mask(size, device):\n",
        "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
        "     mô hình không nhìn thấy được các từ ở tương lai\n",
        "    \"\"\"\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    np_mask = np_mask.to(device)\n",
        "\n",
        "    return np_mask\n",
        "\n",
        "def create_masks(src, trg, src_pad, trg_pad, device):\n",
        "    \"\"\" Tạo mask cho encoder,\n",
        "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào\n",
        "    \"\"\"\n",
        "    src_mask = (src != src_pad).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size, device)\n",
        "        if trg.is_cuda:\n",
        "            np_mask.cuda()\n",
        "        trg_mask = trg_mask & np_mask\n",
        "\n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9YoUVx4xjEb7"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "\n",
        "def get_synonym(word, SRC):\n",
        "    syns = wordnet.synsets(word)\n",
        "    for s in syns:\n",
        "        for l in s.lemmas():\n",
        "            if SRC.vocab.stoi[l.name()] != 0:\n",
        "                return SRC.vocab.stoi[l.name()]\n",
        "\n",
        "    return 0\n",
        "\n",
        "def multiple_replace(dict, text):\n",
        "  # Create a regular expression  from the dictionary keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # For each match, look-up corresponding value in dictionary\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1IJpUEIMgMbw"
      },
      "outputs": [],
      "source": [
        "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
        "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
        "    \"\"\"\n",
        "    init_tok = TRG.vocab.stoi['<sos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "\n",
        "    # tính sẵn output của encoder\n",
        "    e_output = model.encoder(src, src_mask)\n",
        "\n",
        "    outputs = torch.LongTensor([[init_tok]])\n",
        "\n",
        "    outputs = outputs.to(device)\n",
        "\n",
        "    trg_mask = nopeak_mask(1, device)\n",
        "    # dự đoán kí tự đầu tiên\n",
        "    out = model.out(model.decoder(outputs,\n",
        "    e_output, src_mask, trg_mask))\n",
        "    out = F.softmax(out, dim=-1)\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
        "\n",
        "    outputs = torch.zeros(k, max_len).long()\n",
        "    outputs = outputs.to(device)\n",
        "    outputs[:, 0] = init_tok\n",
        "    outputs[:, 1] = ix[0]\n",
        "\n",
        "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
        "\n",
        "    e_outputs = e_outputs.to(device)\n",
        "    e_outputs[:, :] = e_output[0]\n",
        "\n",
        "    return outputs, e_outputs, log_scores\n",
        "\n",
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "\n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "\n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "\n",
        "    return outputs, log_scores\n",
        "\n",
        "def beam_search(src, model, SRC, TRG, device, k, max_len):\n",
        "\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
        "    eos_tok = TRG.vocab.stoi['<eos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "    ind = None\n",
        "    for i in range(2, max_len):\n",
        "\n",
        "        trg_mask = nopeak_mask(i, device)\n",
        "\n",
        "        out = model.out(model.decoder(outputs[:,:i],\n",
        "        e_outputs, src_mask, trg_mask))\n",
        "\n",
        "        out = F.softmax(out, dim=-1)\n",
        "\n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
        "\n",
        "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
        "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
        "        for vec in ones:\n",
        "            i = vec[0]\n",
        "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
        "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
        "\n",
        "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
        "\n",
        "        if num_finished_sentences == k:\n",
        "            alpha = 0.7\n",
        "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
        "            _, ind = torch.max(log_scores * div, 1)\n",
        "            ind = ind.data[0]\n",
        "            break\n",
        "\n",
        "    if ind is None:\n",
        "\n",
        "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
        "\n",
        "    else:\n",
        "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_uVO0yr_NQJC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from torchtext import data\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def clean_text(sentence: str) -> str:\n",
        "    sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "    sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "    sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "    sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "    sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "    sentence = sentence.lower()\n",
        "    return sentence.strip()\n",
        "\n",
        "def read_data(src_file, trg_file):\n",
        "    src_data = open(src_file, encoding=\"utf-8\").read().strip().split('\\n')\n",
        "    trg_data = open(trg_file, encoding=\"utf-8\").read().strip().split('\\n')\n",
        "    return src_data, trg_data\n",
        "\n",
        "\n",
        "def train_spm_bpe_from_lines(lines, model_prefix, vocab_size=16000,\n",
        "                            character_coverage=1.0, user_defined_symbols=None):\n",
        "    tmp = f\"{model_prefix}_corpus.txt\"\n",
        "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
        "        for s in lines:\n",
        "            f.write(clean_text(s) + \"\\n\")   # ✅ train on cleaned text\n",
        "\n",
        "    uds = user_defined_symbols or []\n",
        "    uds_arg = \",\".join(uds) if uds else \"\"\n",
        "\n",
        "    spm.SentencePieceTrainer.Train(\n",
        "        input=tmp,\n",
        "        model_prefix=model_prefix,\n",
        "        vocab_size=vocab_size,\n",
        "        model_type=\"bpe\",\n",
        "        character_coverage=character_coverage,\n",
        "        unk_id=0,\n",
        "        pad_id=-1,\n",
        "        bos_id=-1,\n",
        "        eos_id=-1,\n",
        "        user_defined_symbols=uds_arg if uds_arg else None\n",
        "    )\n",
        "    os.remove(tmp)\n",
        "\n",
        "class tokenize(object):\n",
        "    def __init__(self, model_file):\n",
        "        self.sp = spm.SentencePieceProcessor(model_file=model_file)\n",
        "\n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = clean_text(sentence)\n",
        "        return self.sp.encode(sentence, out_type=str)\n",
        "\n",
        "    def detokenize(self, pieces):\n",
        "        return self.sp.decode(pieces)\n",
        "\n",
        "def create_fields(src_lang, trg_lang):\n",
        "    print(\"loading sentencepiece BPE tokenizers (with cleaning)...\")\n",
        "\n",
        "    t_src = tokenize(src_lang)\n",
        "    t_trg = tokenize(trg_lang)\n",
        "\n",
        "    SRC = data.Field(\n",
        "        lower=False,\n",
        "        tokenize=t_src.tokenizer,\n",
        "        pad_token=\"<pad>\",\n",
        "        unk_token=\"<unk>\"\n",
        "    )\n",
        "\n",
        "    TRG = data.Field(\n",
        "        lower=False,\n",
        "        tokenize=t_trg.tokenizer,\n",
        "        init_token=\"<sos>\",\n",
        "        eos_token=\"<eos>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        unk_token=\"<unk>\"\n",
        "    )\n",
        "\n",
        "    return SRC, TRG, t_src, t_trg\n",
        "\n",
        "def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n",
        "    print(\"creating dataset and iterator... \")\n",
        "\n",
        "    raw_data = {'src': [line for line in src_data], 'trg': [line for line in trg_data]}\n",
        "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
        "\n",
        "    # ✅ lọc đúng theo số token sau preprocess (clean + sentencepiece)\n",
        "    mask = (\n",
        "        df['src'].apply(lambda s: len(SRC.preprocess(s)) < max_strlen) &\n",
        "        df['trg'].apply(lambda s: len(TRG.preprocess(s)) < max_strlen)\n",
        "    )\n",
        "    df = df.loc[mask]\n",
        "\n",
        "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
        "\n",
        "    data_fields = [('src', SRC), ('trg', TRG)]\n",
        "    ds = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
        "\n",
        "    it = MyIterator(\n",
        "        ds, batch_size=batchsize, device=device,\n",
        "        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "        batch_size_fn=batch_size_fn, train=istrain, shuffle=True\n",
        "    )\n",
        "\n",
        "    os.remove('translate_transformer_temp.csv')\n",
        "\n",
        "    if istrain:\n",
        "        SRC.build_vocab(ds)\n",
        "        TRG.build_vocab(ds)\n",
        "\n",
        "    return it\n",
        "\n",
        "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len, sp_trg):\n",
        "    model.eval()\n",
        "\n",
        "    src_tokens = SRC.preprocess(sentence)  # clean + sp encode\n",
        "    unk = SRC.vocab.stoi.get(\"<unk>\", 0)\n",
        "    src_ids = [SRC.vocab.stoi.get(tok, unk) for tok in src_tokens]\n",
        "    src_tensor = torch.LongTensor([src_ids]).to(device)\n",
        "\n",
        "    pred_pieces_str = beam_search(src_tensor, model, SRC, TRG, device, k, max_len)\n",
        "    pieces = pred_pieces_str.split()\n",
        "    pred_text = sp_trg.detokenize(pieces)\n",
        "    return pred_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "l4POJRxdNQJF"
      },
      "outputs": [],
      "source": [
        "def step(model, optimizer,batch, criterion):\n",
        "    \"\"\"\n",
        "    Một lần cập nhật mô hình\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    src = batch.src.transpose(0,1).cuda()\n",
        "    trg = batch.trg.transpose(0,1).cuda()\n",
        "    trg_input = trg[:, :-1]\n",
        "    src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "    preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "    ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "    loss.backward()\n",
        "    optimizer.step_and_update_lr()\n",
        "\n",
        "    loss = loss.item()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "c5sPA-k_NQJI"
      },
      "outputs": [],
      "source": [
        "def validiate(model, valid_iter, criterion):\n",
        "    \"\"\" Tính loss trên tập validation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        total_loss = []\n",
        "        for batch in valid_iter:\n",
        "            src = batch.src.transpose(0,1).cuda()\n",
        "            trg = batch.trg.transpose(0,1).cuda()\n",
        "            trg_input = trg[:, :-1]\n",
        "            src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "            ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "\n",
        "            loss = loss.item()\n",
        "\n",
        "            total_loss.append(loss)\n",
        "\n",
        "    avg_loss = np.mean(total_loss)\n",
        "\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OW8pRq91rwJR"
      },
      "outputs": [],
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.init_lr = init_lr\n",
        "        self.d_model = d_model\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_steps = 0\n",
        "\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients with the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        d_model = self.d_model\n",
        "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
        "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
        "\n",
        "    def state_dict(self):\n",
        "        optimizer_state_dict = {\n",
        "            'init_lr':self.init_lr,\n",
        "            'd_model':self.d_model,\n",
        "            'n_warmup_steps':self.n_warmup_steps,\n",
        "            'n_steps':self.n_steps,\n",
        "            '_optimizer':self._optimizer.state_dict(),\n",
        "        }\n",
        "\n",
        "        return optimizer_state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.init_lr = state_dict['init_lr']\n",
        "        self.d_model = state_dict['d_model']\n",
        "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
        "        self.n_steps = state_dict['n_steps']\n",
        "\n",
        "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LHGeSHThtlj-"
      },
      "outputs": [],
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            # true_dist = pred.data.clone()\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "            true_dist[:, self.padding_idx] = 0\n",
        "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "            if mask.dim() > 0:\n",
        "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Sg257Gk_Kzzw"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def bleu(valid_src_data, valid_trg_data, model, SRC, TRG, device, k, max_strlen, sp_trg):\n",
        "    SPECIALS = {\"<pad>\", \"<sos>\", \"<eos>\"}\n",
        "    \n",
        "    pred_sents = []\n",
        "    for sentence in valid_src_data:\n",
        "        pred_text = translate_sentence(sentence, model, SRC, TRG, device, k, max_strlen, sp_trg)\n",
        "        pred_tok = [t for t in TRG.preprocess(pred_text) if t not in SPECIALS]\n",
        "        pred_sents.append(pred_tok)\n",
        "\n",
        "    trg_sents = []\n",
        "    for sent in valid_trg_data:\n",
        "        ref_tok = [t for t in TRG.preprocess(sent) if t not in SPECIALS]\n",
        "        trg_sents.append([ref_tok])\n",
        "\n",
        "    return bleu_score(pred_sents, trg_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Nhgu-SPTNQJL"
      },
      "outputs": [],
      "source": [
        "opt = {\n",
        "    'train_src_data':'./data/train.vi.txt',\n",
        "    'train_trg_data':'./data/train.en.txt',\n",
        "    'valid_src_data':'./data/public_test.vi.txt',\n",
        "    'valid_trg_data':'./data/public_test.en.txt',\n",
        "\n",
        "    'src_lang': 'spm_vi_bpe.model',\n",
        "    'trg_lang': 'spm_en_bpe.model',\n",
        "\n",
        "    'max_strlen':256,\n",
        "    'batchsize':1500,\n",
        "    'device':'cuda',\n",
        "    'd_model': 512,\n",
        "    'n_layers': 6,\n",
        "    'heads': 8,\n",
        "    'dropout': 0.1,\n",
        "    'lr':0.0001,\n",
        "    'epochs':20,\n",
        "    'printevery': 200,\n",
        "    'k':5,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HOay5MrNQJO",
        "outputId": "6fd8ff36-ff81-43bb-eb10-adec0912dddd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'gdown' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "os.makedirs('./data/', exist_ok=True)\n",
        "! gdown --id 1Fuo_ALIFKlUvOPbK5rUA5OfAS2wKn_95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIWJTjdLNQJR",
        "outputId": "c614ef7d-3073-49b1-c925-1ca2c894942b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "! unzip -o en_vi.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBotIB8pNQJU",
        "outputId": "039ac075-b78f-421d-adfa-3154acd87ba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done: spm_vi_bpe.model spm_en_bpe.model\n",
            "loading sentencepiece BPE tokenizers (with cleaning)...\n",
            "creating dataset and iterator... \n",
            "creating dataset and iterator... \n"
          ]
        }
      ],
      "source": [
        "train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n",
        "valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n",
        "\n",
        "SPECIALS = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "MED_SYMS = [\"mg\", \"ml\", \"mmHg\", \"bpm\", \"°C\", \"mg/dL\", \"HbA1c\", \"BMI\", \"COVID-19\", \"HIV\", \"MRI\", \"CT\"]\n",
        "\n",
        "if not os.path.exists(\"spm_vi_bpe.model\"):\n",
        "    train_spm_bpe_from_lines(train_src_data, \"spm_vi_bpe\",\n",
        "                             vocab_size=16000, character_coverage=1.0,\n",
        "                             user_defined_symbols=SPECIALS + MED_SYMS)\n",
        "\n",
        "if not os.path.exists(\"spm_en_bpe.model\"):\n",
        "    train_spm_bpe_from_lines(train_trg_data, \"spm_en_bpe\",\n",
        "                             vocab_size=16000, character_coverage=1.0,\n",
        "                             user_defined_symbols=SPECIALS + MED_SYMS)\n",
        "\n",
        "print(\"Done:\", \"spm_vi_bpe.model\", \"spm_en_bpe.model\")\n",
        "\n",
        "SRC, TRG, sp_src, sp_trg = create_fields(opt['src_lang'], opt['trg_lang'])\n",
        "\n",
        "train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'],\n",
        "                            opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n",
        "valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'],\n",
        "                            opt['batchsize'], opt['device'], SRC, TRG, istrain=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SRC model: spm_vi_bpe.model\n",
            "TRG model: spm_en_bpe.model\n",
            "SRC tokens: ['▁tôi', '▁bị', '▁sốt', '▁3', '▁ngày', '▁rồi']\n",
            "TRG tokens: ['▁i', '▁have', '▁fever', '▁for', '▁3', '▁days']\n"
          ]
        }
      ],
      "source": [
        "print(\"SRC model:\", opt[\"src_lang\"])\n",
        "print(\"TRG model:\", opt[\"trg_lang\"])\n",
        "print(\"SRC tokens:\", SRC.preprocess(\"Tôi bị sốt 3 ngày rồi\")[:20])\n",
        "print(\"TRG tokens:\", TRG.preprocess(\"I have fever for 3 days\")[:20]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Gnw9xrJeNQJX"
      },
      "outputs": [],
      "source": [
        "src_pad = SRC.vocab.stoi['<pad>']\n",
        "trg_pad = TRG.vocab.stoi['<pad>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5RccNL8VNQJd"
      },
      "outputs": [],
      "source": [
        "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt['d_model'], opt['n_layers'], opt['heads'], opt['dropout'])\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(opt['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "12debLGiNQJg"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = ScheduledOptim(\n",
        "        torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "        0.2, opt['d_model'], 4000)\n",
        "\n",
        "criterion = LabelSmoothingLoss(len(TRG.vocab), padding_idx=trg_pad, smoothing=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, glob, re\n",
        "import torch\n",
        "\n",
        "def save_checkpoint(path, epoch, model, optimizer, best_metric, history, opt, extra=None):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    ckpt = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": optimizer._optimizer.state_dict() if hasattr(optimizer, \"_optimizer\") else optimizer.state_dict(),\n",
        "        \"best_metric\": best_metric,\n",
        "        \"history\": history,\n",
        "        \"opt\": opt,\n",
        "        \"extra\": extra or {}\n",
        "    }\n",
        "    # ScheduledOptim có n_steps riêng -> lưu để resume LR schedule đúng\n",
        "    if hasattr(optimizer, \"n_steps\"):\n",
        "        ckpt[\"sched_n_steps\"] = optimizer.n_steps\n",
        "\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "def load_checkpoint(path, model, optimizer=None, map_location=\"cpu\", strict=True):\n",
        "    ckpt = torch.load(path, map_location=map_location)\n",
        "    model.load_state_dict(ckpt[\"model_state\"], strict=strict)\n",
        "\n",
        "    if optimizer is not None:\n",
        "        # restore optimizer state\n",
        "        if hasattr(optimizer, \"_optimizer\"):\n",
        "            optimizer._optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "        else:\n",
        "            optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\n",
        "        # restore ScheduledOptim step count\n",
        "        if hasattr(optimizer, \"n_steps\") and \"sched_n_steps\" in ckpt:\n",
        "            optimizer.n_steps = ckpt[\"sched_n_steps\"]\n",
        "\n",
        "    return ckpt\n",
        "\n",
        "def find_latest_checkpoint(ckpt_dir, prefix=\"last_epoch_\", ext=\".pt\"):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        return None\n",
        "    files = glob.glob(os.path.join(ckpt_dir, f\"{prefix}*{ext}\"))\n",
        "    if not files:\n",
        "        return None\n",
        "    # sort by epoch number in filename\n",
        "    def get_epoch(f):\n",
        "        m = re.search(rf\"{re.escape(prefix)}(\\d+){re.escape(ext)}$\", os.path.basename(f))\n",
        "        return int(m.group(1)) if m else -1\n",
        "    files = sorted(files, key=get_epoch)\n",
        "    return files[-1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "CKPT_DIR = \"./checkpoints\"\n",
        "LAST_CKPT = os.path.join(CKPT_DIR, \"last.pt\")\n",
        "BEST_CKPT = os.path.join(CKPT_DIR, \"best.pt\")\n",
        "\n",
        "# chọn tiêu chí lưu best:\n",
        "# - \"bleu\": maximize\n",
        "# - \"loss\": minimize\n",
        "BEST_BY = \"bleu\"   # hoặc \"loss\"\n",
        "\n",
        "resume_path = find_latest_checkpoint(CKPT_DIR, prefix=\"last_epoch_\", ext=\".pt\") # ví dụ: \"./checkpoints/last.pt\" hoặc None\n",
        "# nếu muốn auto-resume từ checkpoint mới nhất:\n",
        "# resume_path = find_latest_checkpoint(CKPT_DIR, prefix=\"last_epoch_\", ext=\".pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "JeZqfQPANQJl",
        "outputId": "467a5beb-217b-47b3-8afc-4f0dc28426a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RESUME] from ./checkpoints\\last_epoch_002.pt | start_epoch=3 | best_metric=0.1913117915391922\n",
            "epoch 003 | iter 00199 | train loss 3.0035 | dt 0.13s\n",
            "epoch 003 | iter 00399 | train loss 3.0140 | dt 0.16s\n",
            "epoch 003 | iter 00599 | train loss 3.0174 | dt 0.15s\n",
            "epoch 003 | iter 00799 | train loss 2.9974 | dt 0.16s\n",
            "epoch 003 | iter 00999 | train loss 2.9783 | dt 0.15s\n",
            "epoch 003 | iter 01199 | train loss 2.9437 | dt 0.15s\n",
            "epoch 003 | iter 01399 | train loss 3.0187 | dt 0.14s\n",
            "epoch 003 | iter 01599 | train loss 3.0067 | dt 0.15s\n",
            "epoch 003 | iter 01799 | train loss 3.0188 | dt 0.17s\n",
            "epoch 003 | iter 01999 | train loss 2.9824 | dt 0.17s\n",
            "epoch 003 | iter 02199 | train loss 2.9683 | dt 0.16s\n",
            "epoch 003 | iter 02399 | train loss 2.9811 | dt 0.16s\n",
            "epoch 003 | iter 02599 | train loss 3.0148 | dt 0.17s\n",
            "epoch 003 | iter 02799 | train loss 2.9554 | dt 0.17s\n",
            "epoch 003 | iter 02999 | train loss 3.0380 | dt 0.17s\n",
            "epoch 003 | iter 03199 | train loss 2.9762 | dt 0.17s\n",
            "epoch 003 | iter 03399 | train loss 3.0178 | dt 0.16s\n",
            "epoch 003 | iter 03599 | train loss 2.9717 | dt 0.17s\n",
            "epoch 003 | iter 03799 | train loss 2.9721 | dt 0.16s\n",
            "epoch 003 | iter 03999 | train loss 2.9224 | dt 0.17s\n",
            "epoch 003 | iter 04199 | train loss 2.8929 | dt 0.17s\n",
            "epoch 003 | iter 04399 | train loss 2.9619 | dt 0.15s\n",
            "epoch 003 | iter 04599 | train loss 3.0380 | dt 0.16s\n",
            "epoch 003 | iter 04799 | train loss 3.0021 | dt 0.17s\n",
            "epoch 003 | iter 04999 | train loss 3.0106 | dt 0.16s\n",
            "epoch 003 | iter 05199 | train loss 2.9590 | dt 0.16s\n",
            "epoch 003 | iter 05399 | train loss 3.0024 | dt 0.15s\n",
            "epoch 003 | iter 05599 | train loss 2.9836 | dt 0.15s\n",
            "epoch 003 | iter 05799 | train loss 2.9412 | dt 0.17s\n",
            "epoch 003 | iter 05999 | train loss 3.0005 | dt 0.14s\n",
            "epoch 003 | iter 06199 | train loss 2.9791 | dt 0.17s\n",
            "epoch 003 | iter 06399 | train loss 2.9881 | dt 0.15s\n",
            "epoch 003 | iter 06599 | train loss 2.9345 | dt 0.15s\n",
            "epoch 003 | iter 06799 | train loss 2.9726 | dt 0.16s\n",
            "epoch 003 | iter 06999 | train loss 2.9932 | dt 0.16s\n",
            "epoch 003 | iter 07199 | train loss 2.9733 | dt 0.16s\n",
            "epoch 003 | iter 07399 | train loss 2.9492 | dt 0.17s\n",
            "epoch 003 | iter 07599 | train loss 2.9885 | dt 0.15s\n",
            "epoch 003 | iter 07799 | train loss 2.9440 | dt 0.15s\n",
            "epoch 003 | iter 07999 | train loss 2.9162 | dt 0.16s\n",
            "epoch 003 | iter 08199 | train loss 2.9621 | dt 0.15s\n",
            "epoch 003 | iter 08399 | train loss 2.9659 | dt 0.16s\n",
            "epoch 003 | iter 08599 | train loss 2.9141 | dt 0.16s\n",
            "epoch 003 | iter 08799 | train loss 2.9293 | dt 0.16s\n",
            "epoch 003 | iter 08999 | train loss 2.9221 | dt 0.17s\n",
            "epoch 003 | iter 09199 | train loss 2.9701 | dt 0.15s\n",
            "epoch 003 | iter 09399 | train loss 2.8942 | dt 0.16s\n",
            "epoch 003 | iter 09599 | train loss 2.9679 | dt 0.16s\n",
            "epoch 003 | iter 09799 | train loss 2.9607 | dt 0.14s\n",
            "epoch 003 | iter 09999 | train loss 2.9495 | dt 0.16s\n",
            "epoch 003 | iter 10199 | train loss 2.9538 | dt 0.16s\n",
            "epoch 003 | iter 10399 | train loss 2.9574 | dt 0.17s\n",
            "epoch 003 | iter 10599 | train loss 2.8572 | dt 0.17s\n",
            "epoch 003 | iter 10799 | train loss 2.7799 | dt 0.16s\n",
            "epoch 003 | iter 10999 | train loss 2.8363 | dt 0.15s\n",
            "epoch 003 | iter 11199 | train loss 2.8091 | dt 0.16s\n",
            "epoch 003 | iter 11399 | train loss 2.8004 | dt 0.17s\n",
            "epoch 003 | iter 11599 | train loss 2.7872 | dt 0.16s\n",
            "[EPOCH 003] valid loss=2.1214 | valid ppl=8.34 | valid BLEU=0.3169 | epoch time=2955.1s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.3169\n",
            "epoch 004 | iter 00199 | train loss 2.8296 | dt 0.17s\n",
            "epoch 004 | iter 00399 | train loss 2.8722 | dt 0.17s\n",
            "epoch 004 | iter 00599 | train loss 2.8954 | dt 0.17s\n",
            "epoch 004 | iter 00799 | train loss 2.8852 | dt 0.16s\n",
            "epoch 004 | iter 00999 | train loss 2.8772 | dt 0.19s\n",
            "epoch 004 | iter 01199 | train loss 2.8386 | dt 0.16s\n",
            "epoch 004 | iter 01399 | train loss 2.8964 | dt 0.18s\n",
            "epoch 004 | iter 01599 | train loss 2.8540 | dt 0.19s\n",
            "epoch 004 | iter 01799 | train loss 2.8504 | dt 0.17s\n",
            "epoch 004 | iter 01999 | train loss 2.8483 | dt 0.13s\n",
            "epoch 004 | iter 02199 | train loss 2.8438 | dt 0.13s\n",
            "epoch 004 | iter 02399 | train loss 2.8378 | dt 0.13s\n",
            "epoch 004 | iter 02599 | train loss 2.8788 | dt 0.13s\n",
            "epoch 004 | iter 02799 | train loss 2.8322 | dt 0.19s\n",
            "epoch 004 | iter 02999 | train loss 2.8640 | dt 0.13s\n",
            "epoch 004 | iter 03199 | train loss 2.8549 | dt 0.13s\n",
            "epoch 004 | iter 03399 | train loss 2.8345 | dt 0.10s\n",
            "epoch 004 | iter 03599 | train loss 2.7996 | dt 0.10s\n",
            "epoch 004 | iter 03799 | train loss 2.8935 | dt 0.11s\n",
            "epoch 004 | iter 03999 | train loss 2.8556 | dt 0.11s\n",
            "epoch 004 | iter 04199 | train loss 2.8301 | dt 0.12s\n",
            "epoch 004 | iter 04399 | train loss 2.8473 | dt 0.10s\n",
            "epoch 004 | iter 04599 | train loss 2.8671 | dt 0.11s\n",
            "epoch 004 | iter 04799 | train loss 2.8401 | dt 0.10s\n",
            "epoch 004 | iter 04999 | train loss 2.8431 | dt 0.11s\n",
            "epoch 004 | iter 05199 | train loss 2.8839 | dt 0.11s\n",
            "epoch 004 | iter 05399 | train loss 2.8464 | dt 0.10s\n",
            "epoch 004 | iter 05599 | train loss 2.8382 | dt 0.11s\n",
            "epoch 004 | iter 05799 | train loss 2.8481 | dt 0.11s\n",
            "epoch 004 | iter 05999 | train loss 2.8470 | dt 0.11s\n",
            "epoch 004 | iter 06199 | train loss 2.8612 | dt 0.11s\n",
            "epoch 004 | iter 06399 | train loss 2.8579 | dt 0.10s\n",
            "epoch 004 | iter 06599 | train loss 2.8299 | dt 0.10s\n",
            "epoch 004 | iter 06799 | train loss 2.8335 | dt 0.10s\n",
            "epoch 004 | iter 06999 | train loss 2.7778 | dt 0.11s\n",
            "epoch 004 | iter 07199 | train loss 2.8440 | dt 0.12s\n",
            "epoch 004 | iter 07399 | train loss 2.8166 | dt 0.11s\n",
            "epoch 004 | iter 07599 | train loss 2.8214 | dt 0.10s\n",
            "epoch 004 | iter 07799 | train loss 2.8172 | dt 0.12s\n",
            "epoch 004 | iter 07999 | train loss 2.8412 | dt 0.10s\n",
            "epoch 004 | iter 08199 | train loss 2.8241 | dt 0.12s\n",
            "epoch 004 | iter 08399 | train loss 2.8529 | dt 0.10s\n",
            "epoch 004 | iter 08599 | train loss 2.8165 | dt 0.11s\n",
            "epoch 004 | iter 08799 | train loss 2.8436 | dt 0.14s\n",
            "epoch 004 | iter 08999 | train loss 2.8345 | dt 0.13s\n",
            "epoch 004 | iter 09199 | train loss 2.8790 | dt 0.14s\n",
            "epoch 004 | iter 09399 | train loss 2.8404 | dt 0.11s\n",
            "epoch 004 | iter 09599 | train loss 2.8593 | dt 0.12s\n",
            "epoch 004 | iter 09799 | train loss 2.8785 | dt 0.11s\n",
            "epoch 004 | iter 09999 | train loss 2.8450 | dt 0.11s\n",
            "epoch 004 | iter 10199 | train loss 2.8531 | dt 0.11s\n",
            "epoch 004 | iter 10399 | train loss 2.8376 | dt 0.12s\n",
            "epoch 004 | iter 10599 | train loss 2.7841 | dt 0.10s\n",
            "epoch 004 | iter 10799 | train loss 2.6851 | dt 0.11s\n",
            "epoch 004 | iter 10999 | train loss 2.6549 | dt 0.12s\n",
            "epoch 004 | iter 11199 | train loss 2.6611 | dt 0.11s\n",
            "epoch 004 | iter 11399 | train loss 2.7085 | dt 0.10s\n",
            "epoch 004 | iter 11599 | train loss 2.7241 | dt 0.11s\n",
            "[EPOCH 004] valid loss=2.0659 | valid ppl=7.89 | valid BLEU=0.3325 | epoch time=2207.1s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.3325\n",
            "epoch 005 | iter 00199 | train loss 2.7418 | dt 0.11s\n",
            "epoch 005 | iter 00399 | train loss 2.7679 | dt 0.11s\n",
            "epoch 005 | iter 00599 | train loss 2.7343 | dt 0.11s\n",
            "epoch 005 | iter 00799 | train loss 2.7353 | dt 0.11s\n",
            "epoch 005 | iter 00999 | train loss 2.7622 | dt 0.11s\n",
            "epoch 005 | iter 01199 | train loss 2.7633 | dt 0.12s\n",
            "epoch 005 | iter 01399 | train loss 2.7659 | dt 0.11s\n",
            "epoch 005 | iter 01599 | train loss 2.7552 | dt 0.11s\n",
            "epoch 005 | iter 01799 | train loss 2.7764 | dt 0.11s\n",
            "epoch 005 | iter 01999 | train loss 2.7889 | dt 0.10s\n",
            "epoch 005 | iter 02199 | train loss 2.7547 | dt 0.12s\n",
            "epoch 005 | iter 02399 | train loss 2.7584 | dt 0.12s\n",
            "epoch 005 | iter 02599 | train loss 2.7700 | dt 0.11s\n",
            "epoch 005 | iter 02799 | train loss 2.7789 | dt 0.11s\n",
            "epoch 005 | iter 02999 | train loss 2.7780 | dt 0.11s\n",
            "epoch 005 | iter 03199 | train loss 2.7695 | dt 0.11s\n",
            "epoch 005 | iter 03399 | train loss 2.7396 | dt 0.10s\n",
            "epoch 005 | iter 03599 | train loss 2.7907 | dt 0.10s\n",
            "epoch 005 | iter 03799 | train loss 2.7565 | dt 0.11s\n",
            "epoch 005 | iter 03999 | train loss 2.7413 | dt 0.11s\n",
            "epoch 005 | iter 04199 | train loss 2.7785 | dt 0.10s\n",
            "epoch 005 | iter 04399 | train loss 2.7815 | dt 0.12s\n",
            "epoch 005 | iter 04599 | train loss 2.7062 | dt 0.12s\n",
            "epoch 005 | iter 04799 | train loss 2.7303 | dt 0.11s\n",
            "epoch 005 | iter 04999 | train loss 2.7532 | dt 0.11s\n",
            "epoch 005 | iter 05199 | train loss 2.7476 | dt 0.11s\n",
            "epoch 005 | iter 05399 | train loss 2.7449 | dt 0.11s\n",
            "epoch 005 | iter 05599 | train loss 2.7537 | dt 0.12s\n",
            "epoch 005 | iter 05799 | train loss 2.7527 | dt 0.11s\n",
            "epoch 005 | iter 05999 | train loss 2.7357 | dt 0.10s\n",
            "epoch 005 | iter 06199 | train loss 2.7862 | dt 0.11s\n",
            "epoch 005 | iter 06399 | train loss 2.7443 | dt 0.12s\n",
            "epoch 005 | iter 06599 | train loss 2.7727 | dt 0.12s\n",
            "epoch 005 | iter 06799 | train loss 2.7646 | dt 0.11s\n",
            "epoch 005 | iter 06999 | train loss 2.7770 | dt 0.10s\n",
            "epoch 005 | iter 07199 | train loss 2.7290 | dt 0.12s\n",
            "epoch 005 | iter 07399 | train loss 2.7325 | dt 0.10s\n",
            "epoch 005 | iter 07599 | train loss 2.7458 | dt 0.11s\n",
            "epoch 005 | iter 07799 | train loss 2.7619 | dt 0.11s\n",
            "epoch 005 | iter 07999 | train loss 2.7657 | dt 0.11s\n",
            "epoch 005 | iter 08199 | train loss 2.7604 | dt 0.12s\n",
            "epoch 005 | iter 08399 | train loss 2.7653 | dt 0.13s\n",
            "epoch 005 | iter 08599 | train loss 2.7982 | dt 0.11s\n",
            "epoch 005 | iter 08799 | train loss 2.7342 | dt 0.11s\n",
            "epoch 005 | iter 08999 | train loss 2.7504 | dt 0.11s\n",
            "epoch 005 | iter 09199 | train loss 2.7162 | dt 0.11s\n",
            "epoch 005 | iter 09399 | train loss 2.7483 | dt 0.12s\n",
            "epoch 005 | iter 09599 | train loss 2.7595 | dt 0.11s\n",
            "epoch 005 | iter 09799 | train loss 2.7416 | dt 0.10s\n",
            "epoch 005 | iter 09999 | train loss 2.7235 | dt 0.11s\n",
            "epoch 005 | iter 10199 | train loss 2.7561 | dt 0.12s\n",
            "epoch 005 | iter 10399 | train loss 2.7466 | dt 0.11s\n",
            "epoch 005 | iter 10599 | train loss 2.6899 | dt 0.11s\n",
            "epoch 005 | iter 10799 | train loss 2.5964 | dt 0.12s\n",
            "epoch 005 | iter 10999 | train loss 2.6095 | dt 0.11s\n",
            "epoch 005 | iter 11199 | train loss 2.6452 | dt 0.11s\n",
            "epoch 005 | iter 11399 | train loss 2.6510 | dt 0.12s\n",
            "epoch 005 | iter 11599 | train loss 2.6133 | dt 0.12s\n",
            "[EPOCH 005] valid loss=2.0304 | valid ppl=7.62 | valid BLEU=0.3405 | epoch time=2034.8s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.3405\n",
            "epoch 006 | iter 00199 | train loss 2.6786 | dt 0.10s\n",
            "epoch 006 | iter 00399 | train loss 2.6991 | dt 0.12s\n",
            "epoch 006 | iter 00599 | train loss 2.6921 | dt 0.11s\n",
            "epoch 006 | iter 00799 | train loss 2.6664 | dt 0.11s\n",
            "epoch 006 | iter 00999 | train loss 2.6626 | dt 0.10s\n",
            "epoch 006 | iter 01199 | train loss 2.6737 | dt 0.12s\n",
            "epoch 006 | iter 01399 | train loss 2.7111 | dt 0.11s\n",
            "epoch 006 | iter 01599 | train loss 2.6467 | dt 0.11s\n",
            "epoch 006 | iter 01799 | train loss 2.6701 | dt 0.11s\n",
            "epoch 006 | iter 01999 | train loss 2.6268 | dt 0.11s\n",
            "epoch 006 | iter 02199 | train loss 2.7069 | dt 0.11s\n",
            "epoch 006 | iter 02399 | train loss 2.7144 | dt 0.12s\n",
            "epoch 006 | iter 02599 | train loss 2.6855 | dt 0.10s\n",
            "epoch 006 | iter 02799 | train loss 2.6791 | dt 0.12s\n",
            "epoch 006 | iter 02999 | train loss 2.6961 | dt 0.10s\n",
            "epoch 006 | iter 03199 | train loss 2.7042 | dt 0.09s\n",
            "epoch 006 | iter 03399 | train loss 2.7050 | dt 0.12s\n",
            "epoch 006 | iter 03599 | train loss 2.6748 | dt 0.11s\n",
            "epoch 006 | iter 03799 | train loss 2.6983 | dt 0.12s\n",
            "epoch 006 | iter 03999 | train loss 2.6867 | dt 0.12s\n",
            "epoch 006 | iter 04199 | train loss 2.6734 | dt 0.11s\n",
            "epoch 006 | iter 04399 | train loss 2.6897 | dt 0.11s\n",
            "epoch 006 | iter 04599 | train loss 2.7237 | dt 0.11s\n",
            "epoch 006 | iter 04799 | train loss 2.7025 | dt 0.12s\n",
            "epoch 006 | iter 04999 | train loss 2.6848 | dt 0.12s\n",
            "epoch 006 | iter 05199 | train loss 2.6753 | dt 0.11s\n",
            "epoch 006 | iter 05399 | train loss 2.7088 | dt 0.10s\n",
            "epoch 006 | iter 05599 | train loss 2.6540 | dt 0.11s\n",
            "epoch 006 | iter 05799 | train loss 2.6988 | dt 0.11s\n",
            "epoch 006 | iter 05999 | train loss 2.6675 | dt 0.11s\n",
            "epoch 006 | iter 06199 | train loss 2.7061 | dt 0.10s\n",
            "epoch 006 | iter 06399 | train loss 2.6908 | dt 0.11s\n",
            "epoch 006 | iter 06599 | train loss 2.6794 | dt 0.11s\n",
            "epoch 006 | iter 06799 | train loss 2.7027 | dt 0.12s\n",
            "epoch 006 | iter 06999 | train loss 2.6519 | dt 0.11s\n",
            "epoch 006 | iter 07199 | train loss 2.6561 | dt 0.10s\n",
            "epoch 006 | iter 07399 | train loss 2.6668 | dt 0.12s\n",
            "epoch 006 | iter 07599 | train loss 2.6682 | dt 0.10s\n",
            "epoch 006 | iter 07799 | train loss 2.6668 | dt 0.11s\n",
            "epoch 006 | iter 07999 | train loss 2.6956 | dt 0.11s\n",
            "epoch 006 | iter 08199 | train loss 2.6887 | dt 0.10s\n",
            "epoch 006 | iter 08399 | train loss 2.6786 | dt 0.11s\n",
            "epoch 006 | iter 08599 | train loss 2.7094 | dt 0.12s\n",
            "epoch 006 | iter 08799 | train loss 2.6684 | dt 0.11s\n",
            "epoch 006 | iter 08999 | train loss 2.6995 | dt 0.11s\n",
            "epoch 006 | iter 09199 | train loss 2.6549 | dt 0.11s\n",
            "epoch 006 | iter 09399 | train loss 2.6855 | dt 0.12s\n",
            "epoch 006 | iter 09599 | train loss 2.6842 | dt 0.12s\n",
            "epoch 006 | iter 09799 | train loss 2.7049 | dt 0.12s\n",
            "epoch 006 | iter 09999 | train loss 2.6660 | dt 0.10s\n",
            "epoch 006 | iter 10199 | train loss 2.6696 | dt 0.11s\n",
            "epoch 006 | iter 10399 | train loss 2.6914 | dt 0.11s\n",
            "epoch 006 | iter 10599 | train loss 2.5655 | dt 0.12s\n",
            "epoch 006 | iter 10799 | train loss 2.5631 | dt 0.12s\n",
            "epoch 006 | iter 10999 | train loss 2.5956 | dt 0.10s\n",
            "epoch 006 | iter 11199 | train loss 2.6069 | dt 0.08s\n",
            "epoch 006 | iter 11399 | train loss 2.5473 | dt 0.11s\n",
            "epoch 006 | iter 11599 | train loss 2.5595 | dt 0.11s\n",
            "[EPOCH 006] valid loss=1.9944 | valid ppl=7.35 | valid BLEU=0.3505 | epoch time=2060.2s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.3505\n",
            "epoch 007 | iter 00199 | train loss 2.6093 | dt 0.10s\n",
            "epoch 007 | iter 00399 | train loss 2.5907 | dt 0.12s\n",
            "epoch 007 | iter 00599 | train loss 2.6295 | dt 0.12s\n",
            "epoch 007 | iter 00799 | train loss 2.5891 | dt 0.10s\n",
            "epoch 007 | iter 00999 | train loss 2.6130 | dt 0.11s\n",
            "epoch 007 | iter 01199 | train loss 2.6357 | dt 0.11s\n",
            "epoch 007 | iter 01399 | train loss 2.6256 | dt 0.11s\n",
            "epoch 007 | iter 01599 | train loss 2.6414 | dt 0.10s\n",
            "epoch 007 | iter 01799 | train loss 2.6153 | dt 0.10s\n",
            "epoch 007 | iter 01999 | train loss 2.6609 | dt 0.12s\n",
            "epoch 007 | iter 02199 | train loss 2.6598 | dt 0.11s\n",
            "epoch 007 | iter 02399 | train loss 2.6183 | dt 0.11s\n",
            "epoch 007 | iter 02599 | train loss 2.6388 | dt 0.09s\n",
            "epoch 007 | iter 02799 | train loss 2.6113 | dt 0.11s\n",
            "epoch 007 | iter 02999 | train loss 2.6058 | dt 0.11s\n",
            "epoch 007 | iter 03199 | train loss 2.5815 | dt 0.10s\n",
            "epoch 007 | iter 03399 | train loss 2.5918 | dt 0.11s\n",
            "epoch 007 | iter 03599 | train loss 2.5819 | dt 0.10s\n",
            "epoch 007 | iter 03799 | train loss 2.6643 | dt 0.11s\n",
            "epoch 007 | iter 03999 | train loss 2.6166 | dt 0.11s\n",
            "epoch 007 | iter 04199 | train loss 2.6270 | dt 0.11s\n",
            "epoch 007 | iter 04399 | train loss 2.6316 | dt 0.11s\n",
            "epoch 007 | iter 04599 | train loss 2.6086 | dt 0.11s\n",
            "epoch 007 | iter 04799 | train loss 2.6554 | dt 0.12s\n",
            "epoch 007 | iter 04999 | train loss 2.6460 | dt 0.11s\n",
            "epoch 007 | iter 05199 | train loss 2.6244 | dt 0.11s\n",
            "epoch 007 | iter 05399 | train loss 2.5777 | dt 0.12s\n",
            "epoch 007 | iter 05599 | train loss 2.6423 | dt 0.12s\n",
            "epoch 007 | iter 05799 | train loss 2.6134 | dt 0.11s\n",
            "epoch 007 | iter 05999 | train loss 2.6189 | dt 0.11s\n",
            "epoch 007 | iter 06199 | train loss 2.6449 | dt 0.10s\n",
            "epoch 007 | iter 06399 | train loss 2.6307 | dt 0.11s\n",
            "epoch 007 | iter 06599 | train loss 2.6424 | dt 0.12s\n",
            "epoch 007 | iter 06799 | train loss 2.6406 | dt 0.12s\n",
            "epoch 007 | iter 06999 | train loss 2.6036 | dt 0.11s\n",
            "epoch 007 | iter 07199 | train loss 2.6156 | dt 0.11s\n",
            "epoch 007 | iter 07399 | train loss 2.6072 | dt 0.11s\n",
            "epoch 007 | iter 07599 | train loss 2.6190 | dt 0.11s\n",
            "epoch 007 | iter 07799 | train loss 2.6190 | dt 0.12s\n",
            "epoch 007 | iter 07999 | train loss 2.6195 | dt 0.11s\n",
            "epoch 007 | iter 08199 | train loss 2.6111 | dt 0.10s\n",
            "epoch 007 | iter 08399 | train loss 2.6140 | dt 0.12s\n",
            "epoch 007 | iter 08599 | train loss 2.6647 | dt 0.10s\n",
            "epoch 007 | iter 08799 | train loss 2.6369 | dt 0.11s\n",
            "epoch 007 | iter 08999 | train loss 2.6317 | dt 0.11s\n",
            "epoch 007 | iter 09199 | train loss 2.6323 | dt 0.11s\n",
            "epoch 007 | iter 09399 | train loss 2.6371 | dt 0.10s\n",
            "epoch 007 | iter 09599 | train loss 2.6587 | dt 0.11s\n",
            "epoch 007 | iter 09799 | train loss 2.5991 | dt 0.10s\n",
            "epoch 007 | iter 09999 | train loss 2.6137 | dt 0.11s\n",
            "epoch 007 | iter 10199 | train loss 2.6840 | dt 0.12s\n",
            "epoch 007 | iter 10399 | train loss 2.6206 | dt 0.12s\n",
            "epoch 007 | iter 10599 | train loss 2.5681 | dt 0.12s\n",
            "epoch 007 | iter 10799 | train loss 2.5100 | dt 0.10s\n",
            "epoch 007 | iter 10999 | train loss 2.4841 | dt 0.11s\n",
            "epoch 007 | iter 11199 | train loss 2.5541 | dt 0.12s\n",
            "epoch 007 | iter 11399 | train loss 2.5342 | dt 0.12s\n",
            "epoch 007 | iter 11599 | train loss 2.5047 | dt 0.10s\n",
            "[EPOCH 007] valid loss=1.9677 | valid ppl=7.15 | valid BLEU=0.3588 | epoch time=2036.7s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.3588\n",
            "epoch 008 | iter 00199 | train loss 2.5761 | dt 0.10s\n",
            "epoch 008 | iter 00399 | train loss 2.5513 | dt 0.11s\n",
            "epoch 008 | iter 00599 | train loss 2.6002 | dt 0.10s\n",
            "epoch 008 | iter 00799 | train loss 2.5837 | dt 0.10s\n",
            "epoch 008 | iter 00999 | train loss 2.5488 | dt 0.11s\n",
            "epoch 008 | iter 01199 | train loss 2.5517 | dt 0.11s\n",
            "epoch 008 | iter 01399 | train loss 2.5768 | dt 0.11s\n",
            "epoch 008 | iter 01599 | train loss 2.5754 | dt 0.10s\n",
            "epoch 008 | iter 01799 | train loss 2.6085 | dt 0.11s\n",
            "epoch 008 | iter 01999 | train loss 2.5734 | dt 0.11s\n",
            "epoch 008 | iter 02199 | train loss 2.5377 | dt 0.11s\n",
            "epoch 008 | iter 02399 | train loss 2.5375 | dt 0.11s\n",
            "epoch 008 | iter 02599 | train loss 2.5793 | dt 0.11s\n",
            "epoch 008 | iter 02799 | train loss 2.5976 | dt 0.12s\n",
            "epoch 008 | iter 02999 | train loss 2.5760 | dt 0.11s\n",
            "epoch 008 | iter 03199 | train loss 2.5879 | dt 0.10s\n",
            "epoch 008 | iter 03399 | train loss 2.5487 | dt 0.11s\n",
            "epoch 008 | iter 03599 | train loss 2.5514 | dt 0.11s\n",
            "epoch 008 | iter 03799 | train loss 2.5940 | dt 0.10s\n",
            "epoch 008 | iter 03999 | train loss 2.5828 | dt 0.11s\n",
            "epoch 008 | iter 04199 | train loss 2.6029 | dt 0.11s\n",
            "epoch 008 | iter 04399 | train loss 2.6053 | dt 0.12s\n",
            "epoch 008 | iter 04599 | train loss 2.5693 | dt 0.12s\n",
            "epoch 008 | iter 04799 | train loss 2.5126 | dt 0.11s\n",
            "epoch 008 | iter 04999 | train loss 2.6058 | dt 0.11s\n",
            "epoch 008 | iter 05199 | train loss 2.5507 | dt 0.11s\n",
            "epoch 008 | iter 05399 | train loss 2.5700 | dt 0.11s\n",
            "epoch 008 | iter 05599 | train loss 2.5849 | dt 0.12s\n",
            "epoch 008 | iter 05799 | train loss 2.5868 | dt 0.10s\n",
            "epoch 008 | iter 05999 | train loss 2.5800 | dt 0.10s\n",
            "epoch 008 | iter 06199 | train loss 2.5855 | dt 0.11s\n",
            "epoch 008 | iter 06399 | train loss 2.5806 | dt 0.10s\n",
            "epoch 008 | iter 06599 | train loss 2.5655 | dt 0.10s\n",
            "epoch 008 | iter 06799 | train loss 2.5828 | dt 0.12s\n",
            "epoch 008 | iter 06999 | train loss 2.6037 | dt 0.11s\n",
            "epoch 008 | iter 07199 | train loss 2.5549 | dt 0.12s\n",
            "epoch 008 | iter 07399 | train loss 2.5484 | dt 0.12s\n",
            "epoch 008 | iter 07599 | train loss 2.6128 | dt 0.11s\n",
            "epoch 008 | iter 07799 | train loss 2.5657 | dt 0.12s\n",
            "epoch 008 | iter 07999 | train loss 2.5813 | dt 0.12s\n",
            "epoch 008 | iter 08199 | train loss 2.5937 | dt 0.12s\n",
            "epoch 008 | iter 08399 | train loss 2.5617 | dt 0.11s\n",
            "epoch 008 | iter 08599 | train loss 2.6101 | dt 0.12s\n",
            "epoch 008 | iter 08799 | train loss 2.6009 | dt 0.11s\n",
            "epoch 008 | iter 08999 | train loss 2.5982 | dt 0.10s\n",
            "epoch 008 | iter 09199 | train loss 2.5594 | dt 0.11s\n",
            "epoch 008 | iter 09399 | train loss 2.5716 | dt 0.12s\n",
            "epoch 008 | iter 09599 | train loss 2.5516 | dt 0.11s\n",
            "epoch 008 | iter 09799 | train loss 2.5854 | dt 0.12s\n",
            "epoch 008 | iter 09999 | train loss 2.5725 | dt 0.11s\n",
            "epoch 008 | iter 10199 | train loss 2.6250 | dt 0.11s\n",
            "epoch 008 | iter 10399 | train loss 2.5691 | dt 0.10s\n",
            "epoch 008 | iter 10599 | train loss 2.5116 | dt 0.12s\n",
            "epoch 008 | iter 10799 | train loss 2.4767 | dt 0.12s\n",
            "epoch 008 | iter 10999 | train loss 2.4632 | dt 0.12s\n",
            "epoch 008 | iter 11199 | train loss 2.4912 | dt 0.12s\n",
            "epoch 008 | iter 11399 | train loss 2.4511 | dt 0.11s\n",
            "epoch 008 | iter 11599 | train loss 2.4345 | dt 0.11s\n",
            "[EPOCH 008] valid loss=1.9470 | valid ppl=7.01 | valid BLEU=0.3602 | epoch time=2059.1s\n",
            "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.3602\n",
            "epoch 009 | iter 00199 | train loss 2.5279 | dt 0.11s\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_iter):\n\u001b[32m     45\u001b[39m     t1 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     loss = \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     running += \u001b[38;5;28mfloat\u001b[39m(loss)\n\u001b[32m     48\u001b[39m     n_print += \u001b[32m1\u001b[39m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mstep\u001b[39m\u001b[34m(model, optimizer, batch, criterion)\u001b[39m\n\u001b[32m     16\u001b[39m loss = criterion(preds.view(-\u001b[32m1\u001b[39m, preds.size(-\u001b[32m1\u001b[39m)), ys)\n\u001b[32m     17\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep_and_update_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m loss = loss.item()\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mScheduledOptim.step_and_update_lr\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mStep with the inner optimizer\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m._update_learning_rate()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\optim\\optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\optim\\optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\optim\\optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\optim\\adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\optim\\adam.py:759\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    756\u001b[39m     torch._foreach_addcdiv_(device_params, device_exp_avgs, exp_avg_sq_sqrt)\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    758\u001b[39m     bias_correction1 = [\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[32m1\u001b[39m - beta1 ** \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    760\u001b[39m     ]\n\u001b[32m    761\u001b[39m     bias_correction2 = [\n\u001b[32m    762\u001b[39m         \u001b[32m1\u001b[39m - beta2 ** _get_value(step) \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m device_state_steps\n\u001b[32m    763\u001b[39m     ]\n\u001b[32m    765\u001b[39m     step_size = _stack_if_compiling([(lr / bc) * -\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m bc \u001b[38;5;129;01min\u001b[39;00m bias_correction1])\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\NLP\\Lib\\site-packages\\torch\\optim\\optimizer.py:97\u001b[39m, in \u001b[36m_get_value\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m x\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import time, math\n",
        "import torch\n",
        "\n",
        "train_loss_hist = []\n",
        "valid_loss_hist = []\n",
        "valid_bleu_hist = []\n",
        "valid_ppl_hist  = []\n",
        "\n",
        "start_epoch = 0\n",
        "\n",
        "# best_metric init\n",
        "if BEST_BY == \"bleu\":\n",
        "    best_metric = -1e9\n",
        "else:\n",
        "    best_metric = 1e9\n",
        "\n",
        "# ---- Resume nếu có ----\n",
        "if resume_path is not None and os.path.exists(resume_path):\n",
        "    ckpt = load_checkpoint(\n",
        "        resume_path,\n",
        "        model=model,\n",
        "        optimizer=optimizer,  # ScheduledOptim wrapper\n",
        "        map_location=opt[\"device\"] if \"cuda\" in str(opt[\"device\"]) else \"cpu\",\n",
        "        strict=True\n",
        "    )\n",
        "    start_epoch = int(ckpt.get(\"epoch\", -1)) + 1\n",
        "    best_metric = ckpt.get(\"best_metric\", best_metric)\n",
        "\n",
        "    hist = ckpt.get(\"history\", None)\n",
        "    if hist:\n",
        "        train_loss_hist = hist.get(\"train_loss_hist\", train_loss_hist)\n",
        "        valid_loss_hist = hist.get(\"valid_loss_hist\", valid_loss_hist)\n",
        "        valid_bleu_hist = hist.get(\"valid_bleu_hist\", valid_bleu_hist)\n",
        "        valid_ppl_hist  = hist.get(\"valid_ppl_hist\",  valid_ppl_hist)\n",
        "\n",
        "    print(f\"[RESUME] from {resume_path} | start_epoch={start_epoch} | best_metric={best_metric}\")\n",
        "\n",
        "for epoch in range(start_epoch, opt['epochs']):\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    n_print = 0\n",
        "    t0 = time.time()\n",
        "\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        t1 = time.time()\n",
        "        loss = step(model, optimizer, batch, criterion)\n",
        "        running += float(loss)\n",
        "        n_print += 1\n",
        "\n",
        "        if (i + 1) % opt['printevery'] == 0:\n",
        "            avg_loss = running / n_print\n",
        "            print(f\"epoch {epoch:03d} | iter {i:05d} | train loss {avg_loss:.4f} | dt {time.time()-t1:.2f}s\")\n",
        "            running = 0.0\n",
        "            n_print = 0\n",
        "\n",
        "    # ----- validation -----\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        vloss = validiate(model, valid_iter, criterion)\n",
        "\n",
        "    vppl = math.exp(vloss) if vloss < 20 else float('inf')  # guard overflow\n",
        "    vbleu = bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)\n",
        "\n",
        "    # lưu history\n",
        "    train_loss_hist.append(None)  # nếu bạn muốn epoch-train-loss thì tính riêng\n",
        "    valid_loss_hist.append(float(vloss))\n",
        "    valid_ppl_hist.append(float(vppl))\n",
        "    valid_bleu_hist.append(float(vbleu))\n",
        "\n",
        "    print(f\"[EPOCH {epoch:03d}] valid loss={vloss:.4f} | valid ppl={vppl:.2f} | valid BLEU={vbleu:.4f} | epoch time={time.time()-t0:.1f}s\")\n",
        "\n",
        "    # ----- Save \"last\" checkpoint -----\n",
        "    history = {\n",
        "        \"train_loss_hist\": train_loss_hist,\n",
        "        \"valid_loss_hist\": valid_loss_hist,\n",
        "        \"valid_bleu_hist\": valid_bleu_hist,\n",
        "        \"valid_ppl_hist\":  valid_ppl_hist,\n",
        "    }\n",
        "\n",
        "    # last (overwrite)\n",
        "    save_checkpoint(\n",
        "        LAST_CKPT,\n",
        "        epoch=epoch,\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        best_metric=best_metric,\n",
        "        history=history,\n",
        "        opt=opt\n",
        "    )\n",
        "\n",
        "    # optional: keep per-epoch file\n",
        "    save_checkpoint(\n",
        "        os.path.join(CKPT_DIR, f\"last_epoch_{epoch:03d}.pt\"),\n",
        "        epoch=epoch,\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        best_metric=best_metric,\n",
        "        history=history,\n",
        "        opt=opt\n",
        "    )\n",
        "\n",
        "    # ----- Save \"best\" checkpoint -----\n",
        "    improved = False\n",
        "    if BEST_BY == \"bleu\":\n",
        "        if vbleu > best_metric:\n",
        "            improved = True\n",
        "            best_metric = float(vbleu)\n",
        "    else:  # BEST_BY == \"loss\"\n",
        "        if vloss < best_metric:\n",
        "            improved = True\n",
        "            best_metric = float(vloss)\n",
        "\n",
        "    if improved:\n",
        "        save_checkpoint(\n",
        "            BEST_CKPT,\n",
        "            epoch=epoch,\n",
        "            model=model,\n",
        "            optimizer=optimizer,\n",
        "            best_metric=best_metric,\n",
        "            history=history,\n",
        "            opt=opt,\n",
        "            extra={\"best_by\": BEST_BY}\n",
        "        )\n",
        "        print(f\"[BEST] saved -> {BEST_CKPT} | best_{BEST_BY}={best_metric:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[EPOCH 000] valid loss=2.7394 | valid BLEU=0.1913\n",
            "[EPOCH 001] valid loss=2.3583 | valid BLEU=0.2704\n",
            "[EPOCH 002] valid loss=2.2071 | valid BLEU=0.3007\n",
            "[EPOCH 003] valid loss=2.1214 | valid BLEU=0.3169\n",
            "[EPOCH 004] valid loss=2.0659 | valid BLEU=0.3325\n",
            "[EPOCH 005] valid loss=2.0304 | valid BLEU=0.3405\n",
            "[EPOCH 006] valid loss=1.9944 | valid BLEU=0.3505\n",
            "[EPOCH 007] valid loss=1.9677 | valid BLEU=0.3588\n",
            "[EPOCH 008] valid loss=1.9470 | valid BLEU=0.3602\n",
            "[EPOCH 008] valid loss=1.9470 | valid BLEU=0.3602\n",
            "[EPOCH 008] valid loss=1.9470 | valid BLEU=0.3602\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import torch\n",
        "\n",
        "CKPT_DIR = Path(\"./checkpoints\")          # dùng /\n",
        "ckpt_paths = sorted(CKPT_DIR.glob(\"*.pt\"))\n",
        "\n",
        "# nếu file bạn dạng last_epoch_001.pt\n",
        "_epoch_re = re.compile(r\"last_epoch_(\\d+)\\.pt$\", re.IGNORECASE)\n",
        "\n",
        "def get_epoch_from_name(p: Path):\n",
        "    m = _epoch_re.search(p.name)\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def extract_loss_bleu_from_ckpt(ckpt):\n",
        "    \"\"\"\n",
        "    Trả về (valid_loss, valid_bleu) nếu đọc được từ ckpt, không thì (None, None)\n",
        "    Hỗ trợ các format save phổ biến:\n",
        "      - ckpt[\"history\"][\"valid_loss_hist\"], ckpt[\"history\"][\"valid_bleu_hist\"] (lấy phần tử cuối)\n",
        "      - ckpt[\"valid_loss\"], ckpt[\"valid_bleu\"]\n",
        "      - ckpt[\"metrics\"] dict\n",
        "    \"\"\"\n",
        "    vloss = vbleu = None\n",
        "\n",
        "    if isinstance(ckpt, dict):\n",
        "        # 1) history\n",
        "        hist = ckpt.get(\"history\", None)\n",
        "        if isinstance(hist, dict):\n",
        "            vloss_hist = hist.get(\"valid_loss_hist\", None)\n",
        "            vbleu_hist = hist.get(\"valid_bleu_hist\", None)\n",
        "            if isinstance(vloss_hist, list) and len(vloss_hist):\n",
        "                vloss = float(vloss_hist[-1])\n",
        "            if isinstance(vbleu_hist, list) and len(vbleu_hist):\n",
        "                vbleu = float(vbleu_hist[-1])\n",
        "\n",
        "        # 2) direct keys\n",
        "        if vloss is None and \"valid_loss\" in ckpt:\n",
        "            try: vloss = float(ckpt[\"valid_loss\"])\n",
        "            except: pass\n",
        "        if vbleu is None and \"valid_bleu\" in ckpt:\n",
        "            try: vbleu = float(ckpt[\"valid_bleu\"])\n",
        "            except: pass\n",
        "\n",
        "        # 3) metrics dict\n",
        "        metrics = ckpt.get(\"metrics\", None)\n",
        "        if isinstance(metrics, dict):\n",
        "            if vloss is None and \"valid_loss\" in metrics:\n",
        "                try: vloss = float(metrics[\"valid_loss\"])\n",
        "                except: pass\n",
        "            if vbleu is None and \"valid_bleu\" in metrics:\n",
        "                try: vbleu = float(metrics[\"valid_bleu\"])\n",
        "                except: pass\n",
        "\n",
        "    return vloss, vbleu\n",
        "\n",
        "\n",
        "# (tuỳ chọn) sort theo epoch trong tên file cho đẹp; nếu không match thì giữ thứ tự glob\n",
        "def sort_key(p: Path):\n",
        "    e = get_epoch_from_name(p)\n",
        "    return (e is None, e if e is not None else p.name)\n",
        "\n",
        "ckpt_paths = sorted(ckpt_paths, key=sort_key)\n",
        "\n",
        "for p in ckpt_paths:\n",
        "    ckpt = torch.load(p.as_posix(), map_location=\"cpu\")\n",
        "\n",
        "    # epoch: ưu tiên epoch trong tên file, fallback sang ckpt[\"epoch\"] nếu có\n",
        "    epoch = get_epoch_from_name(p)\n",
        "    if epoch is None and isinstance(ckpt, dict) and \"epoch\" in ckpt:\n",
        "        try:\n",
        "            epoch = int(ckpt[\"epoch\"])\n",
        "        except:\n",
        "            epoch = None\n",
        "\n",
        "    vloss, vbleu = extract_loss_bleu_from_ckpt(ckpt)\n",
        "\n",
        "    epoch_str = f\"{epoch:03d}\" if isinstance(epoch, int) else \"???\"\n",
        "    loss_str  = f\"{vloss:.4f}\" if isinstance(vloss, (int, float)) else \"NA\"\n",
        "    bleu_str  = f\"{vbleu:.4f}\" if isinstance(vbleu, (int, float)) else \"NA\"\n",
        "\n",
        "    print(f\"[EPOCH {epoch_str}] valid loss={loss_str} | valid BLEU={bleu_str}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded: checkpoints/best.pt | epoch: 8 | best_metric: 0.36017054319381714\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "BEST_CKPT = \"checkpoints/best.pt\"\n",
        "DEVICE = opt[\"device\"]\n",
        "\n",
        "def strip_prefix(sd, prefix):\n",
        "    if not prefix:\n",
        "        return sd\n",
        "    out = {}\n",
        "    for k, v in sd.items():\n",
        "        out[k[len(prefix):] if k.startswith(prefix) else k] = v\n",
        "    return out\n",
        "\n",
        "def load_best(model, ckpt_path=BEST_CKPT, device=DEVICE, strict=True):\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    sd = ckpt[\"model_state\"]  # ✅ đúng format của bạn\n",
        "\n",
        "    # nếu từng train bằng DataParallel thì có \"module.\"\n",
        "    sd = strip_prefix(sd, \"module.\")\n",
        "    # đôi khi save prefix \"model.\"\n",
        "    sd = strip_prefix(sd, \"model.\")\n",
        "\n",
        "    model.load_state_dict(sd, strict=strict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return ckpt\n",
        "\n",
        "ckpt = load_best(model)\n",
        "print(\"Loaded:\", BEST_CKPT, \"| epoch:\", ckpt.get(\"epoch\"), \"| best_metric:\", ckpt.get(\"best_metric\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Translating tst: 100%|██████████| 3000/3000 [12:43<00:00,  3.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: test.pred.txt | n_lines: 3000\n",
            "HYP ex: current situation of knowledge and practice of health insurance cards in the use of medical examination and treatment services in public health facilities and some influencing factors in vien lien, lao cai province in 2017\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def detok_spm_text(s: str) -> str:\n",
        "    s = s.replace(\"▁\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)\n",
        "    return s\n",
        "\n",
        "def normalize_pred(pred):\n",
        "    if isinstance(pred, list):\n",
        "        s = \"\".join(pred) if any(\"▁\" in t for t in pred) else \" \".join(pred)\n",
        "    else:\n",
        "        s = str(pred)\n",
        "\n",
        "    for t in [\"<sos>\", \"<eos>\", \"<pad>\"]:\n",
        "        s = s.replace(t, \" \")\n",
        "    s = \" \".join(s.split()).strip()\n",
        "    return detok_spm_text(s)\n",
        "\n",
        "LIMIT_TRANSLATE = None  # 50 để test nhanh, None=full\n",
        "src_used = valid_src_data if LIMIT_TRANSLATE is None else valid_src_data[:LIMIT_TRANSLATE]\n",
        "\n",
        "test_hyp_vi = []\n",
        "for src in tqdm(src_used, desc=\"Translating tst\"):\n",
        "    pred = translate_sentence(src, model, SRC, TRG, opt[\"device\"], opt[\"k\"], opt[\"max_strlen\"], sp_trg)\n",
        "    test_hyp_vi.append(normalize_pred(pred))\n",
        "\n",
        "OUT_TXT = \"test.pred.txt\"\n",
        "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for line in test_hyp_vi:\n",
        "        f.write(line + \"\\n\")\n",
        "\n",
        "print(\"Saved:\", OUT_TXT, \"| n_lines:\", len(test_hyp_vi))\n",
        "print(\"HYP ex:\", test_hyp_vi[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SacreBLEU score: 24.18\n",
            "N-gram precisions: [55.69242703902844, 30.20650447287016, 18.519326411307915, 12.097252421427159]\n"
          ]
        }
      ],
      "source": [
        "import sacrebleu\n",
        "\n",
        "# Đọc file reference\n",
        "with open(\"data/public_test.en.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    refs = [f.read().splitlines()] # Phải để trong list của list\n",
        "\n",
        "# Đọc file prediction\n",
        "with open(\"test.pred.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    preds = f.read().splitlines()\n",
        "\n",
        "# Tính toán\n",
        "# Chú ý: mặc định sacrebleu dùng tokenizer '13a' (chuẩn quốc tế)\n",
        "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "\n",
        "print(f\"SacreBLEU score: {bleu.score:.2f}\")\n",
        "print(f\"N-gram precisions: {bleu.precisions}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5E8G0-8QFbj"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'BLEUScore' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_src_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_trg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSRC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdevice\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mk\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax_strlen\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp_trg\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mTypeError\u001b[39m: 'BLEUScore' object is not callable"
          ]
        }
      ],
      "source": [
        "bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0CwtdJeUNQJo",
        "outputId": "190c4a93-436a-4b00-832b-ae8f4c183fe4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cần có các biện pháp can thiệp sớm để phòng ngừa tiến triển và hạn chế các biến chứng của suy tĩnh mạch chi dưới cho sinh viên điều dưỡng có dấu hiệu sớm suy tĩnh mạch chi dưới và đảm bảo sức khoẻ, an toàn và nâng cao chất lượng cuộc sống cho sinh viên điều dưỡng và nhân viên y tế trong tương lai.'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = 'cần có các biện pháp can thiệp sớm để phòng ngừa tiến triển và hạn chế các biến chứng của suy tĩnh mạch chi dưới cho sinh viên điều dưỡng có dấu hiệu sớm suy tĩnh mạch chi dưới và đảm bảo sức khoẻ, an toàn và nâng cao chất lượng cuộc sống cho sinh viên điều dưỡng và nhân viên y tế trong tương lai.'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)\n",
        "trans_sent\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
