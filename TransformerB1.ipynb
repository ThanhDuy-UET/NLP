{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pbcquoc/transformer/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVOKeezWPsSs",
    "outputId": "24fe59fa-1dd1-4afd-bb3b-c2015314b9a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip -q install torchtext==0.6.0\n",
    "! pip -q install pyvi\n",
    "! pip -q install sentencepiece\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8gvN64qvNQIS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "X9da_ZuSNQIW"
   },
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "# Embedder(100, 512)(torch.LongTensor([1,2,3,4])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rP64KizDNQIa"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        # Bảng pe mình vẽ ở trên\n",
    "        for pos in range(max_seq_length):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
    "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x*math.sqrt(self.d_model)\n",
    "        seq_length = x.size(1)\n",
    "\n",
    "        pe = Variable(self.pe[:, :seq_length], requires_grad=False)\n",
    "\n",
    "        if x.is_cuda:\n",
    "            pe.cuda()\n",
    "        # cộng embedding vector với pe\n",
    "        x = x + pe\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# PositionalEncoder(512)(torch.rand(5, 30, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2nJMcGuUNQId"
   },
   "outputs": [],
   "source": [
    "def attention(q, k, v, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    q: batch_size x head x seq_length x d_model\n",
    "    k: batch_size x head x seq_length x d_model\n",
    "    v: batch_size x head x seq_length x d_model\n",
    "    mask: batch_size x 1 x 1 x seq_length\n",
    "    output: batch_size x head x seq_length x d_model\n",
    "    \"\"\"\n",
    "\n",
    "    # attention score được tính bằng cách nhân q với k\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "    # xong rồi thì chuẩn hóa bằng softmax\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output, scores\n",
    "\n",
    "# attention(torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ANQ4C3EENQIh"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model//heads\n",
    "        self.h = heads\n",
    "        self.attn = None\n",
    "\n",
    "        # tạo ra 3 ma trận trọng số là q_linear, k_linear, v_linear như hình trên\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        q: batch_size x seq_length x d_model\n",
    "        k: batch_size x seq_length x d_model\n",
    "        v: batch_size x seq_length x d_model\n",
    "        mask: batch_size x 1 x seq_length\n",
    "        output: batch_size x seq_length x d_model\n",
    "        \"\"\"\n",
    "        bs = q.size(0)\n",
    "        # nhân ma trận trọng số q_linear, k_linear, v_linear với dữ liệu đầu vào q, k, v\n",
    "        # ở bước encode các bạn lưu ý rằng q, k, v chỉ là một (xem hình trên)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # tính attention score\n",
    "        scores, self.attn = attention(q, k, v, mask, self.dropout)\n",
    "\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "\n",
    "        output = self.out(concat)\n",
    "        return output\n",
    "\n",
    "# MultiHeadAttention(8, 512)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 30, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "n6-_9Hq-NQIk"
   },
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "\n",
    "        self.size = d_model\n",
    "\n",
    "        # create two learnable parameters to calibrate normalisation\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "H1ndbdMXNQIn"
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Trong kiến trúc của chúng ta có tầng linear\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-Wwo91xDNQIq"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        x: batch_size x seq_length x d_model\n",
    "        mask: batch_size x 1 x seq_length\n",
    "        output: batch_size x seq_length x d_model\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        x2 = self.norm_1(x)\n",
    "        # tính attention value, các bạn để ý q, k, v là giống nhau\n",
    "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "# EncoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32 , 1, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6mDt2NPeNQIu"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.norm_3 = Norm(d_model)\n",
    "\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "\n",
    "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        x: batch_size x seq_length x d_model\n",
    "        e_outputs: batch_size x seq_length x d_model\n",
    "        src_mask: batch_size x 1 x seq_length\n",
    "        trg_mask: batch_size x 1 x seq_length\n",
    "        \"\"\"\n",
    "        # Các bạn xem hình trên, kiến trúc mình vẽ với code ở chỗ này tương đương nhau.\n",
    "        x2 = self.norm_1(x)\n",
    "        # multihead attention thứ nhất, chú ý các từ ở target\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        # masked mulithead attention thứ 2. k, v là giá trị output của mô hình encoder\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "# DecoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZcU8nyvzNQIx"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def get_clones(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Một encoder có nhiều encoder layer nhé !!!\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        \"\"\"\n",
    "        src: batch_size x seq_length\n",
    "        mask: batch_size x 1 x seq_length\n",
    "        output: batch_size x seq_length x d_model\n",
    "        \"\"\"\n",
    "        x = self.embed(src)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# Encoder(232, 512,6,8,0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "5lBRYMg_NQI0"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Một decoder có nhiều decoder layer nhé !!!\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        trg: batch_size x seq_length\n",
    "        e_outputs: batch_size x seq_length x d_model\n",
    "        src_mask: batch_size x 1 x seq_length\n",
    "        trg_mask: batch_size x 1 x seq_length\n",
    "        output: batch_size x seq_length x d_model\n",
    "        \"\"\"\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# Decoder(232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DpxSCRILNQI3"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\" Cuối cùng ghép chúng lại với nhau để được mô hình transformer hoàn chỉnh\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        src: batch_size x seq_length\n",
    "        trg: batch_size x seq_length\n",
    "        src_mask: batch_size x 1 x seq_length\n",
    "        trg_mask batch_size x 1 x seq_length\n",
    "        output: batch_size x seq_length x vocab_size\n",
    "        \"\"\"\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "\n",
    "# Transformer(232, 232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.LongTensor(32, 30).random_(0, 10),torch.rand(32, 1, 30),torch.rand(32, 1, 30)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "M5tvzW9jNQI6"
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "\n",
    "class MyIterator(data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(d, random_shuffler):\n",
    "                for p in data.batch(d, self.batch_size * 100):\n",
    "                    p_batch = data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))\n",
    "\n",
    "global max_src_in_batch, max_tgt_in_batch\n",
    "\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NkBjLH96NQI8"
   },
   "outputs": [],
   "source": [
    "\n",
    "def nopeak_mask(size, device):\n",
    "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
    "     mô hình không nhìn thấy được các từ ở tương lai\n",
    "    \"\"\"\n",
    "    np_mask = np.triu(np.ones((1, size, size)),\n",
    "    k=1).astype('uint8')\n",
    "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
    "    np_mask = np_mask.to(device)\n",
    "\n",
    "    return np_mask\n",
    "\n",
    "def create_masks(src, trg, src_pad, trg_pad, device):\n",
    "    \"\"\" Tạo mask cho encoder,\n",
    "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào\n",
    "    \"\"\"\n",
    "    src_mask = (src != src_pad).unsqueeze(-2)\n",
    "\n",
    "    if trg is not None:\n",
    "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
    "        size = trg.size(1) # get seq_len for matrix\n",
    "        np_mask = nopeak_mask(size, device)\n",
    "        if trg.is_cuda:\n",
    "            np_mask.cuda()\n",
    "        trg_mask = trg_mask & np_mask\n",
    "\n",
    "    else:\n",
    "        trg_mask = None\n",
    "    return src_mask, trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9YoUVx4xjEb7"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "def get_synonym(word, SRC):\n",
    "    syns = wordnet.synsets(word)\n",
    "    for s in syns:\n",
    "        for l in s.lemmas():\n",
    "            if SRC.vocab.stoi[l.name()] != 0:\n",
    "                return SRC.vocab.stoi[l.name()]\n",
    "\n",
    "    return 0\n",
    "\n",
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1IJpUEIMgMbw"
   },
   "outputs": [],
   "source": [
    "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
    "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
    "    \"\"\"\n",
    "    init_tok = TRG.vocab.stoi['<sos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "\n",
    "    # tính sẵn output của encoder\n",
    "    e_output = model.encoder(src, src_mask)\n",
    "\n",
    "    outputs = torch.LongTensor([[init_tok]])\n",
    "\n",
    "    outputs = outputs.to(device)\n",
    "\n",
    "    trg_mask = nopeak_mask(1, device)\n",
    "    # dự đoán kí tự đầu tiên\n",
    "    out = model.out(model.decoder(outputs,\n",
    "    e_output, src_mask, trg_mask))\n",
    "    out = F.softmax(out, dim=-1)\n",
    "\n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
    "\n",
    "    outputs = torch.zeros(k, max_len).long()\n",
    "    outputs = outputs.to(device)\n",
    "    outputs[:, 0] = init_tok\n",
    "    outputs[:, 1] = ix[0]\n",
    "\n",
    "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
    "\n",
    "    e_outputs = e_outputs.to(device)\n",
    "    e_outputs[:, :] = e_output[0]\n",
    "\n",
    "    return outputs, e_outputs, log_scores\n",
    "\n",
    "def k_best_outputs(outputs, out, log_scores, i, k):\n",
    "\n",
    "    probs, ix = out[:, -1].data.topk(k)\n",
    "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
    "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
    "\n",
    "    row = k_ix // k\n",
    "    col = k_ix % k\n",
    "\n",
    "    outputs[:, :i] = outputs[row, :i]\n",
    "    outputs[:, i] = ix[row, col]\n",
    "\n",
    "    log_scores = k_probs.unsqueeze(0)\n",
    "\n",
    "    return outputs, log_scores\n",
    "\n",
    "def beam_search(src, model, SRC, TRG, device, k, max_len):\n",
    "\n",
    "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
    "    eos_tok = TRG.vocab.stoi['<eos>']\n",
    "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
    "    ind = None\n",
    "    for i in range(2, max_len):\n",
    "\n",
    "        trg_mask = nopeak_mask(i, device)\n",
    "\n",
    "        out = model.out(model.decoder(outputs[:,:i],\n",
    "        e_outputs, src_mask, trg_mask))\n",
    "\n",
    "        out = F.softmax(out, dim=-1)\n",
    "\n",
    "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
    "\n",
    "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
    "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
    "        for vec in ones:\n",
    "            i = vec[0]\n",
    "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
    "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
    "\n",
    "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
    "\n",
    "        if num_finished_sentences == k:\n",
    "            alpha = 0.7\n",
    "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
    "            _, ind = torch.max(log_scores * div, 1)\n",
    "            ind = ind.data[0]\n",
    "            break\n",
    "\n",
    "    if ind is None:\n",
    "\n",
    "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
    "\n",
    "    else:\n",
    "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
    "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "s-AFuSOIhi7X"
   },
   "outputs": [],
   "source": [
    "# def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n",
    "#     \"\"\"Dịch một câu sử dụng beamsearch\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     indexed = []\n",
    "#     sentence = SRC.preprocess(sentence)\n",
    "\n",
    "#     for tok in sentence:\n",
    "#         if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n",
    "#             indexed.append(SRC.vocab.stoi[tok])\n",
    "#         else:\n",
    "#             indexed.append(get_synonym(tok, SRC))\n",
    "\n",
    "#     sentence = Variable(torch.LongTensor([indexed]))\n",
    "\n",
    "#     sentence = sentence.to(device)\n",
    "\n",
    "#     sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n",
    "\n",
    "#     return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Uee4YaQNQI_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_uVO0yr_NQJC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from torchtext import data\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def clean_text(sentence: str) -> str:\n",
    "    sentence = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
    "    sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
    "    sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
    "    sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
    "    sentence = sentence.lower()\n",
    "    return sentence.strip()\n",
    "\n",
    "def read_data(src_file, trg_file):\n",
    "    src_data = open(src_file, encoding=\"utf-8\").read().strip().split('\\n')\n",
    "    trg_data = open(trg_file, encoding=\"utf-8\").read().strip().split('\\n')\n",
    "    return src_data, trg_data\n",
    "\n",
    "\n",
    "def train_spm_bpe_from_lines(lines, model_prefix, vocab_size=16000,\n",
    "                            character_coverage=1.0, user_defined_symbols=None):\n",
    "    tmp = f\"{model_prefix}_corpus.txt\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        for s in lines:\n",
    "            f.write(clean_text(s) + \"\\n\")   # ✅ train on cleaned text\n",
    "\n",
    "    uds = user_defined_symbols or []\n",
    "    uds_arg = \",\".join(uds) if uds else \"\"\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=tmp,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        model_type=\"bpe\",\n",
    "        character_coverage=character_coverage,\n",
    "        unk_id=0,\n",
    "        pad_id=-1,\n",
    "        bos_id=-1,\n",
    "        eos_id=-1,\n",
    "        user_defined_symbols=uds_arg if uds_arg else None\n",
    "    )\n",
    "    os.remove(tmp)\n",
    "\n",
    "class tokenize(object):\n",
    "    def __init__(self, model_file):\n",
    "        self.sp = spm.SentencePieceProcessor(model_file=model_file)\n",
    "\n",
    "    def tokenizer(self, sentence):\n",
    "        sentence = clean_text(sentence)\n",
    "        return self.sp.encode(sentence, out_type=str)\n",
    "\n",
    "    def detokenize(self, pieces):\n",
    "        return self.sp.decode(pieces)\n",
    "\n",
    "def create_fields(src_lang, trg_lang,\n",
    "                  src_model_file=\"spm_src_bpe.model\",\n",
    "                  trg_model_file=\"spm_trg_bpe.model\"):\n",
    "    print(\"loading sentencepiece BPE tokenizers (with cleaning)...\")\n",
    "\n",
    "    t_src = tokenize(src_model_file)\n",
    "    t_trg = tokenize(trg_model_file)\n",
    "\n",
    "    SRC = data.Field(\n",
    "        lower=False,\n",
    "        tokenize=t_src.tokenizer,\n",
    "        pad_token=\"<pad>\",\n",
    "        unk_token=\"<unk>\"\n",
    "    )\n",
    "\n",
    "    TRG = data.Field(\n",
    "        lower=False,\n",
    "        tokenize=t_trg.tokenizer,\n",
    "        init_token=\"<sos>\",\n",
    "        eos_token=\"<eos>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        unk_token=\"<unk>\"\n",
    "    )\n",
    "\n",
    "    return SRC, TRG, t_src, t_trg\n",
    "\n",
    "def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n",
    "    print(\"creating dataset and iterator... \")\n",
    "\n",
    "    raw_data = {'src': [line for line in src_data], 'trg': [line for line in trg_data]}\n",
    "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
    "\n",
    "    # ✅ lọc đúng theo số token sau preprocess (clean + sentencepiece)\n",
    "    mask = (\n",
    "        df['src'].apply(lambda s: len(SRC.preprocess(s)) < max_strlen) &\n",
    "        df['trg'].apply(lambda s: len(TRG.preprocess(s)) < max_strlen)\n",
    "    )\n",
    "    df = df.loc[mask]\n",
    "\n",
    "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
    "\n",
    "    data_fields = [('src', SRC), ('trg', TRG)]\n",
    "    ds = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
    "\n",
    "    it = MyIterator(\n",
    "        ds, batch_size=batchsize, device=device,\n",
    "        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
    "        batch_size_fn=batch_size_fn, train=istrain, shuffle=True\n",
    "    )\n",
    "\n",
    "    os.remove('translate_transformer_temp.csv')\n",
    "\n",
    "    if istrain:\n",
    "        SRC.build_vocab(ds)\n",
    "        TRG.build_vocab(ds)\n",
    "\n",
    "    return it\n",
    "\n",
    "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len, sp_trg):\n",
    "    model.eval()\n",
    "\n",
    "    src_tokens = SRC.preprocess(sentence)  # clean + sp encode\n",
    "    unk = SRC.vocab.stoi.get(\"<unk>\", 0)\n",
    "    src_ids = [SRC.vocab.stoi.get(tok, unk) for tok in src_tokens]\n",
    "    src_tensor = torch.LongTensor([src_ids]).to(device)\n",
    "\n",
    "    pred_pieces_str = beam_search(src_tensor, model, SRC, TRG, device, k, max_len)\n",
    "    pieces = pred_pieces_str.split()\n",
    "    pred_text = sp_trg.detokenize(pieces)\n",
    "    return pred_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "l4POJRxdNQJF"
   },
   "outputs": [],
   "source": [
    "def step(model, optimizer,batch, criterion):\n",
    "    \"\"\"\n",
    "    Một lần cập nhật mô hình\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    src = batch.src.transpose(0,1).cuda()\n",
    "    trg = batch.trg.transpose(0,1).cuda()\n",
    "    trg_input = trg[:, :-1]\n",
    "    src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
    "    preds = model(src, trg_input, src_mask, trg_mask)\n",
    "\n",
    "    ys = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
    "    loss.backward()\n",
    "    optimizer.step_and_update_lr()\n",
    "\n",
    "    loss = loss.item()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c5sPA-k_NQJI"
   },
   "outputs": [],
   "source": [
    "def validiate(model, valid_iter, criterion):\n",
    "    \"\"\" Tính loss trên tập validation\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = []\n",
    "        for batch in valid_iter:\n",
    "            src = batch.src.transpose(0,1).cuda()\n",
    "            trg = batch.trg.transpose(0,1).cuda()\n",
    "            trg_input = trg[:, :-1]\n",
    "            src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
    "            preds = model(src, trg_input, src_mask, trg_mask)\n",
    "\n",
    "            ys = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
    "\n",
    "            loss = loss.item()\n",
    "\n",
    "            total_loss.append(loss)\n",
    "\n",
    "    avg_loss = np.mean(total_loss)\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OW8pRq91rwJR"
   },
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "    def state_dict(self):\n",
    "        optimizer_state_dict = {\n",
    "            'init_lr':self.init_lr,\n",
    "            'd_model':self.d_model,\n",
    "            'n_warmup_steps':self.n_warmup_steps,\n",
    "            'n_steps':self.n_steps,\n",
    "            '_optimizer':self._optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        return optimizer_state_dict\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.init_lr = state_dict['init_lr']\n",
    "        self.d_model = state_dict['d_model']\n",
    "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
    "        self.n_steps = state_dict['n_steps']\n",
    "\n",
    "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "LHGeSHThtlj-"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "            true_dist[:, self.padding_idx] = 0\n",
    "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
    "            if mask.dim() > 0:\n",
    "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Sg257Gk_Kzzw"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def bleu(valid_src_data, valid_trg_data, model, SRC, TRG, device, k, max_strlen, sp_trg):\n",
    "    SPECIALS = {\"<pad>\", \"<sos>\", \"<eos>\"}\n",
    "\n",
    "    pred_sents = []\n",
    "    for sentence in valid_src_data:\n",
    "        pred_text = translate_sentence(sentence, model, SRC, TRG, device, k, max_strlen, sp_trg)\n",
    "        pred_tok = [t for t in TRG.preprocess(pred_text) if t not in SPECIALS]\n",
    "        pred_sents.append(pred_tok)\n",
    "\n",
    "    trg_sents = []\n",
    "    for sent in valid_trg_data:\n",
    "        ref_tok = [t for t in TRG.preprocess(sent) if t not in SPECIALS]\n",
    "        trg_sents.append([ref_tok])\n",
    "\n",
    "    return bleu_score(pred_sents, trg_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Nhgu-SPTNQJL"
   },
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'train_src_data':'./data/train.en',\n",
    "    'train_trg_data':'./data/train.vi',\n",
    "    'valid_src_data':'./data/tst2013.en',\n",
    "    'valid_trg_data':'./data/tst2013.vi',\n",
    "    'src_lang': 'spm_src_bpe.model',\n",
    "    'trg_lang': 'spm_trg_bpe.model',\n",
    "    'max_strlen':256,\n",
    "    'batchsize':1500,\n",
    "    'device':'cuda',\n",
    "    'd_model': 512,\n",
    "    'n_layers': 6,\n",
    "    'heads': 8,\n",
    "    'dropout': 0.1,\n",
    "    'lr':0.0001,\n",
    "    'epochs':20,\n",
    "    'printevery': 200,\n",
    "    'k':5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7HOay5MrNQJO",
    "outputId": "6fd8ff36-ff81-43bb-eb10-adec0912dddd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'gdown' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('./data/', exist_ok=True)\n",
    "! gdown --id 1Fuo_ALIFKlUvOPbK5rUA5OfAS2wKn_95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIWJTjdLNQJR",
    "outputId": "c614ef7d-3073-49b1-c925-1ca2c894942b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "! unzip -o en_vi.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBotIB8pNQJU",
    "outputId": "039ac075-b78f-421d-adfa-3154acd87ba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: spm_src_bpe.model spm_trg_bpe.model\n",
      "loading sentencepiece BPE tokenizers (with cleaning)...\n",
      "creating dataset and iterator... \n",
      "creating dataset and iterator... \n"
     ]
    }
   ],
   "source": [
    "train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n",
    "valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n",
    "\n",
    "SPECIALS = [\"<pad>\", \"<sos>\", \"<eos>\"]\n",
    "\n",
    "if not os.path.exists(\"spm_src_bpe.model\"):\n",
    "    train_spm_bpe_from_lines(train_src_data, \"spm_src_bpe\",\n",
    "                             vocab_size=16000, character_coverage=1.0,\n",
    "                             user_defined_symbols=SPECIALS)\n",
    "\n",
    "if not os.path.exists(\"spm_trg_bpe.model\"):\n",
    "    train_spm_bpe_from_lines(train_trg_data, \"spm_trg_bpe\",\n",
    "                             vocab_size=16000, character_coverage=1.0,\n",
    "                             user_defined_symbols=SPECIALS)\n",
    "\n",
    "print(\"Done:\", \"spm_src_bpe.model\", \"spm_trg_bpe.model\")\n",
    "\n",
    "SRC, TRG, sp_src, sp_trg = create_fields(opt['src_lang'], opt['trg_lang'])\n",
    "\n",
    "train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'],\n",
    "                            opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n",
    "valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'],\n",
    "                            opt['batchsize'], opt['device'], SRC, TRG, istrain=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Gnw9xrJeNQJX"
   },
   "outputs": [],
   "source": [
    "src_pad = SRC.vocab.stoi['<pad>']\n",
    "trg_pad = TRG.vocab.stoi['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "5RccNL8VNQJd"
   },
   "outputs": [],
   "source": [
    "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt['d_model'], opt['n_layers'], opt['heads'], opt['dropout'])\n",
    "\n",
    "for p in model.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "model = model.to(opt['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "12debLGiNQJg"
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = ScheduledOptim(\n",
    "        torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
    "        0.2, opt['d_model'], 4000)\n",
    "\n",
    "criterion = LabelSmoothingLoss(len(TRG.vocab), padding_idx=trg_pad, smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "import torch\n",
    "\n",
    "def save_checkpoint(path, epoch, model, optimizer, best_metric, history, opt, extra=None):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer._optimizer.state_dict() if hasattr(optimizer, \"_optimizer\") else optimizer.state_dict(),\n",
    "        \"best_metric\": best_metric,\n",
    "        \"history\": history,\n",
    "        \"opt\": opt,\n",
    "        \"extra\": extra or {}\n",
    "    }\n",
    "    # ScheduledOptim có n_steps riêng -> lưu để resume LR schedule đúng\n",
    "    if hasattr(optimizer, \"n_steps\"):\n",
    "        ckpt[\"sched_n_steps\"] = optimizer.n_steps\n",
    "\n",
    "    torch.save(ckpt, path)\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, map_location=\"cpu\", strict=True):\n",
    "    ckpt = torch.load(path, map_location=map_location)\n",
    "    model.load_state_dict(ckpt[\"model_state\"], strict=strict)\n",
    "\n",
    "    if optimizer is not None:\n",
    "        # restore optimizer state\n",
    "        if hasattr(optimizer, \"_optimizer\"):\n",
    "            optimizer._optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "        else:\n",
    "            optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "\n",
    "        # restore ScheduledOptim step count\n",
    "        if hasattr(optimizer, \"n_steps\") and \"sched_n_steps\" in ckpt:\n",
    "            optimizer.n_steps = ckpt[\"sched_n_steps\"]\n",
    "\n",
    "    return ckpt\n",
    "\n",
    "def find_latest_checkpoint(ckpt_dir, prefix=\"last_epoch_\", ext=\".pt\"):\n",
    "    if not os.path.exists(ckpt_dir):\n",
    "        return None\n",
    "    files = glob.glob(os.path.join(ckpt_dir, f\"{prefix}*{ext}\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    # sort by epoch number in filename\n",
    "    def get_epoch(f):\n",
    "        m = re.search(rf\"{re.escape(prefix)}(\\d+){re.escape(ext)}$\", os.path.basename(f))\n",
    "        return int(m.group(1)) if m else -1\n",
    "    files = sorted(files, key=get_epoch)\n",
    "    return files[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_DIR = \"./checkpoints\"\n",
    "LAST_CKPT = os.path.join(CKPT_DIR, \"last.pt\")\n",
    "BEST_CKPT = os.path.join(CKPT_DIR, \"best.pt\")\n",
    "\n",
    "# chọn tiêu chí lưu best:\n",
    "# - \"bleu\": maximize\n",
    "# - \"loss\": minimize\n",
    "BEST_BY = \"bleu\"   # hoặc \"loss\"\n",
    "\n",
    "resume_path = None  # ví dụ: \"./checkpoints/last.pt\" hoặc None\n",
    "# nếu muốn auto-resume từ checkpoint mới nhất:\n",
    "# resume_path = find_latest_checkpoint(CKPT_DIR, prefix=\"last_epoch_\", ext=\".pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "JeZqfQPANQJl",
    "outputId": "467a5beb-217b-47b3-8afc-4f0dc28426a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 000 | iter 00199 | train loss 8.9455 | dt 0.12s\n",
      "epoch 000 | iter 00399 | train loss 8.1809 | dt 0.10s\n",
      "epoch 000 | iter 00599 | train loss 6.9691 | dt 0.11s\n",
      "epoch 000 | iter 00799 | train loss 6.4649 | dt 0.11s\n",
      "epoch 000 | iter 00999 | train loss 6.3497 | dt 0.10s\n",
      "epoch 000 | iter 01199 | train loss 6.2267 | dt 0.11s\n",
      "epoch 000 | iter 01399 | train loss 6.0346 | dt 0.11s\n",
      "epoch 000 | iter 01599 | train loss 5.7830 | dt 0.10s\n",
      "epoch 000 | iter 01799 | train loss 5.6761 | dt 0.10s\n",
      "epoch 000 | iter 01999 | train loss 5.5408 | dt 0.11s\n",
      "epoch 000 | iter 02199 | train loss 5.3378 | dt 0.10s\n",
      "epoch 000 | iter 02399 | train loss 5.2844 | dt 0.10s\n",
      "[EPOCH 000] valid loss=3.9493 | valid ppl=51.90 | valid BLEU=0.0275 | epoch time=545.8s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.0275\n",
      "epoch 001 | iter 00199 | train loss 5.1412 | dt 0.11s\n",
      "epoch 001 | iter 00399 | train loss 5.0443 | dt 0.09s\n",
      "epoch 001 | iter 00599 | train loss 4.9234 | dt 0.10s\n",
      "epoch 001 | iter 00799 | train loss 4.9179 | dt 0.11s\n",
      "epoch 001 | iter 00999 | train loss 4.8094 | dt 0.09s\n",
      "epoch 001 | iter 01199 | train loss 4.7326 | dt 0.11s\n",
      "epoch 001 | iter 01399 | train loss 4.6598 | dt 0.10s\n",
      "epoch 001 | iter 01599 | train loss 4.5762 | dt 0.10s\n",
      "epoch 001 | iter 01799 | train loss 4.4854 | dt 0.11s\n",
      "epoch 001 | iter 01999 | train loss 4.4807 | dt 0.10s\n",
      "epoch 001 | iter 02199 | train loss 4.3902 | dt 0.10s\n",
      "epoch 001 | iter 02399 | train loss 4.2598 | dt 0.10s\n",
      "[EPOCH 001] valid loss=3.1849 | valid ppl=24.17 | valid BLEU=0.1258 | epoch time=566.9s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.1258\n",
      "epoch 002 | iter 00199 | train loss 4.1066 | dt 0.11s\n",
      "epoch 002 | iter 00399 | train loss 4.0989 | dt 0.10s\n",
      "epoch 002 | iter 00599 | train loss 4.1071 | dt 0.10s\n",
      "epoch 002 | iter 00799 | train loss 4.0530 | dt 0.10s\n",
      "epoch 002 | iter 00999 | train loss 3.9832 | dt 0.10s\n",
      "epoch 002 | iter 01199 | train loss 3.9158 | dt 0.10s\n",
      "epoch 002 | iter 01399 | train loss 3.9093 | dt 0.11s\n",
      "epoch 002 | iter 01599 | train loss 3.9260 | dt 0.11s\n",
      "epoch 002 | iter 01799 | train loss 3.8585 | dt 0.10s\n",
      "epoch 002 | iter 01999 | train loss 3.8521 | dt 0.11s\n",
      "epoch 002 | iter 02199 | train loss 3.8218 | dt 0.10s\n",
      "epoch 002 | iter 02399 | train loss 3.7821 | dt 0.10s\n",
      "[EPOCH 002] valid loss=2.7966 | valid ppl=16.39 | valid BLEU=0.2036 | epoch time=573.7s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2036\n",
      "epoch 003 | iter 00199 | train loss 3.6236 | dt 0.10s\n",
      "epoch 003 | iter 00399 | train loss 3.6321 | dt 0.11s\n",
      "epoch 003 | iter 00599 | train loss 3.6379 | dt 0.10s\n",
      "epoch 003 | iter 00799 | train loss 3.6170 | dt 0.11s\n",
      "epoch 003 | iter 00999 | train loss 3.6166 | dt 0.09s\n",
      "epoch 003 | iter 01199 | train loss 3.5869 | dt 0.10s\n",
      "epoch 003 | iter 01399 | train loss 3.5951 | dt 0.11s\n",
      "epoch 003 | iter 01599 | train loss 3.5521 | dt 0.11s\n",
      "epoch 003 | iter 01799 | train loss 3.5383 | dt 0.11s\n",
      "epoch 003 | iter 01999 | train loss 3.5368 | dt 0.10s\n",
      "epoch 003 | iter 02199 | train loss 3.4670 | dt 0.09s\n",
      "epoch 003 | iter 02399 | train loss 3.4310 | dt 0.11s\n",
      "[EPOCH 003] valid loss=2.6357 | valid ppl=13.95 | valid BLEU=0.2377 | epoch time=590.1s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2377\n",
      "epoch 004 | iter 00199 | train loss 3.3842 | dt 0.10s\n",
      "epoch 004 | iter 00399 | train loss 3.3952 | dt 0.11s\n",
      "epoch 004 | iter 00599 | train loss 3.3623 | dt 0.10s\n",
      "epoch 004 | iter 00799 | train loss 3.3844 | dt 0.09s\n",
      "epoch 004 | iter 00999 | train loss 3.3391 | dt 0.11s\n",
      "epoch 004 | iter 01199 | train loss 3.3388 | dt 0.11s\n",
      "epoch 004 | iter 01399 | train loss 3.3515 | dt 0.09s\n",
      "epoch 004 | iter 01599 | train loss 3.3757 | dt 0.10s\n",
      "epoch 004 | iter 01799 | train loss 3.3685 | dt 0.11s\n",
      "epoch 004 | iter 01999 | train loss 3.3335 | dt 0.11s\n",
      "epoch 004 | iter 02199 | train loss 3.3177 | dt 0.10s\n",
      "epoch 004 | iter 02399 | train loss 3.3246 | dt 0.10s\n",
      "[EPOCH 004] valid loss=2.5376 | valid ppl=12.65 | valid BLEU=0.2565 | epoch time=579.1s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2565\n",
      "epoch 005 | iter 00199 | train loss 3.1925 | dt 0.10s\n",
      "epoch 005 | iter 00399 | train loss 3.1954 | dt 0.11s\n",
      "epoch 005 | iter 00599 | train loss 3.2105 | dt 0.11s\n",
      "epoch 005 | iter 00799 | train loss 3.2320 | dt 0.10s\n",
      "epoch 005 | iter 00999 | train loss 3.2134 | dt 0.10s\n",
      "epoch 005 | iter 01199 | train loss 3.2698 | dt 0.10s\n",
      "epoch 005 | iter 01399 | train loss 3.2357 | dt 0.10s\n",
      "epoch 005 | iter 01599 | train loss 3.2425 | dt 0.10s\n",
      "epoch 005 | iter 01799 | train loss 3.2207 | dt 0.10s\n",
      "epoch 005 | iter 01999 | train loss 3.1970 | dt 0.10s\n",
      "epoch 005 | iter 02199 | train loss 3.1917 | dt 0.10s\n",
      "epoch 005 | iter 02399 | train loss 3.1772 | dt 0.10s\n",
      "[EPOCH 005] valid loss=2.4782 | valid ppl=11.92 | valid BLEU=0.2679 | epoch time=589.0s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2679\n",
      "epoch 006 | iter 00199 | train loss 3.1134 | dt 0.10s\n",
      "epoch 006 | iter 00399 | train loss 3.1248 | dt 0.11s\n",
      "epoch 006 | iter 00599 | train loss 3.0883 | dt 0.10s\n",
      "epoch 006 | iter 00799 | train loss 3.1070 | dt 0.10s\n",
      "epoch 006 | iter 00999 | train loss 3.1145 | dt 0.11s\n",
      "epoch 006 | iter 01199 | train loss 3.1196 | dt 0.11s\n",
      "epoch 006 | iter 01399 | train loss 3.0963 | dt 0.10s\n",
      "epoch 006 | iter 01599 | train loss 3.1318 | dt 0.11s\n",
      "epoch 006 | iter 01799 | train loss 3.1402 | dt 0.10s\n",
      "epoch 006 | iter 01999 | train loss 3.1023 | dt 0.10s\n",
      "epoch 006 | iter 02199 | train loss 3.1535 | dt 0.10s\n",
      "epoch 006 | iter 02399 | train loss 3.1034 | dt 0.10s\n",
      "[EPOCH 006] valid loss=2.4457 | valid ppl=11.54 | valid BLEU=0.2784 | epoch time=591.7s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2784\n",
      "epoch 007 | iter 00199 | train loss 2.9616 | dt 0.10s\n",
      "epoch 007 | iter 00399 | train loss 3.0597 | dt 0.11s\n",
      "epoch 007 | iter 00599 | train loss 3.0344 | dt 0.10s\n",
      "epoch 007 | iter 00799 | train loss 3.0331 | dt 0.11s\n",
      "epoch 007 | iter 00999 | train loss 3.0372 | dt 0.11s\n",
      "epoch 007 | iter 01199 | train loss 3.0331 | dt 0.10s\n",
      "epoch 007 | iter 01399 | train loss 2.9918 | dt 0.10s\n",
      "epoch 007 | iter 01599 | train loss 3.0548 | dt 0.10s\n",
      "epoch 007 | iter 01799 | train loss 3.0458 | dt 0.11s\n",
      "epoch 007 | iter 01999 | train loss 3.0758 | dt 0.10s\n",
      "epoch 007 | iter 02199 | train loss 3.0287 | dt 0.11s\n",
      "epoch 007 | iter 02399 | train loss 3.0688 | dt 0.10s\n",
      "[EPOCH 007] valid loss=2.4166 | valid ppl=11.21 | valid BLEU=0.2723 | epoch time=609.2s\n",
      "epoch 008 | iter 00199 | train loss 2.9739 | dt 0.09s\n",
      "epoch 008 | iter 00399 | train loss 2.9417 | dt 0.10s\n",
      "epoch 008 | iter 00599 | train loss 2.9402 | dt 0.10s\n",
      "epoch 008 | iter 00799 | train loss 2.9561 | dt 0.11s\n",
      "epoch 008 | iter 00999 | train loss 2.9771 | dt 0.11s\n",
      "epoch 008 | iter 01199 | train loss 2.9766 | dt 0.10s\n",
      "epoch 008 | iter 01399 | train loss 2.9581 | dt 0.11s\n",
      "epoch 008 | iter 01599 | train loss 2.9869 | dt 0.11s\n",
      "epoch 008 | iter 01799 | train loss 2.9862 | dt 0.10s\n",
      "epoch 008 | iter 01999 | train loss 2.9909 | dt 0.10s\n",
      "epoch 008 | iter 02199 | train loss 2.9695 | dt 0.10s\n",
      "epoch 008 | iter 02399 | train loss 2.9600 | dt 0.10s\n",
      "[EPOCH 008] valid loss=2.3951 | valid ppl=10.97 | valid BLEU=0.2851 | epoch time=591.0s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2851\n",
      "epoch 009 | iter 00199 | train loss 2.8774 | dt 0.11s\n",
      "epoch 009 | iter 00399 | train loss 2.8850 | dt 0.10s\n",
      "epoch 009 | iter 00599 | train loss 2.9087 | dt 0.10s\n",
      "epoch 009 | iter 00799 | train loss 2.9130 | dt 0.10s\n",
      "epoch 009 | iter 00999 | train loss 2.9095 | dt 0.10s\n",
      "epoch 009 | iter 01199 | train loss 2.9071 | dt 0.10s\n",
      "epoch 009 | iter 01399 | train loss 2.9327 | dt 0.11s\n",
      "epoch 009 | iter 01599 | train loss 2.9031 | dt 0.11s\n",
      "epoch 009 | iter 01799 | train loss 2.9359 | dt 0.11s\n",
      "epoch 009 | iter 01999 | train loss 2.9347 | dt 0.11s\n",
      "epoch 009 | iter 02199 | train loss 2.9300 | dt 0.12s\n",
      "epoch 009 | iter 02399 | train loss 2.9585 | dt 0.11s\n",
      "[EPOCH 009] valid loss=2.3777 | valid ppl=10.78 | valid BLEU=0.2792 | epoch time=599.2s\n",
      "epoch 010 | iter 00199 | train loss 2.8756 | dt 0.10s\n",
      "epoch 010 | iter 00399 | train loss 2.8587 | dt 0.10s\n",
      "epoch 010 | iter 00599 | train loss 2.8511 | dt 0.10s\n",
      "epoch 010 | iter 00799 | train loss 2.8686 | dt 0.10s\n",
      "epoch 010 | iter 00999 | train loss 2.8705 | dt 0.11s\n",
      "epoch 010 | iter 01199 | train loss 2.8309 | dt 0.11s\n",
      "epoch 010 | iter 01399 | train loss 2.8868 | dt 0.10s\n",
      "epoch 010 | iter 01599 | train loss 2.8685 | dt 0.10s\n",
      "epoch 010 | iter 01799 | train loss 2.8586 | dt 0.10s\n",
      "epoch 010 | iter 01999 | train loss 2.8972 | dt 0.10s\n",
      "epoch 010 | iter 02199 | train loss 2.8728 | dt 0.10s\n",
      "epoch 010 | iter 02399 | train loss 2.8582 | dt 0.10s\n",
      "[EPOCH 010] valid loss=2.3705 | valid ppl=10.70 | valid BLEU=0.2895 | epoch time=590.7s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2895\n",
      "epoch 011 | iter 00199 | train loss 2.8029 | dt 0.10s\n",
      "epoch 011 | iter 00399 | train loss 2.8144 | dt 0.09s\n",
      "epoch 011 | iter 00599 | train loss 2.8128 | dt 0.10s\n",
      "epoch 011 | iter 00799 | train loss 2.8172 | dt 0.10s\n",
      "epoch 011 | iter 00999 | train loss 2.8372 | dt 0.11s\n",
      "epoch 011 | iter 01199 | train loss 2.8778 | dt 0.11s\n",
      "epoch 011 | iter 01399 | train loss 2.8160 | dt 0.11s\n",
      "epoch 011 | iter 01599 | train loss 2.8199 | dt 0.11s\n",
      "epoch 011 | iter 01799 | train loss 2.8449 | dt 0.11s\n",
      "epoch 011 | iter 01999 | train loss 2.8259 | dt 0.11s\n",
      "epoch 011 | iter 02199 | train loss 2.8083 | dt 0.11s\n",
      "epoch 011 | iter 02399 | train loss 2.8460 | dt 0.10s\n",
      "[EPOCH 011] valid loss=2.3639 | valid ppl=10.63 | valid BLEU=0.2865 | epoch time=653.5s\n",
      "epoch 012 | iter 00199 | train loss 2.7508 | dt 0.12s\n",
      "epoch 012 | iter 00399 | train loss 2.7515 | dt 0.12s\n",
      "epoch 012 | iter 00599 | train loss 2.7713 | dt 0.11s\n",
      "epoch 012 | iter 00799 | train loss 2.7844 | dt 0.10s\n",
      "epoch 012 | iter 00999 | train loss 2.7746 | dt 0.10s\n",
      "epoch 012 | iter 01199 | train loss 2.8115 | dt 0.10s\n",
      "epoch 012 | iter 01399 | train loss 2.8156 | dt 0.11s\n",
      "epoch 012 | iter 01599 | train loss 2.8069 | dt 0.10s\n",
      "epoch 012 | iter 01799 | train loss 2.8020 | dt 0.11s\n",
      "epoch 012 | iter 01999 | train loss 2.7923 | dt 0.10s\n",
      "epoch 012 | iter 02199 | train loss 2.7626 | dt 0.11s\n",
      "epoch 012 | iter 02399 | train loss 2.7987 | dt 0.09s\n",
      "[EPOCH 012] valid loss=2.3531 | valid ppl=10.52 | valid BLEU=0.2874 | epoch time=614.4s\n",
      "epoch 013 | iter 00199 | train loss 2.7361 | dt 0.10s\n",
      "epoch 013 | iter 00399 | train loss 2.7159 | dt 0.10s\n",
      "epoch 013 | iter 00599 | train loss 2.7459 | dt 0.10s\n",
      "epoch 013 | iter 00799 | train loss 2.7537 | dt 0.11s\n",
      "epoch 013 | iter 00999 | train loss 2.7377 | dt 0.10s\n",
      "epoch 013 | iter 01199 | train loss 2.7693 | dt 0.10s\n",
      "epoch 013 | iter 01399 | train loss 2.7323 | dt 0.11s\n",
      "epoch 013 | iter 01599 | train loss 2.7685 | dt 0.11s\n",
      "epoch 013 | iter 01799 | train loss 2.7628 | dt 0.11s\n",
      "epoch 013 | iter 01999 | train loss 2.7315 | dt 0.10s\n",
      "epoch 013 | iter 02199 | train loss 2.7844 | dt 0.11s\n",
      "epoch 013 | iter 02399 | train loss 2.7711 | dt 0.10s\n",
      "[EPOCH 013] valid loss=2.3462 | valid ppl=10.45 | valid BLEU=0.2935 | epoch time=593.0s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2935\n",
      "epoch 014 | iter 00199 | train loss 2.6834 | dt 0.10s\n",
      "epoch 014 | iter 00399 | train loss 2.6949 | dt 0.11s\n",
      "epoch 014 | iter 00599 | train loss 2.7223 | dt 0.10s\n",
      "epoch 014 | iter 00799 | train loss 2.7099 | dt 0.10s\n",
      "epoch 014 | iter 00999 | train loss 2.7082 | dt 0.10s\n",
      "epoch 014 | iter 01199 | train loss 2.7211 | dt 0.10s\n",
      "epoch 014 | iter 01399 | train loss 2.7018 | dt 0.10s\n",
      "epoch 014 | iter 01599 | train loss 2.7033 | dt 0.10s\n",
      "epoch 014 | iter 01799 | train loss 2.7336 | dt 0.10s\n",
      "epoch 014 | iter 01999 | train loss 2.7522 | dt 0.11s\n",
      "epoch 014 | iter 02199 | train loss 2.7529 | dt 0.10s\n",
      "epoch 014 | iter 02399 | train loss 2.7423 | dt 0.10s\n",
      "[EPOCH 014] valid loss=2.3465 | valid ppl=10.45 | valid BLEU=0.2949 | epoch time=595.2s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2949\n",
      "epoch 015 | iter 00199 | train loss 2.6466 | dt 0.10s\n",
      "epoch 015 | iter 00399 | train loss 2.6571 | dt 0.11s\n",
      "epoch 015 | iter 00599 | train loss 2.6991 | dt 0.11s\n",
      "epoch 015 | iter 00799 | train loss 2.6702 | dt 0.11s\n",
      "epoch 015 | iter 00999 | train loss 2.6788 | dt 0.10s\n",
      "epoch 015 | iter 01199 | train loss 2.7129 | dt 0.11s\n",
      "epoch 015 | iter 01399 | train loss 2.6735 | dt 0.10s\n",
      "epoch 015 | iter 01599 | train loss 2.6878 | dt 0.10s\n",
      "epoch 015 | iter 01799 | train loss 2.7083 | dt 0.10s\n",
      "epoch 015 | iter 01999 | train loss 2.7414 | dt 0.11s\n",
      "epoch 015 | iter 02199 | train loss 2.7030 | dt 0.11s\n",
      "epoch 015 | iter 02399 | train loss 2.6954 | dt 0.11s\n",
      "[EPOCH 015] valid loss=2.3428 | valid ppl=10.41 | valid BLEU=0.2945 | epoch time=598.6s\n",
      "epoch 016 | iter 00199 | train loss 2.6509 | dt 0.11s\n",
      "epoch 016 | iter 00399 | train loss 2.6266 | dt 0.11s\n",
      "epoch 016 | iter 00599 | train loss 2.6387 | dt 0.10s\n",
      "epoch 016 | iter 00799 | train loss 2.6624 | dt 0.10s\n",
      "epoch 016 | iter 00999 | train loss 2.6314 | dt 0.10s\n",
      "epoch 016 | iter 01199 | train loss 2.6719 | dt 0.09s\n",
      "epoch 016 | iter 01399 | train loss 2.6583 | dt 0.12s\n",
      "epoch 016 | iter 01599 | train loss 2.6749 | dt 0.10s\n",
      "epoch 016 | iter 01799 | train loss 2.6606 | dt 0.11s\n",
      "epoch 016 | iter 01999 | train loss 2.7142 | dt 0.11s\n",
      "epoch 016 | iter 02199 | train loss 2.6593 | dt 0.10s\n",
      "epoch 016 | iter 02399 | train loss 2.6936 | dt 0.11s\n",
      "[EPOCH 016] valid loss=2.3406 | valid ppl=10.39 | valid BLEU=0.2948 | epoch time=601.6s\n",
      "epoch 017 | iter 00199 | train loss 2.6111 | dt 0.10s\n",
      "epoch 017 | iter 00399 | train loss 2.5926 | dt 0.11s\n",
      "epoch 017 | iter 00599 | train loss 2.6398 | dt 0.11s\n",
      "epoch 017 | iter 00799 | train loss 2.6342 | dt 0.11s\n",
      "epoch 017 | iter 00999 | train loss 2.6179 | dt 0.11s\n",
      "epoch 017 | iter 01199 | train loss 2.6432 | dt 0.10s\n",
      "epoch 017 | iter 01399 | train loss 2.6286 | dt 0.11s\n",
      "epoch 017 | iter 01599 | train loss 2.6407 | dt 0.10s\n",
      "epoch 017 | iter 01799 | train loss 2.6483 | dt 0.10s\n",
      "epoch 017 | iter 01999 | train loss 2.6715 | dt 0.10s\n",
      "epoch 017 | iter 02199 | train loss 2.6579 | dt 0.10s\n",
      "epoch 017 | iter 02399 | train loss 2.6430 | dt 0.11s\n",
      "[EPOCH 017] valid loss=2.3459 | valid ppl=10.44 | valid BLEU=0.2961 | epoch time=597.7s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2961\n",
      "epoch 018 | iter 00199 | train loss 2.5870 | dt 0.10s\n",
      "epoch 018 | iter 00399 | train loss 2.5920 | dt 0.09s\n",
      "epoch 018 | iter 00599 | train loss 2.5721 | dt 0.10s\n",
      "epoch 018 | iter 00799 | train loss 2.5977 | dt 0.11s\n",
      "epoch 018 | iter 00999 | train loss 2.5892 | dt 0.10s\n",
      "epoch 018 | iter 01199 | train loss 2.5975 | dt 0.09s\n",
      "epoch 018 | iter 01399 | train loss 2.6393 | dt 0.10s\n",
      "epoch 018 | iter 01599 | train loss 2.6374 | dt 0.10s\n",
      "epoch 018 | iter 01799 | train loss 2.6361 | dt 0.11s\n",
      "epoch 018 | iter 01999 | train loss 2.6066 | dt 0.10s\n",
      "epoch 018 | iter 02199 | train loss 2.6304 | dt 0.11s\n",
      "epoch 018 | iter 02399 | train loss 2.6528 | dt 0.11s\n",
      "[EPOCH 018] valid loss=2.3420 | valid ppl=10.40 | valid BLEU=0.2968 | epoch time=599.8s\n",
      "[BEST] saved -> ./checkpoints\\best.pt | best_bleu=0.2968\n",
      "epoch 019 | iter 00199 | train loss 2.5949 | dt 0.10s\n",
      "epoch 019 | iter 00399 | train loss 2.5708 | dt 0.11s\n",
      "epoch 019 | iter 00599 | train loss 2.5799 | dt 0.11s\n",
      "epoch 019 | iter 00799 | train loss 2.5605 | dt 0.10s\n",
      "epoch 019 | iter 00999 | train loss 2.6031 | dt 0.09s\n",
      "epoch 019 | iter 01199 | train loss 2.5823 | dt 0.10s\n",
      "epoch 019 | iter 01399 | train loss 2.5527 | dt 0.10s\n",
      "epoch 019 | iter 01599 | train loss 2.6038 | dt 0.10s\n",
      "epoch 019 | iter 01799 | train loss 2.6185 | dt 0.10s\n",
      "epoch 019 | iter 01999 | train loss 2.5899 | dt 0.10s\n",
      "epoch 019 | iter 02199 | train loss 2.5928 | dt 0.11s\n",
      "epoch 019 | iter 02399 | train loss 2.5972 | dt 0.10s\n",
      "[EPOCH 019] valid loss=2.3496 | valid ppl=10.48 | valid BLEU=0.2959 | epoch time=594.7s\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "import torch\n",
    "\n",
    "train_loss_hist = []\n",
    "valid_loss_hist = []\n",
    "valid_bleu_hist = []\n",
    "valid_ppl_hist  = []\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "# best_metric init\n",
    "if BEST_BY == \"bleu\":\n",
    "    best_metric = -1e9\n",
    "else:\n",
    "    best_metric = 1e9\n",
    "\n",
    "# ---- Resume nếu có ----\n",
    "if resume_path is not None and os.path.exists(resume_path):\n",
    "    ckpt = load_checkpoint(\n",
    "        resume_path,\n",
    "        model=model,\n",
    "        optimizer=optimizer,  # ScheduledOptim wrapper\n",
    "        map_location=opt[\"device\"] if \"cuda\" in str(opt[\"device\"]) else \"cpu\",\n",
    "        strict=True\n",
    "    )\n",
    "    start_epoch = int(ckpt.get(\"epoch\", -1)) + 1\n",
    "    best_metric = ckpt.get(\"best_metric\", best_metric)\n",
    "\n",
    "    hist = ckpt.get(\"history\", None)\n",
    "    if hist:\n",
    "        train_loss_hist = hist.get(\"train_loss_hist\", train_loss_hist)\n",
    "        valid_loss_hist = hist.get(\"valid_loss_hist\", valid_loss_hist)\n",
    "        valid_bleu_hist = hist.get(\"valid_bleu_hist\", valid_bleu_hist)\n",
    "        valid_ppl_hist  = hist.get(\"valid_ppl_hist\",  valid_ppl_hist)\n",
    "\n",
    "    print(f\"[RESUME] from {resume_path} | start_epoch={start_epoch} | best_metric={best_metric}\")\n",
    "\n",
    "for epoch in range(start_epoch, opt['epochs']):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    n_print = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        t1 = time.time()\n",
    "        loss = step(model, optimizer, batch, criterion)\n",
    "        running += float(loss)\n",
    "        n_print += 1\n",
    "\n",
    "        if (i + 1) % opt['printevery'] == 0:\n",
    "            avg_loss = running / n_print\n",
    "            print(f\"epoch {epoch:03d} | iter {i:05d} | train loss {avg_loss:.4f} | dt {time.time()-t1:.2f}s\")\n",
    "            running = 0.0\n",
    "            n_print = 0\n",
    "\n",
    "    # ----- validation -----\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vloss = validiate(model, valid_iter, criterion)\n",
    "\n",
    "    vppl = math.exp(vloss) if vloss < 20 else float('inf')  # guard overflow\n",
    "    vbleu = bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)\n",
    "\n",
    "    # lưu history\n",
    "    train_loss_hist.append(None)  # nếu bạn muốn epoch-train-loss thì tính riêng\n",
    "    valid_loss_hist.append(float(vloss))\n",
    "    valid_ppl_hist.append(float(vppl))\n",
    "    valid_bleu_hist.append(float(vbleu))\n",
    "\n",
    "    print(f\"[EPOCH {epoch:03d}] valid loss={vloss:.4f} | valid ppl={vppl:.2f} | valid BLEU={vbleu:.4f} | epoch time={time.time()-t0:.1f}s\")\n",
    "\n",
    "    # ----- Save \"last\" checkpoint -----\n",
    "    history = {\n",
    "        \"train_loss_hist\": train_loss_hist,\n",
    "        \"valid_loss_hist\": valid_loss_hist,\n",
    "        \"valid_bleu_hist\": valid_bleu_hist,\n",
    "        \"valid_ppl_hist\":  valid_ppl_hist,\n",
    "    }\n",
    "\n",
    "    # last (overwrite)\n",
    "    save_checkpoint(\n",
    "        LAST_CKPT,\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        best_metric=best_metric,\n",
    "        history=history,\n",
    "        opt=opt\n",
    "    )\n",
    "\n",
    "    # optional: keep per-epoch file\n",
    "    save_checkpoint(\n",
    "        os.path.join(CKPT_DIR, f\"last_epoch_{epoch:03d}.pt\"),\n",
    "        epoch=epoch,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        best_metric=best_metric,\n",
    "        history=history,\n",
    "        opt=opt\n",
    "    )\n",
    "\n",
    "    # ----- Save \"best\" checkpoint -----\n",
    "    improved = False\n",
    "    if BEST_BY == \"bleu\":\n",
    "        if vbleu > best_metric:\n",
    "            improved = True\n",
    "            best_metric = float(vbleu)\n",
    "    else:  # BEST_BY == \"loss\"\n",
    "        if vloss < best_metric:\n",
    "            improved = True\n",
    "            best_metric = float(vloss)\n",
    "\n",
    "    if improved:\n",
    "        save_checkpoint(\n",
    "            BEST_CKPT,\n",
    "            epoch=epoch,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            best_metric=best_metric,\n",
    "            history=history,\n",
    "            opt=opt,\n",
    "            extra={\"best_by\": BEST_BY}\n",
    "        )\n",
    "        print(f\"[BEST] saved -> {BEST_CKPT} | best_{BEST_BY}={best_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 000] valid loss=3.9493 | valid BLEU=0.0275\n",
      "[EPOCH 001] valid loss=3.1849 | valid BLEU=0.1258\n",
      "[EPOCH 002] valid loss=2.7966 | valid BLEU=0.2036\n",
      "[EPOCH 003] valid loss=2.6357 | valid BLEU=0.2377\n",
      "[EPOCH 004] valid loss=2.5376 | valid BLEU=0.2565\n",
      "[EPOCH 005] valid loss=2.4782 | valid BLEU=0.2679\n",
      "[EPOCH 006] valid loss=2.4457 | valid BLEU=0.2784\n",
      "[EPOCH 007] valid loss=2.4166 | valid BLEU=0.2723\n",
      "[EPOCH 008] valid loss=2.3951 | valid BLEU=0.2851\n",
      "[EPOCH 009] valid loss=2.3777 | valid BLEU=0.2792\n",
      "[EPOCH 010] valid loss=2.3705 | valid BLEU=0.2895\n",
      "[EPOCH 011] valid loss=2.3639 | valid BLEU=0.2865\n",
      "[EPOCH 012] valid loss=2.3531 | valid BLEU=0.2874\n",
      "[EPOCH 013] valid loss=2.3462 | valid BLEU=0.2935\n",
      "[EPOCH 014] valid loss=2.3465 | valid BLEU=0.2949\n",
      "[EPOCH 015] valid loss=2.3428 | valid BLEU=0.2945\n",
      "[EPOCH 016] valid loss=2.3406 | valid BLEU=0.2948\n",
      "[EPOCH 017] valid loss=2.3459 | valid BLEU=0.2961\n",
      "[EPOCH 018] valid loss=2.3420 | valid BLEU=0.2968\n",
      "[EPOCH 019] valid loss=2.3496 | valid BLEU=0.2959\n",
      "[EPOCH 018] valid loss=2.3420 | valid BLEU=0.2968\n",
      "[EPOCH 019] valid loss=2.3496 | valid BLEU=0.2959\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import torch\n",
    "\n",
    "CKPT_DIR = Path(\"./checkpoints\")          # dùng /\n",
    "ckpt_paths = sorted(CKPT_DIR.glob(\"*.pt\"))\n",
    "\n",
    "# nếu file bạn dạng last_epoch_001.pt\n",
    "_epoch_re = re.compile(r\"last_epoch_(\\d+)\\.pt$\", re.IGNORECASE)\n",
    "\n",
    "def get_epoch_from_name(p: Path):\n",
    "    m = _epoch_re.search(p.name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def extract_loss_bleu_from_ckpt(ckpt):\n",
    "    \"\"\"\n",
    "    Trả về (valid_loss, valid_bleu) nếu đọc được từ ckpt, không thì (None, None)\n",
    "    Hỗ trợ các format save phổ biến:\n",
    "      - ckpt[\"history\"][\"valid_loss_hist\"], ckpt[\"history\"][\"valid_bleu_hist\"] (lấy phần tử cuối)\n",
    "      - ckpt[\"valid_loss\"], ckpt[\"valid_bleu\"]\n",
    "      - ckpt[\"metrics\"] dict\n",
    "    \"\"\"\n",
    "    vloss = vbleu = None\n",
    "\n",
    "    if isinstance(ckpt, dict):\n",
    "        # 1) history\n",
    "        hist = ckpt.get(\"history\", None)\n",
    "        if isinstance(hist, dict):\n",
    "            vloss_hist = hist.get(\"valid_loss_hist\", None)\n",
    "            vbleu_hist = hist.get(\"valid_bleu_hist\", None)\n",
    "            if isinstance(vloss_hist, list) and len(vloss_hist):\n",
    "                vloss = float(vloss_hist[-1])\n",
    "            if isinstance(vbleu_hist, list) and len(vbleu_hist):\n",
    "                vbleu = float(vbleu_hist[-1])\n",
    "\n",
    "        # 2) direct keys\n",
    "        if vloss is None and \"valid_loss\" in ckpt:\n",
    "            try: vloss = float(ckpt[\"valid_loss\"])\n",
    "            except: pass\n",
    "        if vbleu is None and \"valid_bleu\" in ckpt:\n",
    "            try: vbleu = float(ckpt[\"valid_bleu\"])\n",
    "            except: pass\n",
    "\n",
    "        # 3) metrics dict\n",
    "        metrics = ckpt.get(\"metrics\", None)\n",
    "        if isinstance(metrics, dict):\n",
    "            if vloss is None and \"valid_loss\" in metrics:\n",
    "                try: vloss = float(metrics[\"valid_loss\"])\n",
    "                except: pass\n",
    "            if vbleu is None and \"valid_bleu\" in metrics:\n",
    "                try: vbleu = float(metrics[\"valid_bleu\"])\n",
    "                except: pass\n",
    "\n",
    "    return vloss, vbleu\n",
    "\n",
    "\n",
    "# (tuỳ chọn) sort theo epoch trong tên file cho đẹp; nếu không match thì giữ thứ tự glob\n",
    "def sort_key(p: Path):\n",
    "    e = get_epoch_from_name(p)\n",
    "    return (e is None, e if e is not None else p.name)\n",
    "\n",
    "ckpt_paths = sorted(ckpt_paths, key=sort_key)\n",
    "\n",
    "for p in ckpt_paths:\n",
    "    ckpt = torch.load(p.as_posix(), map_location=\"cpu\")\n",
    "\n",
    "    # epoch: ưu tiên epoch trong tên file, fallback sang ckpt[\"epoch\"] nếu có\n",
    "    epoch = get_epoch_from_name(p)\n",
    "    if epoch is None and isinstance(ckpt, dict) and \"epoch\" in ckpt:\n",
    "        try:\n",
    "            epoch = int(ckpt[\"epoch\"])\n",
    "        except:\n",
    "            epoch = None\n",
    "\n",
    "    vloss, vbleu = extract_loss_bleu_from_ckpt(ckpt)\n",
    "\n",
    "    epoch_str = f\"{epoch:03d}\" if isinstance(epoch, int) else \"???\"\n",
    "    loss_str  = f\"{vloss:.4f}\" if isinstance(vloss, (int, float)) else \"NA\"\n",
    "    bleu_str  = f\"{vbleu:.4f}\" if isinstance(vbleu, (int, float)) else \"NA\"\n",
    "\n",
    "    print(f\"[EPOCH {epoch_str}] valid loss={loss_str} | valid BLEU={bleu_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: checkpoints/best.pt | epoch: 18 | best_metric: 0.2967609167098999\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "BEST_CKPT = \"checkpoints/best.pt\"\n",
    "DEVICE = opt[\"device\"]\n",
    "\n",
    "def strip_prefix(sd, prefix):\n",
    "    if not prefix:\n",
    "        return sd\n",
    "    out = {}\n",
    "    for k, v in sd.items():\n",
    "        out[k[len(prefix):] if k.startswith(prefix) else k] = v\n",
    "    return out\n",
    "\n",
    "def load_best(model, ckpt_path=BEST_CKPT, device=DEVICE, strict=True):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    sd = ckpt[\"model_state\"]  # ✅ đúng format của bạn\n",
    "\n",
    "    # nếu từng train bằng DataParallel thì có \"module.\"\n",
    "    sd = strip_prefix(sd, \"module.\")\n",
    "    # đôi khi save prefix \"model.\"\n",
    "    sd = strip_prefix(sd, \"model.\")\n",
    "\n",
    "    model.load_state_dict(sd, strict=strict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return ckpt\n",
    "\n",
    "ckpt = load_best(model)\n",
    "print(\"Loaded:\", BEST_CKPT, \"| epoch:\", ckpt.get(\"epoch\"), \"| best_metric:\", ckpt.get(\"best_metric\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_test: 1553 1553\n",
      "EN ex: How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?\n",
      "VI ex: Làm sao tôi có thể trình bày trong 10 phút về sợi dây liên kết những người phụ nữ qua ba thế hệ , về việc làm thế nào những sợi dây mạnh mẽ đáng kinh ngạc ấy đã níu chặt lấy cuộc sống của một cô bé bốn tuổi co quắp với đứa em gái nhỏ của cô bé , với mẹ và bà trong suốt năm ngày đêm trên con thuyền nhỏ lênh đênh trên Biển Đông hơn 30 năm trước , những sợi dây liên kết đã níu lấy cuộc đời cô bé ấy và không bao giờ rời đi -- cô bé ấy giờ sống ở San Francisco và đang nói chuyện với các bạn hôm nay ?\n"
     ]
    }
   ],
   "source": [
    "TEST_EN = \"./data/tst2012.en\"\n",
    "TEST_VI = \"./data/tst2012.vi\"\n",
    "\n",
    "test_src_en, test_ref_vi = read_data(TEST_EN, TEST_VI)\n",
    "\n",
    "print(\"n_test:\", len(test_src_en), len(test_ref_vi))\n",
    "assert len(test_src_en) == len(test_ref_vi)\n",
    "print(\"EN ex:\", test_src_en[0])\n",
    "print(\"VI ex:\", test_ref_vi[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating tst2012: 100%|██████████| 1553/1553 [06:01<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tst2012.pred.vi.txt | n_lines: 1553\n",
      "HYP ex: làm thế nào tôi có thể nói trong vòng 10 phút về sự kết nối của phụ nữ hơn ba thế hệ, về sức mạnh đáng kinh ngạc của những liên kết đó đã nắm giữ trong cuộc sống của một cô gái bốn tuổi cùng em gái, em gái và bà mẹ của cô trong 5 ngày và đêm trong một chiếc thuyền nhỏ ở trung quốc hơn 30 năm về trước, mang đến cho cuộc đời của cô gái nhỏ và không bao giờ từ bỏ cuộc sống ở san francisco?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detok_spm_text(s: str) -> str:\n",
    "    s = s.replace(\"▁\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "def normalize_pred(pred):\n",
    "    if isinstance(pred, list):\n",
    "        s = \"\".join(pred) if any(\"▁\" in t for t in pred) else \" \".join(pred)\n",
    "    else:\n",
    "        s = str(pred)\n",
    "\n",
    "    for t in [\"<sos>\", \"<eos>\", \"<pad>\"]:\n",
    "        s = s.replace(t, \" \")\n",
    "    s = \" \".join(s.split()).strip()\n",
    "    return detok_spm_text(s)\n",
    "\n",
    "LIMIT_TRANSLATE = None  # 50 để test nhanh, None=full\n",
    "src_used = test_src_en if LIMIT_TRANSLATE is None else test_src_en[:LIMIT_TRANSLATE]\n",
    "\n",
    "test_hyp_vi = []\n",
    "for src in tqdm(src_used, desc=\"Translating tst2012\"):\n",
    "    pred = translate_sentence(src, model, SRC, TRG, opt[\"device\"], opt[\"k\"], opt[\"max_strlen\"], sp_trg)\n",
    "    test_hyp_vi.append(normalize_pred(pred))\n",
    "\n",
    "OUT_TXT = \"tst2012.pred.vi.txt\"\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in test_hyp_vi:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_TXT, \"| n_lines:\", len(test_hyp_vi))\n",
    "print(\"HYP ex:\", test_hyp_vi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating tst2013: 100%|██████████| 1268/1268 [05:44<00:00,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tst2013.pred.vi.txt | n_lines: 1268\n",
      "HYP ex: khi tôi còn nhỏ, tôi nghĩ đất nước tôi là đất nước tốt nhất trên hành tinh này, và tôi lớn lên hát một bài hát có tên là &quot không có gì để ghen tị. &quot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detok_spm_text(s: str) -> str:\n",
    "    s = s.replace(\"▁\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    s = re.sub(r\"\\s+([.,!?;:])\", r\"\\1\", s)\n",
    "    return s\n",
    "\n",
    "def normalize_pred(pred):\n",
    "    if isinstance(pred, list):\n",
    "        s = \"\".join(pred) if any(\"▁\" in t for t in pred) else \" \".join(pred)\n",
    "    else:\n",
    "        s = str(pred)\n",
    "\n",
    "    for t in [\"<sos>\", \"<eos>\", \"<pad>\"]:\n",
    "        s = s.replace(t, \" \")\n",
    "    s = \" \".join(s.split()).strip()\n",
    "    return detok_spm_text(s)\n",
    "\n",
    "LIMIT_TRANSLATE = None  # 50 để test nhanh, None=full\n",
    "src_used = valid_src_data if LIMIT_TRANSLATE is None else valid_src_data[:LIMIT_TRANSLATE]\n",
    "\n",
    "test_hyp_vi1 = []\n",
    "for src in tqdm(src_used, desc=\"Translating tst2013\"):\n",
    "    pred = translate_sentence(src, model, SRC, TRG, opt[\"device\"], opt[\"k\"], opt[\"max_strlen\"], sp_trg)\n",
    "    test_hyp_vi1.append(normalize_pred(pred))\n",
    "\n",
    "OUT_TXT = \"tst2013.pred.vi.txt\"\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in test_hyp_vi1:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Saved:\", OUT_TXT, \"| n_lines:\", len(test_hyp_vi1))\n",
    "print(\"HYP ex:\", test_hyp_vi1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU score: 23.12\n",
      "N-gram precisions: [53.1068788303515, 29.726703795142676, 17.397313782638744, 10.397100535770564]\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# Đọc file reference\n",
    "with open(\"data/tst2012.vi\", \"r\", encoding=\"utf-8\") as f:\n",
    "    refs = [f.read().splitlines()] # Phải để trong list của list\n",
    "\n",
    "# Đọc file prediction\n",
    "with open(\"tst2012.pred.vi.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    preds = f.read().splitlines()\n",
    "\n",
    "# Tính toán\n",
    "# Chú ý: mặc định sacrebleu dùng tokenizer '13a' (chuẩn quốc tế)\n",
    "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
    "\n",
    "print(f\"SacreBLEU score: {bleu.score:.2f}\")\n",
    "print(f\"N-gram precisions: {bleu.precisions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU score: 25.72\n",
      "N-gram precisions: [56.65107046646817, 32.349913227637735, 19.612576002933178, 12.173802485617113]\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# Đọc file reference\n",
    "with open(\"data/tst2013.vi\", \"r\", encoding=\"utf-8\") as f:\n",
    "    refs = [f.read().splitlines()] # Phải để trong list của list\n",
    "\n",
    "# Đọc file prediction\n",
    "with open(\"tst2013.pred.vi.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    preds = f.read().splitlines()\n",
    "\n",
    "# Tính toán\n",
    "# Chú ý: mặc định sacrebleu dùng tokenizer '13a' (chuẩn quốc tế)\n",
    "bleu = sacrebleu.corpus_bleu(preds, refs)\n",
    "\n",
    "print(f\"SacreBLEU score: {bleu.score:.2f}\")\n",
    "print(f\"N-gram precisions: {bleu.precisions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "v5E8G0-8QFbj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2958706021308899"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0CwtdJeUNQJo",
    "outputId": "190c4a93-436a-4b00-832b-ae8f4c183fe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cần thiết phải có sự can thiệp sớm để ngăn chặn sự phát triển và giới hạn của những biến thể của các trung tâm y học thấp đối với những học sinh trong những trung tâm y học với những dấu hiệu đột biến của các mạch máu dưới cùng cực và đảm bảo sức khoẻ và an toàn chất lượng cuộc sống cho học sinh và nhân viên y tế trong tương lai .'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'It is necessary to have early intervention measures to prevent the progression and limit the complications of varicose veins of the lower extremities for nursing students with early signs of varicose veins of the lower extremities and ensure health and safety and improve the quality of life for nursing students and medical staffs in the future.'\n",
    "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'], sp_trg)\n",
    "trans_sent\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
